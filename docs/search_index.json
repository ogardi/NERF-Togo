[
["index.html", "Manuel de référence Préface", " République Togolaise — Système Nationale de Surveillance des Forêts Manuel de référence Oliver Gardi, Fifonsi Dangbo, Sophie Dzigbod, Ditorgue Bakabima Version NRF v1.1 / 2020-06-10 Préface Ce manuel de référence à comme objectif de décrire le fonctionnement du Système National de Surveillance des Forêts au Togo (SNSF). Les éléments traités sont les arrangements instutitionelles, l’implémentation de l’Inventaires Forestier National (IFN) et de Système Surveillance Terrestres par Satellite (SSTS) et l’approche technique pour en sortir les informations nécessaires pour le Niveau de Référence pour les Forêts du Togo (NRF) ainsi que pour le Monitoring, Reporting et Verification (MRV) dans le cadre de l’engagement du Togo pour le REDD+. La partie Analyses NRF/MRV décrit en détail les outils utilisés pour établir et le Niveau de Référence pour les Forêts du Togo 1.0, soumis au sécrétariat CCNUCC en Janvier 2020 et pour mettre à jour les analyses dans le cadre d’une surveillance de la biomasse forestier continue dans le cadre du Monitoring, Reporting et Verification pour la REDD+. Les résultats de ces analyses sont publiés ailleurs (liens sur les rapports sur le site CCNUCC et géoportail). En cas de questions, veuillez contacter la Coordination nationale REDD+ du Togo "],
["00_introduction.html", "1 Introduction", " 1 Introduction L’objectif du Système National de Surveillance des Forêts (SNSF) est d’évaluer régulièrement l’état des forêts togolaises et leur évolution. Dans le code forestier du Togo, la forêt est définie comme: un espace occupant une superficie de plus de 0,5 hectare avec des arbres atteignant une hauteur supérieure à 5 mètres et un couvert arboré de plus de 10 pour cent, ou avec des arbres capables d’atteindre ces seuils in situ. Pour l’évaluation du développement des zones forestières, le SNSF distingue entre terres forestières avec un couvert des houppiers ≥ 30% et les terres boisées avec un couvert des houppiers entre 10% – 30%. Actuellement, sur base des images Landsat, le SNSF enregistre que l’évolution des terres forestiers avec une couverture des houppiers ≥ 30%. Des données satellitaires de plus haute resolution seront nécessaires pour évaluer également les terres boisées. Le SNSF combine les données recueillies sur le terrain avec les données des images satellites pour fournir des informations sur lévolution du l’ensemble des forêts dans le pays. Comme illustré dans l’image au-dessous et décrit dans les sections suivantes, le SNSF consiste de trois pilliers principaux: Le Inventaire Forestier National (IFN) recueille des informations détaillées sur l’état des forêts sur un nombre limité de placettes d’échantillonnage permanentes sur le terrain. Au moyen du Système de Surveillance Terrestre par Satellite (SSTS), des informations sur la couverture et l’utilisation des sols sont recueillies sur un grand nombre de parcelles d’échantillonnage à partir d’images satellites. Avec l’aide du SSTS, les informations sur l’IFN peuvent être extrapolées à l’ensemble du pays. Le Niveau de Référence des Forêts (NRF) ainsi que le Monitoring, Reporting et Vérification (MRV) des changements dans le réservoirs carbone forestiersest une applications du SNSF pour informer la communauté internationale sur l’engagement du Togo dans le cadre du mécanisme REDD+. Dans ce cadre, les données de la IFN sont utilisées pour déterminer le stockage du carbone dans la biomasse des arbres, tandis que les données de la SSTS sont utilisées pour déterminer le changement de la superficie forestière. Ensemble, cela se traduit par les pertes de carbone dues à la déforestation et la séquestration du carbone provenant du reboisement. En future, d’autres sources de données pourraient être intégrées au SNSF, telles que le carbone organique du sol (prévu à rélever dans l’IFN-2) feux de brousse (base de données SANGE) dégradation des forêts (utilisation des images de haute résolution Sentinel-2) droits fonciers plantations exploitation du bois … Pour l’avenir, il est également prévu d’impliquer la population locale dans la surveillance des forêts, par exemple en signalant les activités irrégulières à l’aide d’une application pour smartphone. "],
["01_arrangements.html", "1.1 Arrangements institutionelles", " 1.1 Arrangements institutionelles L’arrangement institutionnel propose pour le système national de suivi des forêts se presente comme ci-dessous. Coordination: Le Ministère de l’Environnement, du Developpement Durable et de la Protection de la Nature (MEDDPN) à travers la Direction de l’Environnement (DE) est chargée de la soumission des rapports (Communication Nationale et Rapports Biennaux) à la Convention Cadre des Nations Unies sur le Changement Climatique (CCNUCC). La Cellule MRV de la Coordination nationale REDD+ située a l’ODEF est responsable de la coordination de toutes les institutions et organisations impliquées dans l’alimentation du système SNSF. Cette cellule est l’entité clé chargée de faciliter et de soutenir les communications sur NRF/NERF du Togo. Le Groupe de travail NERF/MRV et l’équipe nationale de suivi des forêts sont chargés du travail et des décisions et choix techniques sur les données, résultats et méthodologie adoptés pour le NRF/MRV. C’est la cheville ouvrière de la cellule MRV. Elles sont constituées des cadres des institutions qui interviennent dans le système national de suivi des forêts (SNSF). La Direction de l’Environnement (DE) se charge des inventaires de gaz à effet de serre (I-GES) de tous les secteurs mais assure la cohérence des données d’I-GES du secteur agriculture, foresterie et autres affectations des terres (AFAT) avec les rapports qui seront soumis à la CCNUCC. La DE se chargera d’assurer la cohérence entre la méthodologie utilisée dans le cadre du NRF avec les données d’I-GES du secteur AFAT. Données d’activités: L’Unité de gestion de bases de données cartographiques (UGBDC) de la Direction des études et de la planification (DEP), chargée de la gestion de la cartographie des domaines forestiers du Togo ainsi que la Division cartographie et Télédétection (DCT) de l’Officie de développement et d’exploitation des forêts (ODEF) chargée de la cartographie des forêts classées et plantations étatique se chargeront de produire les données d’activités à travers le système de suivi des terres par satellite (SSTS). L’Agence nationale de gestion de l’environnement (ANGE) est chargée de fournir les données sur les feux de végétation. Facteurs d’émission: La cellule de gestion de la base des données des ressources forestières et des résultats de l’inventaire forestier national (CBDR-IFN) de la Direction des ressources forestières (DRF) et la Division cartographie et télédétection (DCT) de l’ODEF sont chargé de produire les facteurs d’émission à travers les inventaires forestier nationaux et les inventaires des plantations. Données complémentaires: la Direction générale de l’énergie du ministère des mines et énergie DGE/MME se chargera de fournir les données sur la consommation en bois énergie. la Direction de la Statistique agricole de l’Informatique et de la Documentation (DCID) et l’Institut Togolais de Recherche Agronomique (ITRA) produiront des données sur l’agriculture (superficie emblavées et le cheptel). les données de recherche des universités du Togo alimenteront le mécanisme MRV ainsi que le NRF. L’Institut national de la statistique et des études économiques et démographiques (INSEED) donnera des compléments d’informations sur la démographie et autres. Contrôle de qualité / validation interne: L’assurance qualité et le contrôle qualité se fera à travers l’évaluation indépendante interne du Laboratoire de biologie et écologie végétale (LBEV) et le Laboratoire de recherche forestière (LRF) de l’université de Lomé (LBEV/UL) ainsi que la Direction générale de la cartographie (DGC). Les laboratoires universitaires LRF et LBEV évalueront les méthodes et nouveaux données au fur et à mesure qu’ils seront générés. "],
["02_how-to.html", "1.2 Pour commencer", " 1.2 Pour commencer 1.2.1 Travailler avec le SNSF 1.2.1.1 Installation R et RStudio Le traitement des données au sein du SNSF est principalement basé sur les scripts R (R Project for Statistical Computing). Pour pouvoir travailler avec ces scripts, vous devez disposer d’une installation de R. En outre, l’installation des paquets suivants est nécessaire dans R : install.packages(&quot;sp&quot;, &quot;rgdal&quot;, &quot;raster&quot;, &quot;randomForest&quot;, &quot;caret&quot;, &quot;openxlsx&quot;, &quot;dplyr&quot;, &quot;tidyr&quot;, &quot;ggplot2&quot;, &quot;foreach&quot;, &quot;doParallel&quot;, &quot;knitr&quot;, &quot;rmarkdown&quot;, &quot;tinytex&quot;) Les scripts R dépendent des fois des outils GDAL disponible dans l’environnement. Sur les systèmes Linux, ceux-ci peuvent être installés en utilisant `apt-get install python-gdal’. Comme interface utilisateur graphique pour R, nous recommandons l’installation de RStudio. 1.2.1.2 Se connecter avec GitHub Le code SNSF-Togo se trouve sur un dépôt du code GitHub (https://github.com/ogardi/SNSF-Togo). Pour copier le code sur votre ordinateur, vous pouvez dans RStudio créer un nouveau projet à partir d’un dépôt Git comme illustré dans la figure ci-dessous. Git vous permet de vous synchroniser avec le dépôt sur GitHub. En appuyant sur le bouton “Pull” (point 1. dans la figure ci-dessous), vous mettez à jour votre code local avec le dépôt du code sur GitHub. Pour modifier vous-même le code sur GitHub, vous devez disposer d’un compte GitHub et d’une autorisation du gestionnaire du dépôt. La procédure à suivre pour apporter des modifications au code est la suivante. Avant faire des modifications dans le code: synchroniser avec le dépôt sur GitHub en utilisant le bouton “Pull” (tirer) De temps en temps, pendant le travail: confirmer les modifications apportées au code en appuyant sur le bouton “Commit”. Une nouvelle fenêtre s’ouvrira. sélectionner les fichiers à confirmer (CTRL-A) et les marquer (ESPACE) joindre un message (ce qui a été modifié) confirmer les modifications apportées au code en appuyant sur le bouton “Commit” (confirmer) synchroniser de nouveau avec GitHub (et résoudre les éventuels conflits) télécharger les modifications sur GitHub en utilisant le bouton “Push” (pousser) 1.2.1.3 Travailler sur la documentation Ce manuel de référence fait également partie du dépôt du code sur GitHub. Cela signifie que pour réviser la documentation, vous devez procéder comme décrit ci-dessus. La documentation a été créée dans Markdown, une syntaxe simple pour la structuration et le formatage des documents. Ces documents Markdown sont ensuite traduits en HTML par le paquet R knitr pour générer un site web. Le répertoire 04_Manuel contient les documents Markdown (fichiers .Rmd), le répertoire docs contient le site web résultant (fichiers .html). Dans RStudio, le site web peut être généré en utilisant le bouton “Build Book”. Pour ce faire, il faut d’abord spécifier le répertoire dans lequel se trouvent les fichiers Markdown dans les configurations, comme indiqué dans la figure ci-dessous. "],
["03_commencer.html", "", " 1.2.2 Structure du répértoire Les outils R dépendent d’une certaine structure des fichiers. Le répértoire de base SNSF_Togo est structuré comme suivant: SNSF_Togo ========= ├── data # Données de base externes #################### ├── GADM # :: frontières administratives ├── Landsat # :: images satellitaires ├── SRTM # :: données topographiques └── Worldclim # :: données climatiques └── SNSF_v1.0_20200106 # Répértoire SNSF v1.0 ######################## ├── .Rprofile #:: Script R de initialisation ├── 01_SSTS # SYSTÈME DE SURVEILLANCE TERRESTRE ========= ├── 01_data #:: images et autres données pré-traités -- └── 02_BdD #:: base de données SSTS ------------------ ├── 02_IFN # INVENTAIRE FORESTIER NATIONAL ============= └── 01_IFN-1 #:: données d&#39;inventaire IFN-1 ------------ ├── 03_NRF-MRV # NIVEAU DE REFERENCE / MRV ================ ├── 01_MCF #:: Modification Couvert Forestier -------- ├── 02_AGB #:: cartographie biomasse ----------------- └── 03_report #:: rapport NRF/MRV ----------------------- ├── 04_Manuel # CETTE DOCUMENTATION DU SNSF ============= └── docs #:: site web manuel de référence └── SNSF_v1.x # Répértoire SNSF version actualisé ########### ├── ... └── ... La structure du répertoire est définit dans le script R .Rprofile et peut être ajousté. C’est seulement les répétoires src et manual qui sont mis à diposition dans le dépôt GitHub. Les autres répétoires et données doivent être installés manuellement. 1.2.3 Création d’un nouveau projet Pour la création d’un nouveau projet avec le code le plus actuel, on clone le dépôt du projet sur GitHub par la commande git clone --single-branch https://github.com/ogardi/NERF-Togo.git NOM-REPERTOIRE. L’installation d’une version spécifique peut être fait avec git clone -b VERSION --single-branch https://github.com/ogardi/NERF-Togo.git NOM-REPERTOIRE. En travaillant avec RStudio, le plus facile est de créer directement un projet RStudio au départ due dépôt GitHub (File &gt; New Project ... &gt; Version Control &gt; Git) avec les mêmes paramètre que l’installation directe en haut. Dans une prochaine étape, les données doivent être rendues disponibles, soit par une nouvelle acquisition, soit par une copie des répertoires existants. S’il n’est pas encore disponible, le répertoire ../data/ doit être créé et les données de base correspondantes doivent être fournies. Dans le répertoire ./01_SSTS/02_BdD/, le réseau d’échantillonnage ainsi que les données d’entraînement et de validation doivent être stockées. En outre, les données d’inventaire doivent être stockées dans le répertoire ./02_IFN/. 1.2.4 Définition des variables Le traitement des images ainsi que les différentes analyses se font via des R-Scripts. Les variables et les fonctions utilisées dans les différents scripts sont définit dans le fichier .Rprofile, qui est automatiquement chargé au démarrage de R. Si le processus de chargement a réussi, vous pouvez voir un message de bienvenue suivant sur la console R. Si aucun message n’apparaît, assurez-vous que a) R est lancé dans le répertoire et b) qu’aucun message d’erreur ne se produit. Les messages d’erreur possibles sont des paquets manquants (les paquets correspondants doivent être installés en premier) ou le fait de ne pas trouver les limites administratives de GADM dans ../data/GADM (doivent également être installés en premier). Si nécessaire les variables sont ajustées et R est redémarré. Notamment les informations sur la période analysée YEARS.ALL et les années à prendre en compte pour les différentes évaluations YEARS.JNT, YEARS.VAL et YEARS.REF. Script R: .Rprofile ############################################################################### # .Rprofile: Préparer l&#39;environnement (libraries, variables) # ----------------------------------------------------------------------------- # Bern University of Applied Sciences # Oliver Gardi, &lt;oliver.gardi@bfh.ch&gt; # 13 Mai 2020 # Charger libraries ========================================================== library(&quot;sp&quot;) # Classes et méthodes pour les données spatiales library(&quot;rgdal&quot;) # Geospatial Data Abstraction Library library(&quot;raster&quot;) # Analyse et modélisation des données géographiques library(&quot;randomForest&quot;) # Algorithme de classification et régression library(&quot;caret&quot;) # Outils pour classification et régression library(&quot;openxlsx&quot;) # Lire et écrire des fichiers Excel (xlsx) library(&quot;dplyr&quot;) # Fonctions pour manipuler des données library(&quot;tidyr&quot;) # Fonctions pour reorganiser des données library(&quot;ggplot2&quot;) # Production des figures library(&quot;RColorBrewer&quot;) # palettes de couleurs library(&quot;foreach&quot;) # Faire des calcules en parallèle ... library(&quot;doParallel&quot;) # ... sur plusieurs processeurs library(&quot;knitr&quot;) # pour la documentation html # Créer un environnement caché =============================================== .snsf = new.env() # Années / Périodes ----------------------------- .snsf$YEARS.ALL &lt;- 1985:2019 # - tous .snsf$YEARS.JNT &lt;- c(1987, 2003, 2005, 2007, 2015, 2017, 2018) # - conjointes .snsf$YEARS.REF &lt;- c(1987, 2003, 2015, 2018) # - référence .snsf$YEARS.NRF &lt;- c( 2003, 2015, 2018) # - NRF # Répertoires ----------------------------------- .snsf$DIR.RAW.DAT &lt;- &quot;../data&quot; .snsf$DIR.SST &lt;- &quot;./01_SSTS&quot; .snsf$DIR.SST.DAT &lt;- paste0(.snsf$DIR.SST, &quot;/01_data&quot;) .snsf$DIR.SST.DAT.LST &lt;- paste0(.snsf$DIR.SST.DAT, &quot;/Landsat&quot;) .snsf$DIR.SST.DAT.WC2 &lt;- paste0(.snsf$DIR.SST.DAT, &quot;/Worldclim&quot;) .snsf$DIR.SST.BDD &lt;- paste0(.snsf$DIR.SST, &quot;/02_BdD&quot;) .snsf$DIR.SST.BDD.GRD &lt;- paste0(.snsf$DIR.SST.BDD, &quot;/01_reseau-SSTS&quot;) .snsf$DIR.SST.BDD.TPS &lt;- paste0(.snsf$DIR.SST.BDD, &quot;/02_train-plots&quot;) .snsf$DIR.SST.BDD.VPS &lt;- paste0(.snsf$DIR.SST.BDD, &quot;/03_val-plots&quot;) .snsf$DIR.IFN &lt;- &quot;./02_IFN&quot; .snsf$DIR.IFN.DAT &lt;- paste0(.snsf$DIR.IFN, &quot;/01_field-data&quot;) .snsf$DIR.MRV &lt;- &quot;./03_NRF-MRV&quot; .snsf$DIR.MRV.MCF &lt;- paste0(.snsf$DIR.MRV, &quot;/01_MCF&quot;) .snsf$DIR.MRV.MCF.REF &lt;- paste0(.snsf$DIR.MRV.MCF, &quot;/01_ref-maps&quot;) .snsf$DIR.MRV.MCF.RAW &lt;- paste0(.snsf$DIR.MRV.MCF, &quot;/02_raw-maps&quot;) .snsf$DIR.MRV.MCF.CLN &lt;- paste0(.snsf$DIR.MRV.MCF, &quot;/03_cln-maps&quot;) .snsf$DIR.MRV.MCF.VAL &lt;- paste0(.snsf$DIR.MRV.MCF, &quot;/04_validation&quot;) .snsf$DIR.MRV.AGB &lt;- paste0(.snsf$DIR.MRV, &quot;/02_AGB&quot;) .snsf$DIR.MRV.AGB.REF &lt;- paste0(.snsf$DIR.MRV.AGB, &quot;/01_ref-maps&quot;) .snsf$DIR.MRV.AGB.RES &lt;- paste0(.snsf$DIR.MRV.AGB, &quot;/02_results&quot;) # Système de référence des coordonnées ---------- .snsf$UTM.30 &lt;- crs(&quot;+proj=utm +zone=30 +datum=WGS84 +units=m +no_defs +ellps=WGS84 +towgs84=0,0,0&quot;) .snsf$UTM.31 &lt;- crs(&quot;+proj=utm +zone=31 +datum=WGS84 +units=m +no_defs +ellps=WGS84 +towgs84=0,0,0&quot;) # Domaine d&#39;intérêt ---------------------------- .snsf$TGO &lt;- spTransform( readOGR(paste0(.snsf$DIR.RAW.DAT, &quot;/GADM/gadm36_TGO_0.shp&quot;)), .snsf$UTM.31 ) snsf$TGO.REG &lt;- spTransform( readOGR(paste0(.snsf$DIR.RAW.DAT, &quot;/GADM/gadm36_TGO_1.shp&quot;)), .snsf$UTM.31 ) .snsf$TGO.EXT &lt;- extent(151155, 373005, 670665, 1238175) # xmin, xmax, ymin, ymax # Noms des couches de données ------------------- .snsf$SST.LSBANDS &lt;- c(&quot;B&quot;, &quot;G&quot;, &quot;R&quot;, &quot;NIR&quot;, &quot;SWIR1&quot;, &quot;SWIR2&quot;, &quot;evi&quot;, &quot;msavi&quot;, &quot;nbr&quot;, &quot;nbr2&quot;, &quot;ndmi&quot;, &quot;ndvi&quot;, &quot;savi&quot;) .snsf$SST.BIOCLIM &lt;- paste0(&quot;BIO&quot;, 1:19) # Codes pour les classes ------------------------ .snsf$NONFOR &lt;- 0 # non-forêt .snsf$PREGEN &lt;- 1 # régénération potentielle .snsf$REGEN &lt;- 2 # régénération .snsf$FOREST &lt;- 3 # forêt / forêt initiale # Divers ---------------------------------------- # Processeurs disponibles pour le calcul parallèle .snsf$CORES &lt;- detectCores() # Semence pour le générateur de nombres aléatoires .snsf$RSEED &lt;- 20191114 # Attacher l&#39;environnement attach(.snsf) # Message de bienvenue ======================================================= message(&quot; =&gt; chargé librairies et variables définit en .Rprofile .oPYo. o o .oPYo. ooooo ooooo 8 8b 8 8 8 8 `Yooo. 8`b 8 `Yooo. o8oo 8 .oPYo. .oPYo. .oPYo. `8 8 `b 8 `8 8 8 8 8 8 8 8 8 8 8 `b8 8 8 8 8 8 8 8 8 8 `YooP&#39; 8 `8 `YooP&#39; 8 8 `YooP&#39; `YooP8 `YooP&#39; :.....:..:::..:.....::..::::::::..:::.....::....8 :.....: :::::::::::::::::::::::::::::::::::::::::::::ooP&#39;.::::::: :::::::::::::::::::::::::::::::::::::::::::::...::::::::: \\n&quot;, date(), &quot;\\n&quot; ) "],
["00_STSS.html", "2 Système de Surveillance Terrestre", " 2 Système de Surveillance Terrestre Le Système de Surveillance Terrestre par Satellite (SSTS) consiste d’une base de données spatiales (images satellitaires, modèle numérique du terrain, données climatiques) d’un réseau de parcelles d’échantillonnage, dont la couverture des houppiers et l’utilisation des terres sont régulièrement enregistrées par photo-interprètes sur base des images satellitaires. Pour le moment, les attributs enregistrés dans le SSTS sont : Couverture des houppiers Occupation des terres Les répertoires et les fichiers sont structurés comme suit: 01_SSTS # SYSTÈME DE SURVEILLANCE TERRESTRE ========= ├── 01_data #:: images et autres données pré-traités -- └── ... ├── 02_BdD #:: base de données SSTS ------------------ ├── _src #:: scripts R ├── _create-grid.R # [R] création réseau SSTS ├── _create-train-plots.R # [R] création parcelles d&#39;entraînement └── _create-val-plots.R # [R] création parcelles de validation ├── 01_reseau-SSTS #:: réseau de parcelles SSTS ├── 02_train-plots #:: parcelles d&#39;entraînement └── 03_val-plots #:: parcelles de validation Actuellement, les données de base ainsi que les données générées par les photo-interprètes sont stockées dans des fichiers (tif pour les rasters et shp pour les données vecteurs). Il est prévu de gérer ces données SSTS dans une base de données géographiques (PostGIS) à l’avenir. Cette base de données est actuellement installé sur un serveur central au ministère de l’Environnement et des Forêts (MERF). "],
["01_Landsat.html", "2.1 Données de base", " 2.1 Données de base 2.1.1 Images Landsat Actuellement, l’analyse de l’évolution du couvert forestier est principalement basée sur les images satellitaires Landsat, qui sont disponibles gratuitement dans les archives de l’USGS. Les missions Landsat-4 à Landsat-8 produisent des images de résolution spatiale et radiométrique comparable depuis les années 1980. Les images brutes sont corrigées géométriquement et radiométriquement par l’USGS (USGS Collection 1 Level-2 Surface Reflection Product). La résolution spatiale des images est de 30 mètres. Les bandes spectrales utilisées sont B, G, R, NIR, SWIR-1 et SWIR-2 (voir les désignations des bandes dans la figure ci-dessous). Le territoire du Togo est couvert par un total de 9 images Landsat (scènes WRS 2). Les zones couvertes par les différentes scènes sont indiquées dans la figure ci-dessous. Les images Landsat utilisées pour l’évaluation du couvert forestier ont été prises idéalement à la fin de la saison sèche, c’est-à-dire de (Nov), Déc, Jan, (Fév) et sont disponibles en bonne qualité (sans nuages ou seulement légèrement couvertes par des nuages et des ombres) pour la même date sur l’ensemble du chemin WRS 2 respectif. Pour les années de référence, là ou on aimerait des cartes qui couvrent l’ensemble du territoire du Togo, des images correspondantes sont nécessaires pour tous les trois chemins WRS 2. Le tableau ci-dessous présente les images satellites utilisées pour l’analyse du couvert forestier au Togo. Les années de référence utilisées pour le NRF et les images correspondantes sont indiquées en caractères gras. Ce n’est que pour l’année 1991 que les images de différentes dates d’enregistrement ont été combinées pour le chemin WRS 193. L7* marque les images Landsat-7 avec des lacunes dans les données (SLC-off). La colonne GoogleEarth montre la répartition des dates des images de très haute résolution disponibles sur GoogleEarth. Seules les images de référence GoogleEarth de 2017 – 2018 ont été utilisées pour la calibration de la carte forêt/non-forêt 2018. WRS 192054,055,056 WRS 193052,053,054,055 WRS 194052,053 GoogleEarthRéférence 2019 L8 / 23.12.18 L8 / 16.02.19 L8 / 22.01.19 ++ 2018 L8 / 05.01.18 L8 / 12.01.18 L8 / 18.12.17 +++++++ 2017 L8 / 19.02.17 L8 / 25.01.17 L8 / 31.12.16 +++ 2016 — — — (+) 2015 L8 / 13.01.15 L8 / 04.01.15 L8 / 27.01.15 (+) 2014 — — — (++) 2013 L7* / 31.01.13 L7* / 23.02.13 — (+) 2012 — — L7* / 11.01.12 (++) 2011 L7* / 10.01.11 — — (+) 2010 — — L7* / 21.01.10 (+) 2009 — L7* / 27.01.09 — 2008 — — — 2007 L7* / 30.12.06 L7* / 22.01.07 L5 / 05.01.07 2006 — — — 2005 L7* / 24.12.04 L7* / 17.02.05 L7* / 22.12.04 2004 — — — 2003 L7 / 04.01.03 L7 / 26.12.02 L7 / 17.12.02 (.) 2002 — — — 2001 L7 / 13.12.00 — — (.) 2000 — L7 / 04.02.00 L7 / 26.01.00 … … … … 1997 — — L5 / 10.02.97 … … … … 1991 L4 / 03.01.91 L4 / 10.01.91 &amp; L5 / 28.11.89 — … … … … 1987 L5 / 31.12.86 L5 / 23.01.87 L5 / 29.12.86 1986 L5 / 13.01.86 L5 / 06.03.85 L5 / 11.01.86 2.1.1.1 Acquisition des images Ouvrir le site USGS Earthexplorer. Dans la fenêtre Search Criteria il faut selectionner la période pour laquelle on cherche des images (Nov - Jan). Dans la fenêtre Data Sets, les produits Landsat Level-2 (Surface Reflectance) sont séléctionnés. Dans la fenêtre Additional Criteria il faut choisir les scènes (chemin 192: 054-056 / chemin 193: 052-055 / chemin 194: 052-053). Parmi les images disponibles, on sélectionne celles qui sont disponibles à la même date et en bonne qualité pour l’ensemble du chemin WRS. On copie les identifier des images à télécharger dans un fichier txt. Ensuite on ouvre le site USGS ESPA pour commander les images choisi. On charge le fichier txt avec les identifier des images et on commande les bandes Surface Reflectance et les indices spéctrales (voir image au-dessous). Pour commander des images, il faut qu’on a un compte USGS. Une fois on est notifié par eMail que les images sont prêts, on les téléchargent manuellement ou tous ensemble avec le USGS bulkdownloader et la commande download_espa_order.py -u [nom d'utilisateur] -o ALL -d [répértoire]. On dézip les images et les rangent dans le répétoire data/Landsat sous la scène et l’année correspondante. Pour des images de l’hiver 2019/20, l’année correspondante est 2020. 2.1.1.2 Prétraitement des images Le premier traitement est la préparation des images Landsat et autres variables utilisés pour modéliser la surface forestier ou la biomasse aérienne comme les données topographique et climatiques. L’objectif est qu’on prépare avec les données brutes un jeu de données raster complet sur le même extent (territoir du Togo) et avec la même résolution spatiale de 30 mètres (résolution de base des images Landsat). On ouvre le script 1_prepare-images.R et on modifie la liste des images Landsat à utiliser (in.image.list), par exemple par ajouter les nouveaux image à considérer dans les analyses: p192.2019 = list( paste0(DATA.DIR, &quot;/Landsat/192_054/2019/LC081920542018122301T1-SC20190405164258/&quot;), paste0(DATA.DIR, &quot;/Landsat/192_055/2019/LC081920552018122301T1-SC20190405163359/&quot;), paste0(DATA.DIR, &quot;/Landsat/192_056/2019/LC081920562018122301T1-SC20190405163342/&quot;)) ... p193.2019 = list( paste0(DATA.DIR, &quot;/Landsat/193_052/2019/LC081930522019021601T1-SC20190405183839/&quot;), paste0(DATA.DIR, &quot;/Landsat/193_053/2019/LC081930532019021601T1-SC20190405181518/&quot;), paste0(DATA.DIR, &quot;/Landsat/193_054/2019/LC081930542019021601T1-SC20190405183609/&quot;), paste0(DATA.DIR, &quot;/Landsat/193_055/2019/LC081930552019021601T1-SC20190405181507/&quot;)) ... p194.2019 = list( paste0(DATA.DIR, &quot;/Landsat/194_052/2019/LC081940522019012201T1-SC20190405172019/&quot;), paste0(DATA.DIR, &quot;/Landsat/194_053/2019/LC081940532019012201T1-SC20190405172055/&quot;)) Outre la définition des images à traiter, le script définit une fonction prepare.image pour stacker les différentes bandes des images Landsat, pour les fusioner, masquer et couper les images Landsat chemin par chemin (WRS2 paths 192, 193 et 194 pour Togo). Par défaut, les images qui ont déjà été traité (filename existe déjà) ne sont plus traité (overwrite=FALSE). prepare.image(in.image.dirs, ext=NULL, filename=NULL, overwrite=FALSE, log=TRUE) Dans la deuxième partie du script, là où c’est noté # DO THE WORK ---------, on lance le traitement des images. Avec le code foreach(...) %dopar% { ... } on lance le traitement de chaque chemin pour chaque année sur des différents processeurs au parallèle. À la fin du script on transforme les images du chemin 194 du système de coordonnées UTM 30 vers UTM 31 produit des thumbnails des images Landsat Les images prétraités sont sauveguarder dans le répétoire input/1_images du projet, ensemble avec des Thumbnails des chemins. Dans une prochaîne étape, les images sont néttoyées de l’eau, nuages et ombres en utilisant les bandes Landsat de qualité des pixels. Example Images Landsat de l’année 2019: chemin p194 composé de 2 scénes du 22.01.2019 / p193 avec 4 scènes du 16.02.2019 / p192 avec 3 scènes du 23.12.2018 Script R: 01_SSTS/01_data/_src/prep-Landsat.R ############################################################################### # prep-Landsat.R: lire, nettoyer et empiler des images Landsat # ----------------------------------------------------------------------------- # Bern University of Applied Sciences # Oliver Gardi, &lt;oliver.gardi@bfh.ch&gt; # 13 Mai 2020 # Définitions des variables =================================================== IN.DIR &lt;- paste0(DIR.RAW.DAT, &quot;/Landsat&quot;) OUT.DIR &lt;- paste0(DIR.SST.DAT, &quot;/Landsat&quot;) if(!dir.exists(OUT.DIR)) dir.create(OUT.DIR) WRS &lt;- readOGR(paste0(IN.DIR, &quot;/WRS2/WRS2_descending.shp&quot;)) # Scènes WRS # Masques de nuages et d&#39;eau pour Landsat 4-7 et Landsat 8 ------------------ # voir les guides des produits de réflexion de surface Landsat 4-7 et Landsat 8 # https://www.usgs.gov/land-resources/nli/landsat/landsat-surface-reflectance QA.WATER &lt;- c( 68, 132, # Landsat 4-7 324, 388, 836, 900, 1348 # Landsat 8 ) QA.SHADOW &lt;- c( 72, 136, 328, 392, 840, 904, 1350 ) QA.ICE &lt;- c( 80, 112, 144, 176, 336, 368, 400, 432, 848, 880, 912, 944, 1352 ) QA.CLOUD &lt;- c( 96, 112, 160, 176, 224, 352, 368, 416, 432, 480, 864, 880, 928, 944, 992 ) # Liste des images à préparer et à fusionner ================================== in.image.list &lt;- list( # Chemin WRS p192 ----------------------------- p192.1986 = list(paste0(IN.DIR, &quot;/192_054/1986/LT051920541986011301T1-SC20190405164223/&quot;), paste0(IN.DIR, &quot;/192_055/1986/LT051920551986011301T1-SC20190405164227/&quot;), paste0(IN.DIR, &quot;/192_056/1986/LT051920561986011301T1-SC20190405164153/&quot;)), p192.1987 = list(paste0(IN.DIR, &quot;/192_054/1987/LT051920541986123101T1-SC20190405164150/&quot;), paste0(IN.DIR, &quot;/192_055/1987/LT051920551986123101T1-SC20190405163521/&quot;), paste0(IN.DIR, &quot;/192_056/1987/LT051920561986123101T1-SC20190405164444/&quot;)), p192.1991 = list(paste0(IN.DIR, &quot;/192_054/1991/LT041920541991010301T1-SC20190405164201/&quot;), paste0(IN.DIR, &quot;/192_055/1991/LT041920551991010301T1-SC20190405165911/&quot;), paste0(IN.DIR, &quot;/192_056/1991/LT041920561991010301T1-SC20190405163911/&quot;)), p192.2001 = list(paste0(IN.DIR, &quot;/192_054/2001/LE071920542000121301T1-SC20190405165521/&quot;), paste0(IN.DIR, &quot;/192_055/2001/LE071920552000121301T1-SC20190405165645/&quot;), paste0(IN.DIR, &quot;/192_056/2001/LE071920562000121301T1-SC20190405164029/&quot;)), p192.2003 = list(paste0(IN.DIR, &quot;/192_054/2003/LE071920542003010401T1-SC20190520111322/&quot;), paste0(IN.DIR, &quot;/192_055/2003/LE071920552003010401T1-SC20190520100402/&quot;), paste0(IN.DIR, &quot;/192_056/2003/LE071920562003010401T1-SC20190520100206/&quot;)), p192.2005 = list(paste0(IN.DIR, &quot;/192_054/2005/LE071920542004122401T1-SC20190405165520/&quot;), paste0(IN.DIR, &quot;/192_055/2005/LE071920552004122401T1-SC20190405164050/&quot;), paste0(IN.DIR, &quot;/192_056/2005/LE071920562004122401T1-SC20190405164030/&quot;)), p192.2007 = list(paste0(IN.DIR, &quot;/192_054/2007/LE071920542006123001T1-SC20190406034211/&quot;), paste0(IN.DIR, &quot;/192_055/2007/LE071920552006123001T1-SC20190406034231/&quot;), paste0(IN.DIR, &quot;/192_056/2007/LE071920562006123001T1-SC20190406034202/&quot;)), p192.2011 = list(paste0(IN.DIR, &quot;/192_054/2011/LE071920542011011001T1-SC20190406034214/&quot;), paste0(IN.DIR, &quot;/192_055/2011/LE071920552011011001T1-SC20190406034114/&quot;), paste0(IN.DIR, &quot;/192_056/2011/LE071920562011011001T1-SC20190406034155/&quot;)), p192.2013 = list(paste0(IN.DIR, &quot;/192_054/2013/LE071920542013013101T1-SC20190406034224/&quot;), paste0(IN.DIR, &quot;/192_055/2013/LE071920552013013101T1-SC20190406034046/&quot;), paste0(IN.DIR, &quot;/192_056/2013/LE071920562013013101T1-SC20190406034057/&quot;)), p192.2015 = list(paste0(IN.DIR, &quot;/192_054/2015/LC081920542015011301T1-SC20190405163446/&quot;), paste0(IN.DIR, &quot;/192_055/2015/LC081920552015011301T1-SC20190405163723/&quot;), paste0(IN.DIR, &quot;/192_056/2015/LC081920562015011301T1-SC20190405164231/&quot;)), p192.2017 = list(paste0(IN.DIR, &quot;/192_054/2017/LC081920542017021901T1-SC20190405163339/&quot;), paste0(IN.DIR, &quot;/192_055/2017/LC081920552017021901T1-SC20190405163342/&quot;), paste0(IN.DIR, &quot;/192_056/2017/LC081920562017021901T1-SC20190405163222/&quot;)), p192.2018 = list(paste0(IN.DIR, &quot;/192_054/2018/LC081920542018010501T1-SC20190405164304/&quot;), paste0(IN.DIR, &quot;/192_055/2018/LC081920552018010501T1-SC20190405163402/&quot;), paste0(IN.DIR, &quot;/192_056/2018/LC081920562018010501T1-SC20190405163250/&quot;)), p192.2019 = list(paste0(IN.DIR, &quot;/192_054/2019/LC081920542018122301T1-SC20190405164258/&quot;), paste0(IN.DIR, &quot;/192_055/2019/LC081920552018122301T1-SC20190405163359/&quot;), paste0(IN.DIR, &quot;/192_056/2019/LC081920562018122301T1-SC20190405163342/&quot;)), # Chemin WRS p193 ----------------------------- p193.1985 = list(paste0(IN.DIR, &quot;/193_052/1985/LT051930521985030601T1-SC20190520100259/&quot;), paste0(IN.DIR, &quot;/193_053/1985/LT051930531985030601T1-SC20190520100324/&quot;), paste0(IN.DIR, &quot;/193_054/1985/LT051930541985030601T1-SC20190520100340/&quot;), paste0(IN.DIR, &quot;/193_055/1985/LT051930551985030601T1-SC20190520100140/&quot;)), p193.1987 = list(paste0(IN.DIR, &quot;/193_052/1987/LT051930521987012301T1-SC20190405182322/&quot;), paste0(IN.DIR, &quot;/193_053/1987/LT051930531987012301T1-SC20190405182335/&quot;), paste0(IN.DIR, &quot;/193_054/1987/LT051930541987012301T1-SC20190405182331/&quot;), paste0(IN.DIR, &quot;/193_055/1987/LT051930551987012301T1-SC20190405182328/&quot;)), # Attention: là nous avons des images avec des dates différentes ! p193.1990.1 = list(paste0(IN.DIR, &quot;/193_052/1990/LT051930521989112801T1-SC20190520100201/&quot;), paste0(IN.DIR, &quot;/193_053/1990/LT051930531989112801T1-SC20190520100233/&quot;)), p193.1990.2 = list(paste0(IN.DIR, &quot;/193_054/1991/LT041930541991011001T1-SC20190402043117/&quot;), paste0(IN.DIR, &quot;/193_055/1991/LT041930551991011001T1-SC20190402042453/&quot;)), p193.2000 = list(paste0(IN.DIR, &quot;/193_052/2000/LE071930522000020401T1-SC20190520100729/&quot;), paste0(IN.DIR, &quot;/193_053/2000/LE071930532000020401T1-SC20190520100345/&quot;), paste0(IN.DIR, &quot;/193_054/2000/LE071930542000020401T1-SC20190402045232/&quot;), paste0(IN.DIR, &quot;/193_055/2000/LE071930552000020401T1-SC20190402043121/&quot;)), p193.2003 = list(paste0(IN.DIR, &quot;/193_052/2003/LE071930522002122601T1-SC20190405182352/&quot;), paste0(IN.DIR, &quot;/193_053/2003/LE071930532002122601T1-SC20190405182309/&quot;), paste0(IN.DIR, &quot;/193_054/2003/LE071930542002122601T1-SC20190405182226/&quot;), paste0(IN.DIR, &quot;/193_055/2003/LE071930552002122601T1-SC20190405190255/&quot;)), p193.2005 = list(paste0(IN.DIR, &quot;/193_052/2005/LE071930522005021701T1-SC20190405190117/&quot;), paste0(IN.DIR, &quot;/193_053/2005/LE071930532005021701T1-SC20190405190003/&quot;), paste0(IN.DIR, &quot;/193_054/2005/LE071930542005021701T1-SC20190405182210/&quot;), paste0(IN.DIR, &quot;/193_055/2005/LE071930552005021701T1-SC20190405190021/&quot;)), p193.2007 = list(paste0(IN.DIR, &quot;/193_052/2007/LE071930522007012201T1-SC20190405182221/&quot;), paste0(IN.DIR, &quot;/193_053/2007/LE071930532007012201T1-SC20190405182607/&quot;), paste0(IN.DIR, &quot;/193_054/2007/LE071930542007012201T1-SC20190405182139/&quot;), paste0(IN.DIR, &quot;/193_055/2007/LE071930552007012201T1-SC20190405182418/&quot;)), p193.2009 = list(paste0(IN.DIR, &quot;/193_052/2009/LE071930522009012701T1-SC20190405182143/&quot;), paste0(IN.DIR, &quot;/193_053/2009/LE071930532009012701T1-SC20190405182301/&quot;), paste0(IN.DIR, &quot;/193_054/2009/LE071930542009012701T1-SC20190405182103/&quot;), paste0(IN.DIR, &quot;/193_055/2009/LE071930552009012701T1-SC20190405182754/&quot;)), p193.2013 = list(paste0(IN.DIR, &quot;/193_052/2013/LE071930522013022301T1-SC20190405182200/&quot;), paste0(IN.DIR, &quot;/193_053/2013/LE071930532013022301T1-SC20190405182213/&quot;), paste0(IN.DIR, &quot;/193_054/2013/LE071930542013022301T1-SC20190405182152/&quot;), paste0(IN.DIR, &quot;/193_055/2013/LE071930552013022301T1-SC20190405182331/&quot;)), p193.2015 = list(paste0(IN.DIR, &quot;/193_052/2015/LC081930522015010401T1-SC20190405181512/&quot;), paste0(IN.DIR, &quot;/193_053/2015/LC081930532015010401T1-SC20190405181751/&quot;), paste0(IN.DIR, &quot;/193_054/2015/LC081930542015010401T1-SC20190402042510/&quot;), paste0(IN.DIR, &quot;/193_055/2015/LC081930552015010401T1-SC20190402042446/&quot;)), p193.2017 = list(paste0(IN.DIR, &quot;/193_052/2017/LC081930522017012501T1-SC20190405181511/&quot;), paste0(IN.DIR, &quot;/193_053/2017/LC081930532017012501T1-SC20190405181440/&quot;), paste0(IN.DIR, &quot;/193_054/2017/LC081930542017012501T1-SC20190405181458/&quot;), paste0(IN.DIR, &quot;/193_055/2017/LC081930552017012501T1-SC20190405181444/&quot;)), p193.2018 = list(paste0(IN.DIR, &quot;/193_052/2018/LC081930522018011201T1-SC20190405181524/&quot;), paste0(IN.DIR, &quot;/193_053/2018/LC081930532018011201T1-SC20190405181459/&quot;), paste0(IN.DIR, &quot;/193_054/2018/LC081930542018011201T1-SC20190405181510/&quot;), paste0(IN.DIR, &quot;/193_055/2018/LC081930552018011201T1-SC20190405181442/&quot;)), p193.2019 = list(paste0(IN.DIR, &quot;/193_052/2019/LC081930522019021601T1-SC20190405183839/&quot;), paste0(IN.DIR, &quot;/193_053/2019/LC081930532019021601T1-SC20190405181518/&quot;), paste0(IN.DIR, &quot;/193_054/2019/LC081930542019021601T1-SC20190405183609/&quot;), paste0(IN.DIR, &quot;/193_055/2019/LC081930552019021601T1-SC20190405181507/&quot;)), # Chemin WRS p194 ----------------------------- p194.1986 = list(paste0(IN.DIR, &quot;/194_052/1986/LT051940521986011101T1-SC20190405172804/&quot;), paste0(IN.DIR, &quot;/194_053/1986/LT051940531986011101T1-SC20190405172758/&quot;)), p194.1987 = list(paste0(IN.DIR, &quot;/194_052/1987/LT051940521986122901T1-SC20190405172903/&quot;), paste0(IN.DIR, &quot;/194_053/1987/LT051940531986122901T1-SC20190405174433/&quot;)), p194.1997 = list(paste0(IN.DIR, &quot;/194_052/1997/LT051940521997021001T1-SC20190405181746/&quot;), paste0(IN.DIR, &quot;/194_053/1997/LT051940531997021001T1-SC20190405173130/&quot;)), p194.2000 = list(paste0(IN.DIR, &quot;/194_052/2000/LE071940522000012601T1-SC20190405172721/&quot;), paste0(IN.DIR, &quot;/194_053/2000/LE071940532000012601T1-SC20190405172733/&quot;)), p194.2003 = list(paste0(IN.DIR, &quot;/194_052/2003/LE071940522002121701T1-SC20190405172823/&quot;), paste0(IN.DIR, &quot;/194_053/2003/LE071940532002121701T1-SC20190405172739/&quot;)), p194.2005 = list(paste0(IN.DIR, &quot;/194_052/2005/LE071940522004122201T1-SC20190405172700/&quot;), paste0(IN.DIR, &quot;/194_053/2005/LE071940532004122201T1-SC20190405172612/&quot;)), p194.2007 = list(paste0(IN.DIR, &quot;/194_052/2007/LT051940522007010501T1-SC20190405172919/&quot;), paste0(IN.DIR, &quot;/194_053/2007/LT051940532007010501T1-SC20190405172216/&quot;)), p194.2010 = list(paste0(IN.DIR, &quot;/194_052/2010/LE071940522010012101T1-SC20190405172745/&quot;), paste0(IN.DIR, &quot;/194_053/2010/LE071940532010012101T1-SC20190405173304/&quot;)), p194.2012 = list(paste0(IN.DIR, &quot;/194_052/2012/LE071940522012011101T1-SC20190405173146/&quot;), paste0(IN.DIR, &quot;/194_053/2012/LE071940532012011101T1-SC20190405172236/&quot;)), p194.2015 = list(paste0(IN.DIR, &quot;/194_052/2015/LC081940522015012701T1-SC20190405172055/&quot;), paste0(IN.DIR, &quot;/194_053/2015/LC081940532015012701T1-SC20190405172042/&quot;)), p194.2017 = list(paste0(IN.DIR, &quot;/194_052/2017/LC081940522016123101T1-SC20190405172058/&quot;), paste0(IN.DIR, &quot;/194_053/2017/LC081940532016123101T1-SC20190405172040/&quot;)), p194.2018 = list(paste0(IN.DIR, &quot;/194_052/2018/LC081940522017121801T1-SC20190405172038/&quot;), paste0(IN.DIR, &quot;/194_053/2018/LC081940532017121801T1-SC20190405174114/&quot;)), p194.2019 = list(paste0(IN.DIR, &quot;/194_052/2019/LC081940522019012201T1-SC20190405172019/&quot;), paste0(IN.DIR, &quot;/194_053/2019/LC081940532019012201T1-SC20190405172055/&quot;)) ) # Fonction pour traiter et fusionner un ensemble d&#39;images Landsat ============= # # @param in.image.dirs liste des répertoires des images à traiter # @param ext l&#39;étendue dà utiliser pour le recadrage des images # @param filename nom de fichier pour la sauvegarde de l&#39;image traitée # @param overwrite retraiter et écraser des images déjà existantes # @param log écrire les informations sur le processus dans un fichier # # @return objet raster de l&#39;image traitée (invisible) # prepare.landsat &lt;- function(in.image.dirs, ext=NULL, filename=NULL, overwrite=FALSE, log=TRUE) { # Charger le fichier, si le fichier existe et overwrite==FALSE if(!is.null(filename) &amp;&amp; file.exists(filename) &amp;&amp; overwrite==FALSE) { message(&quot;- loading from file &quot;, filename) out.image &lt;- stack(filename) } else { # Ouvrir le fichier journal si un nom de fichier et log==true if(!is.null(filename) &amp; log==TRUE) { dir.create(dirname(filename), recursive = TRUE, showWarnings = FALSE) logfile &lt;- file(sub(&quot;\\\\.[[:alnum:]]+$&quot;, &quot;.log&quot;, filename), open=&quot;wt&quot;) sink(logfile, type=&quot;output&quot;) sink(logfile, type=&quot;message&quot;) message(date()) } # Listes vides pour les images et des bandes de qualité images &lt;- list() qas &lt;- list() # Pour chaque image ... for(image.dir in in.image.dirs) { image.sensor &lt;- substr(basename(image.dir), 0,4) if(image.sensor==&quot;LC08&quot;) { regexp &lt;- &quot;^.*_(pixel_qa|band2|band3|band4|band5|band6|band7|evi|msavi|nbr|nbr2|ndmi|ndvi|savi).tif$&quot; } else { regexp &lt;- &quot;^.*_(pixel_qa|band1|band2|band3|band4|band5|band7|evi|msavi|nbr|nbr2|ndmi|ndvi|savi).tif$&quot; } image &lt;- stack(grep(regexp, dir(image.dir, full.names=TRUE), value=TRUE)) image.name &lt;- substr(names(image)[1], 1, 40) image.scene &lt;- paste0(substr(image.name, 11, 13), &quot;_&quot;, substr(image.name, 14, 16)) image.date &lt;- substr(image.name, 18, 21) image.path &lt;- as.numeric(substr(image.scene, 1, 3)) image.row &lt;- as.numeric(substr(image.scene, 5, 7)) # ... renommer les couches de l&#39;image names(image) &lt;- c(&quot;qa&quot;, SST.LSBANDS) message(&quot;- &quot;, image.name, &quot;: &quot;, appendLF = FALSE) # ... recadrer l&#39;image avec l&#39;étendue (le cas échéant) if(!is.null(ext)) { message(&quot;crop ext ... &quot;, appendLF = FALSE) image &lt;- crop(image, ext) } # ... recadrer et masquer avec WRS message(&quot;crop/mask WRS2 ... &quot;, appendLF = FALSE) wrs &lt;- spTransform(WRS[WRS$PATH==image.path &amp; WRS$ROW==image.row, ], CRS=crs(image)) image &lt;- mask(crop(image, wrs), wrs) # ... extraire la bande de qualité qa &lt;- image[[1]] image &lt;- dropLayer(image, 1) # ... supprimer les valeurs de réflectance non valides message(&quot;clean sr ... &quot;, appendLF = FALSE) for(i in 1:6) { image[[i]] &lt;- reclassify(image[[i]], cbind(-Inf, 0, NA), right=FALSE) image[[i]] &lt;- reclassify(image[[i]], cbind(10000, Inf, NA), right=TRUE) } # ... supprimer les valeurs d&#39;indices non valides for(i in 7:13) { image[[i]] &lt;- reclassify(image[[i]], cbind(-Inf, -10000, NA), right=FALSE) image[[i]] &lt;- reclassify(image[[i]], cbind(10000, Inf, NA), right=TRUE) } # ... mettre l&#39;ensemble de la pile à NA là où une seule couche est NA m &lt;- sum(image) image &lt;- mask(image, m) # ... ajouter l&#39;image et la bande de qualité aux listes correspondantes images[[length(images)+1]] &lt;- image qas[[length(qas)+1]] &lt;- qa } # Fusionner les images (scènes) dans les listes message(&quot;- merging scenes ... &quot;, appendLF = FALSE) out.image &lt;- do.call(merge, images) out.qa &lt;- do.call(merge, qas) # Sauvegarder l&#39;image fusionné dans un fichier if (!is.null(filename) &amp;&amp; (!file.exists(filename) || overwrite == TRUE)) { message(&quot;writing to file &quot;, filename, &quot; ... &quot;, appendLF = FALSE) out.image &lt;- writeRaster(out.image, filename = filename, overwrite = overwrite, datatype=&quot;INT2S&quot;, options=c(&quot;COMPRESS=NONE&quot;)) names(out.image) &lt;- SST.LSBANDS out.qa &lt;- writeRaster(out.qa, filename = sub(&quot;[.]tif$&quot;, paste0(&quot;_qa&quot;, image.sensor, &quot;.tif&quot;), filename), overwrite = overwrite, datatype=&quot;INT2S&quot;) } } message(&quot;done&quot;) print(out.image) # Fermer le fichier journal if(!is.null(filename) &amp; log==TRUE) { sink(type=&quot;output&quot;) sink(type=&quot;message&quot;) } # Retourner l&#39;image de manière invisible invisible(out.image) } # COMMENCER LE TRAITEMENT ##################################################### # Attention, peut facilement remplir le répertoire tmp ! # Peut-être seulement traiter une partie des images et ensuite redémarrer R # Étendue du Togo + 5km de tampon en UTM 30 pour le chemin p194 TGO.EXT.30 &lt;- extent(spTransform(TGO, UTM.30)) + 10000 # Pour chaque ensembles d&#39;images (année/chemin), en parallèle, ... registerDoParallel(CORES-1) foreach(i=1:length(in.image.list)) %dopar% { # ... extraire la liste des images à traiter in.image.dirs &lt;- in.image.list[[i]] name &lt;- unlist(strsplit(names(in.image.list[i]), &quot;[.]&quot;)) path &lt;- name[1] # p.ex. &quot;p192&quot; year &lt;- name[2] # p.ex. &quot;1986&quot; tile &lt;- name[3] # p.ex. &quot;NA&quot; (ou 1,2, ...) # ... étendue UTM 30 pour chemin p194 et UTM 31 pour les autres if(path == &quot;p194&quot;) tmp.ext &lt;- TGO.EXT.30 else tmp.ext &lt;- TGO.EXT # ... répertoire pour sauvegarder l&#39;image out.image.dir &lt;- paste0(OUT.DIR, &quot;/&quot;, path) if(!dir.exists(out.image.dir)) dir.create(out.image.dir) # ... nom du fichier if(is.na(tile)) { filename &lt;- paste0(out.image.dir, &quot;/&quot;, path, &quot;_&quot;, year, &quot;.tif&quot;) } else { filename &lt;- paste0(out.image.dir, &quot;/&quot;, path, &quot;_&quot;, year, &quot;_&quot;, tile, &quot;.tif&quot;) } # ... et faire le travail message(&quot;Processing &quot;, path, &quot;_&quot;, year) prepare.landsat(in.image.dirs, ext = tmp.ext, filename = filename, overwrite=FALSE, log=TRUE) } # Supprimer les fichiers temporaires dans le répertoir tmp tmp_dir &lt;- tempdir() files &lt;- list.files(tmp_dir, full.names = T, recursive=T) file.remove(files) # Fusionner les fichiers journaux for(dir in dir(paste0(OUT.DIR), full.names=TRUE)) { path &lt;- basename(dir) system(paste0(&quot;tail -n +1 &quot;, dir, &quot;/*.log &gt; &quot;, dir, &quot;/&quot;, path, &quot;.tmp&quot;)) system(paste0(&quot;rm &quot;, dir, &quot;/*.log&quot;)) system(paste0(&quot;mv &quot;, dir, &quot;/&quot;, path, &quot;.tmp &quot;, dir, &quot;/&quot;, path, &quot;.log&quot;)) } # Reprojection des images p194 de UTM 30 à UTM 31 ============================= p194.dir &lt;- paste0(OUT.DIR, &quot;/p194&quot;) # Pour chaque image p194, en parallèle, ... registerDoParallel(CORES-1) foreach(image=dir(p194.dir, pattern=&quot;.*[.]tif$&quot;)) %dopar% { image &lt;- paste0(p194.dir, &quot;/&quot;, image) image.utm30 &lt;- sub(&quot;[.]tif$&quot;, &quot;utm30.tif&quot;, image) file.rename(image, image.utm30) # ... transformer l&#39;image en utilisant l&#39;outil externe &quot;gdalwarp&quot; system(paste(&quot;gdalwarp&quot;, image.utm30, &quot;-t_srs &#39;+proj=utm +zone=31 +datum=WGS84&#39;&quot;, &quot;-tr 30 30&quot;, &quot;-te 147255 1017495 222165 1238265&quot;, image, &quot;-ot &#39;Int16&#39;&quot;, &quot;-overwrite&quot;)) # ... supprimer l&#39;image UTM 30 file.remove(image.utm30) } # Masquer images avec bande de qualité (nuages/ombres) ======================== registerDoParallel(CORES-1) foreach(file=dir(OUT.DIR, pattern=&quot;.*\\\\_[[:digit:]]+\\\\.tif$&quot;, full.names = TRUE, recursive=TRUE)) %dopar% { qa &lt;- raster(dir(dirname(file), pattern=gsub(&quot;\\\\_&quot;, &quot;\\\\_&quot;, sub(&quot;\\\\.tif&quot;, &quot;_qa*&quot;, basename(file))), full.names=TRUE)) image &lt;- mask(brick(file), qa %in% c(QA.CLOUD, QA.SHADOW, QA.WATER, QA.ICE), maskvalue=TRUE) writeRaster(image, sub(&quot;\\\\.tif&quot;, &quot;_m.tif&quot;, file), overwrite = TRUE, datatype=&quot;INT2S&quot;, options=c(&quot;COMPRESS=NONE&quot;)) } # Créer des vignettes de chaque image ========================================= registerDoParallel(CORES-1) foreach(filename=dir(OUT.DIR, pattern=&quot;p19.*[.]tif$&quot;, recursive=TRUE, full.names=TRUE)) %dopar% { image &lt;- brick(filename) jpeg(sub(&quot;[.]tif$&quot;, &quot;.jpeg&quot;, filename), width=1350, height=3000) par(plt=c(0,1,0,1)) plot(spTransform(TGO, UTM.31)) plotRGB(image, r=6, g=5, b=3, stretch=&quot;lin&quot;, add=TRUE) plot(mask(image[[1]], spTransform(TGO, UTM.31), inverse=TRUE), col=&quot;#FFFFFF66&quot;, legend=FALSE, add=TRUE) plot(spTransform(TGO, UTM.31), add=TRUE, lwd=3) dev.off() } "],
["02_Worldclim.html", "", " 2.1.2 WorldClim Les variables climatiques ont une grande influence sur les caractéristiques des forêts. Ils sont donc importants pour l’évaluation des surfaces forestières et de la biomasse sur la base d’images satellitaires. Les données climatiques de WorldClim version 2 ont servi comme base. La température annuelle moyenne (BIO1), la saisonnalité de la température (BIO4), la précipitation annuelle (BIO12) et la saisonnalité de la précipitation (BIO15) étant identifiées comme les variables WorldClim les plus importantes pour l’évaluation des forêts. 2.1.2.1 Acquisition des données Des données climatiques historiques (1970 – 2020) avec une résolution de 30 arcsecondes (environ 1 km2 sur l’équateur) disponible à WorldClim version 2 ont été utilisées. Pour l’analyse, des variables climatiques mensuelles ont été utilisées : prec: Précipitations pour les mois de janvier à décembre (mm) tmax: Température maximale pour les mois de janvier à décembre (ºC) tavg: Température moyenne pour les mois de janvier à décembre (ºC) tmin: Température minimale pour les mois de janvier à décembre (ºC) Et également les variables bioclimatiques qui en découlent : BIO1: Température moyenne annuelle BIO2: Fourchette diurne moyenne (moyenne des températures mensuelles (max temp - min temp)) BIO3: Isothermie (BIO2/BIO7) (×100) BIO4: Saisonnalité de la température (écart-type ×100) BIO5: Température maximale du mois le plus chaud BIO6: Température minimale du mois le plus froid BIO7: Gamme de température annuelle (BIO5-BIO6) BIO8: Température moyenne du trimestre le plus humide BIO9: Température moyenne du trimestre le plus sec BIO10: Température moyenne du trimestre le plus chaud BIO11: Température moyenne du trimestre le plus froid BIO12: Précipitation annuelle BIO13: Précipitation du mois le plus humide BIO14: Précipitation du mois le plus sec BIO15: Saisonalité de la précipitation (Coefficient de variation) BIO16: Précipitation du trimestre le plus humide BIO17: Précipitation du trimestre le plus sec BIO18: Précipitation du trimestre le plus chaud BIO19: Précipitation du trimestre le plus froid 2.1.2.2 Prétraitement des données Les données sont lues et reprojetées sur le raster des images Landsat (UTM 31, résolution de 30 mètres) et coupées à la taille. Example Données bioclimatiques WorldClim version 2: température annuelle moyenne (BIO1) / saisonalité de la température (BIO4) / précipitation annuelle (BIO12) / saisonalité de la précipitation (BIO15) Script R: 01_SSTS/01_data/_src/prep-Worldclim.R ############################################################################### # prep-Worldclim.R: lire et reprojeter les données de WorldClim # ----------------------------------------------------------------------------- # Bern University of Applied Sciences # Oliver Gardi, &lt;oliver.gardi@bfh.ch&gt; # 13 Mai 2020 # Définitions des variables =================================================== IN.DIR &lt;- paste0(DIR.RAW.DAT, &quot;/Worldclim&quot;) OUT.DIR &lt;- paste0(DIR.SST.DAT, &quot;/Worldclim&quot;) if(!dir.exists(OUT.DIR)) dir.create(OUT.DIR) # Reprojection images WorldClim vers Landsat (résolution 30m, UTM 31, ...) ==== foreach(file=dir(IN.DIR, pattern=&quot;.*Togo[.]tif$&quot;)) %dopar% { system(paste(&quot;gdalwarp&quot;, paste0(IN.DIR, &quot;/&quot;, file), &quot;-t_srs &#39;+proj=utm +zone=31 +datum=WGS84&#39;&quot;, &quot;-tr 30 30&quot;, paste(&quot;-te&quot;, TGO.EXT@xmin, TGO.EXT@ymin, TGO.EXT@xmax, TGO.EXT@ymax), paste0(OUT.DIR, &quot;/&quot;, file), &quot;-dstnodata -3.4e+38&quot;, &quot;-co COMPRESS=&#39;LZW&#39;&quot;, &quot;-co INTERLEAVE=&#39;BAND&#39;&quot;, &quot;-overwrite&quot;)) system(paste(&quot;gdalinfo -stats&quot;, paste0(OUT.DIR, &quot;/&quot;, file))) } # Créer des vignettes pour les donées WorldClim =============================== foreach(file=dir(OUT.DIR, pattern=&quot;.*[.]tif$&quot;)) %dopar% { image &lt;- stack(paste0(OUT.DIR, &quot;/&quot;, file)) type &lt;- unlist(strsplit(file, &quot;_&quot;))[3] if (type == &quot;prec&quot;) { zlim &lt;- c(0,320); col &lt;- rev(topo.colors(255)) } else if (type == &quot;tmin&quot;) { zlim &lt;- c(14.0,27.8); col &lt;- rev(heat.colors(255)) } else if (type == &quot;tmax&quot;) { zlim &lt;- c(24.9,37.5); col &lt;- rev(heat.colors(255)) } else if (type == &quot;tavg&quot;) { zlim &lt;- c(19.7,32.7); col &lt;- rev(heat.colors(255)) } else { zlim &lt;- NA; col &lt;- rev(cm.colors(255)) } foreach(i=1:nlayers(image)) %dopar% { jpeg(paste0(OUT.DIR, &quot;/&quot;, sub(&quot;[.]tif$&quot;, &quot;&quot;, file), &quot;-&quot;, str_pad(i, 2, &quot;left&quot;, 0), &quot;.jpeg&quot;), width=1350, height=3000) plot(image[[i]], col=col, zlim=zlim) plot(mask(image[[i]], TGO, inverse=TRUE), col=&quot;#FFFFFF66&quot;, legend=FALSE, add=TRUE) plot(TGO, add=TRUE, lwd=3) dev.off() } } "],
["03_SRTM.html", "", " 2.1.3 SRTM Les variables topographiques telles que l’élévation, la pente et l’aspect ne sont actuellement utilisées ni pour l’évaluation des forestières ni pour l’évaluation de la biomasse, car il a été constaté que ces variables ont peu d’influence sur le pouvoir explicatif des modèles de classification ou de régression. Néanmoins, les données sont conservées et disponibles pour autres analyses. 2.1.3.1 Acquisition des données Les données topographiques du SRTM ont été obtenues à partir de deux sources: Les données originales avec une résolution spatiale de 1 seconde d’arc (environ 30 mètres sur l’équateur) ont été obtenues du jeu de données SRTM 1 Arc-Second Global disponible sur USGS Earthexplorer. Ces données à haute résolution ont des fois des lacunes (manque des données). Les données SRTM ajustées avec une résolution de 3 secondes d’arc ont été obtenues à partir de CGIAR SRTM 90m Digital Elevation Database v4.1 pour combler les lacunes des données à haute résolution. 2.1.3.2 Prétraitement des données Les données de 1 seconde d’arc sont lues et les lacunes éventuelles sont comblées avec les données ajustées de 3 secondes d’arc. Enfin, les données sont reprojetées sur le raster des images Landsat (UTM 31, résolution de 30 mètres) et coupées à la taille. Script R: 01_SSTS/01_data/_src/prep-SRTM.R ############################################################################### # prep-SRTM.R: lire, nettoyer et empiler des données topographiques SRTM # ----------------------------------------------------------------------------- # Bern University of Applied Sciences # Oliver Gardi, &lt;oliver.gardi@bfh.ch&gt; # 13 Mai 2020 # Définitions des variables =================================================== IN.DIR &lt;- paste0(DIR.RAW.DAT, &quot;/SRTM&quot;) OUT.DIR &lt;- paste0(DIR.SST.DAT, &quot;/SRTM&quot;) if(!dir.exists(OUT.DIR)) dir.create(OUT.DIR) # Préparation du modèle numérique d&#39;élévation SRTM ============================ # Lire 90m SRTM MNE sans vides (source: http://srtm.csi.cgiar.org/srtmdata/), dem.90 &lt;- do.call(merge, lapply(as.list(dir(paste0(IN.DIR, &quot;/3arcsecond&quot;), pattern=&quot;.*[.]tif$&quot;, full.names=TRUE)), raster)) # Lire 30m SRTM MNE (source: USGS Earthexplorer) et remplir vides avec 90m SRTM dem.30 &lt;- foreach(tile=dir(paste0(IN.DIR, &quot;/1arcsecond&quot;), pattern=&quot;.*[.]tif$&quot;, full.names=TRUE), .combine=merge, .multicombine=TRUE) %dopar% { dem.30.t &lt;- raster(tile) dem.90.t &lt;- round(projectRaster(dem.90, dem.30.t)) merge(dem.30.t, dem.90.t) } # Reprojection d&#39;images MNE vers Landsat (résolution 30m, UTM 31, ...) writeRaster(dem.30, paste0(OUT.DIR, &quot;/SRTM-1arcsec_raw.tif&quot;), datatype=&quot;INT2S&quot;, overwrite=TRUE) system(paste(&quot;gdalwarp&quot;, paste0(OUT.DIR, &quot;/SRTM-1arcsec_raw.tif&quot;), &quot;-t_srs &#39;+proj=utm +zone=31 +datum=WGS84&#39;&quot;, &quot;-tr 30 30&quot;, paste(&quot;-te&quot;, TGO.EXT@xmin, TGO.EXT@ymin, TGO.EXT@xmax, TGO.EXT@ymax), paste0(OUT.DIR, &quot;/SRTM-1arcsec.tif&quot;), &quot;-ot &#39;Int16&#39;&quot;, &quot;-co COMPRESS=&#39;LZW&#39;&quot;, &quot;-co INTERLEAVE=&#39;BAND&#39;&quot;, &quot;-overwrite&quot;)) file.remove(paste0(OUT.DIR, &quot;/SRTM-1arcsec_raw.tif&quot;)) # Calculer les statistiques GDAL (min, max, ...) system(paste(&quot;gdalinfo -stats&quot;, paste0(OUT.DIR, &quot;/SRTM-1arcsec.tif&quot;))) # Créer une vignettes du MNE ================================================== dem &lt;- raster(paste0(OUT.DIR, &quot;/SRTM-1arcsec.tif&quot;)) jpeg(paste0(OUT.DIR, &quot;/SRTM-1arcsec.jpeg&quot;), width=1350, height=3000) par(plt=c(0,1,0,1)) plot(dem) plot(mask(dem, TGO, inverse=TRUE), col=&quot;#FFFFFF66&quot;, legend=FALSE, add=TRUE) plot(TGO, add=TRUE, lwd=3) dev.off() "],
["04_SoilGrids.html", "", " 2.1.4 SoilGrids Les données SoilGrids sur le type de sol et le carbone du sol n’ont pas encore été incluses dans les analyses de la surface forestière et des modifications du stockage du carbone dues au changement d’utilisation des terres. Ils sont disponibles pour des analyses complémentaires. 2.1.4.1 Acquisition des données Les données sur les sols proviennent des archives SoilGrids (documentation SoilGrids 2017) 2.1.4.2 Prétraitement des données Les données sont lues et reprojetées sur le raster des images Landsat (UTM 31, résolution de 30 mètres) et coupées à la taille. Example Données pédologiques SoilGrids: Typologie des sols WRB à gauche, Carbone du sol à 30 cm de profondeur (tC/ha) à droite Script R: 01_SSTS/01_data/_src/prep-SoilGrids.R ############################################################################### # prep-SoilGrids.R: lire et reprojeter des données SoilGrids # ----------------------------------------------------------------------------- # Bern University of Applied Sciences # Oliver Gardi, &lt;oliver.gardi@bfh.ch&gt; # 13 Mai 2020 # Source des données: https://files.isric.org/soilgrids/data/recent/ # Documentation: https://www.isric.org/explore/soilgrids/faq-soilgrids-2017 # Définitions des variables =================================================== IN.DIR &lt;- paste0(DIR.RAW.DAT, &quot;/SoilGrids&quot;) OUT.DIR &lt;- paste0(DIR.SST.DAT, &quot;/SoilGrids&quot;) if(!dir.exists(OUT.DIR)) dir.create(OUT.DIR) # Reprojection oilGrids vers Landsat (résolution 30m, UTM 31, ...) ============ foreach(file=dir(IN.DIR, pattern=&quot;.*[.]tif$&quot;)) %do% { system(paste(&quot;gdalwarp&quot;, paste0(IN.DIR, &quot;/&quot;, file), &quot;-t_srs &#39;+proj=utm +zone=31 +datum=WGS84&#39;&quot;, &quot;-tr 30 30&quot;, paste(&quot;-te&quot;, TGO.EXT@xmin, TGO.EXT@ymin, TGO.EXT@xmax, TGO.EXT@ymax), paste0(OUT.DIR, &quot;/&quot;, file), &quot;-dstnodata -3.4e+38&quot;, &quot;-co COMPRESS=&#39;LZW&#39;&quot;, &quot;-co INTERLEAVE=&#39;BAND&#39;&quot;, &quot;-overwrite&quot;)) system(paste(&quot;gdalinfo -stats&quot;, paste0(OUT.DIR, &quot;/&quot;, file))) } # Créer des vignettes pour les donées SoilGrids =============================== # Types de sol WRB ------------------------------ jpeg(paste0(OUT.DIR, &quot;/TAXNWRB_250m_ll.jpeg&quot;), width=675, height=1200) soil.types &lt;- raster(paste0(OUT.DIR, &quot;/TAXNWRB_250m_ll.tif&quot;)) soil.types &lt;- as.factor(soil.types) cats &lt;- levels(soil.types)[[1]] attr &lt;- read.csv(paste0(IN.DIR, &quot;/TAXNWRB_250m_ll.tif.csv&quot;)) attr &lt;- merge(cats, attr[,c(&quot;Number&quot;, &quot;Group&quot;, &quot;WRB_group&quot;, &quot;R&quot;, &quot;G&quot;, &quot;B&quot;)], by.x = &quot;ID&quot;, by.y = &quot;Number&quot;) cats[[&quot;Soil Type&quot;]] &lt;- attr$Group levels(soil.types) &lt;- cats levelplot(soil.types, col.regions=paste0(&quot;#&quot;, as.hexmode(tmp$R),as.hexmode(tmp$G),as.hexmode(tmp$B)), xlab=&quot;&quot;, ylab=&quot;&quot;) + levelplot(mask(soil.types, TGO, inverse=TRUE), col.regions=&quot;#FFFFFF66&quot;) + layer(sp.polygons(TGO, lwd=3)) dev.off() # Réservoire carbone à 30 cm -------------------- jpeg(paste0(OUT.DIR, &quot;/OCSTHA_M_30cm_250m_ll.jpeg&quot;), width=675, height=1200) image &lt;- raster(paste0(OUT.DIR, &quot;/OCSTHA_M_30cm_250m_ll.tif&quot;)) raster::plot(image, zlim=c(30,100)) raster::plot(mask(image, TGO, inverse=TRUE), col=&quot;#FFFFFF66&quot;, legend=FALSE, add=TRUE) sp::plot(TGO, add=TRUE, lwd=3) dev.off() # Tableau des surfaces des catégories de sol selon GIEC ====================== tmp &lt;- as.data.frame(table(mask(soil.types, TGO)[]) * 30^2/10000) tmp &lt;- merge(tmp, attr, by.x=&quot;Var1&quot;, by.y=&quot;ID&quot;) tmp$IPCC &lt;- NA tmp$IPCC[tmp$WRB_group %in% c(&quot;Leptosols&quot;,&quot;Vertisols&quot;,&quot;Kastanozems&quot;,&quot;Chernozems&quot;, &quot;Phaeozems&quot;,&quot;Luvisols&quot;,&quot;Alisols&quot;,&quot;Albeluvisols&quot;, &quot;Solonetz&quot;,&quot;Calcisols&quot;,&quot;Gypsisols&quot;,&quot;Umbrisols&quot;, &quot;Cambisols&quot;,&quot;Regosols&quot;)] &lt;- &quot;HAC&quot; tmp$IPCC[tmp$WRB_group %in% c(&quot;Acrisols&quot;,&quot;Lixisols&quot;,&quot;Nitisols&quot;,&quot;Ferralsols&quot;, &quot;Durisols&quot;)] &lt;- &quot;LAC&quot; tmp$IPCC[tmp$WRB_group %in% c(&quot;Arenosols&quot;)] &lt;- &quot;SAN&quot; tmp$IPCC[tmp$WRB_group %in% c(&quot;Podzols&quot;)] &lt;- &quot;POD&quot; tmp$IPCC[tmp$WRB_group %in% c(&quot;Andosols&quot;)] &lt;- &quot;VOL&quot; tmp$IPCC[tmp$WRB_group %in% c(&quot;Gleysols&quot;)] &lt;- &quot;WET&quot; tmp$IPCC[is.na(tmp$WRB_group)] &lt;- &quot;Other&quot; "],
["05_ProREDD.html", "", " 2.1.5 ProREDD Dans le cadre du premier inventaire forestier national de 2015/16, le projet GIZ “ProREDD” a réalisé une carte d’occupation des sols à partir d’images RapidEye des années 2013/14 (Rapport méthodes et résultats). La carte n’est pas utilisée pour l’analyse de l’évolution des surfaces forestières dans le cadre du NRF-MRV REDD+, mais elle est disponible comme carte comparative et pour d’autres analyses. 2.1.5.1 Acquisition des données Les cartes pour les différentes régions ont été fournies par L’Unité de gestion de bases de données cartographiques (UGBDC) de la Direction des études et de la planification (DEP) le ministère sous forme de Shapefiles. 2.1.5.2 Prétraitement des données Les shapefiles sont lus et convertis avec l’outil “grid_gridding” de SAGA GIS en données raster avec résolution originale des images RapidEye de 5 mètres, puis reprojetés sur le raster Landsat de 30 mètres. Example Script R: 01_SSTS/01_data/_src/prep-ProREDD.R ############################################################################### # prep-ProREDD.R: rasterizer la carte d&#39;occupation des terres RapidEye # ----------------------------------------------------------------------------- # Bern University of Applied Sciences # Oliver Gardi, &lt;oliver.gardi@bfh.ch&gt; # 13 Mai 2020 # Définitions des variables =================================================== IN.DIR &lt;- paste0(DIR.RAW.DAT, &quot;/RapidEye/shapefiles&quot;) OUT.DIR &lt;- paste0(DIR.SST.DAT, &quot;/ProREDD&quot;) if(!dir.exists(OUT.DIR)) dir.create(OUT.DIR) # Rasterizer Shapefiles avec l&#39;outil grid_gridding de SAGA ==================== files &lt;- dir(IN.DIR, pattern=&quot;\\\\.shp$&quot;, recursive=TRUE, full.names=TRUE) registerDoParallel(CORES - 1) foreach(file=files) %dopar%{ out.file &lt;- sub(&quot;shp$&quot;, &quot;sdat&quot;, file) system(paste0(&quot;saga_cmd grid_gridding \\&quot;Shapes to Grid\\&quot; &quot;, &quot;-TARGET_DEFINITION 0 -INPUT \\&quot;&quot;, file, &quot;\\&quot; &quot;, &quot;-FIELD \\&quot;code\\&quot; -OUTPUT 2 -MULTIPLE 0 -LINE_TYPE 0 &quot;, &quot;-POLY_TYPE 0 -GRID_TYPE 2 -TARGET_USER_SIZE 30.0 &quot;, &quot;-TARGET_USER_FITS 0 -GRID \\&quot;&quot;, out.file, &quot;\\&quot;&quot;)) } # Lire et fusionner les différentes scènes raster scenes &lt;- lapply(dir(IN.DIR, pattern=&quot;\\\\.sdat$&quot;, recursive=TRUE, full.names=TRUE), raster) scenes[&quot;tolerance&quot;] &lt;- 0.4 RE &lt;- do.call(merge, scenes) # À reviser: Reprojection sur l&#39;image Landsat AGB &lt;- raster(&quot;../output/3_forest-biomass/2_ref-maps/TGO_2015_AGB.tif&quot;) RE_resampled &lt;- resample(RE, AGB, method=&quot;ngb&quot;) writeRaster(RE_resampled, paste0(OUT.DIR, &quot;/TGO_30m.tif&quot;), datatype=&quot;INT2S&quot;) "],
["01_sampling-grid.html", "2.2 Collecte des données", " 2.2 Collecte des données 2.2.1 Réseau d’échantillonnage Les parcelles d’échantillonage couvrent une surface de 30 x 30 mètres et correspondent en taille, forme et position aux pixels des images Landsat. Les parcelles d’échantillonnage sont régulièrement réparties sur le territoire du Togo, sur une grille d’une taille de maille de 480 mètres (tous les 16 pixels Landsat) ou 4,3 parcelles d’échantillonnage par km2. Sur l’ensemble du territoire togolais, cela donne un réseau de 250 000 parcelles. Si nécessaire pour autres utilisation, la taille de maille de 480 mètres permet d’enlargir la maille à 679 ou 960 mètres. D’autre part, il est également possible de comprimer la maille à 340, 240, 170, 120, 85, … mètres. La grille d’échantillonnage est basée sur l’étendue du Togo (alignée avec la grille Landsat) et la taille du maillage. Les points d’échantillonnage résultants sont attribués avec les coordonnées x et y et l’ID correspondant. La grille de points résultante est enregistrée sous forme de shapefile (télécharger archive ZIP). Script R: 01_SSTS/02_BdD/_src/create-grid.R ############################################################################### # create-grid.R: créer une grille de points d&#39;observation SSTS # ----------------------------------------------------------------------------- # Bern University of Applied Sciences # Oliver Gardi, &lt;oliver.gardi@bfh.ch&gt; # 13 Mai 2020 # Définitions des variables =================================================== OUT.DIR &lt;- paste0(DIR.SST.BDD, &quot;/01_reseau-SSTS&quot;) if(!dir.exists(OUT.DIR)) dir.create(OUT.DIR) RES &lt;- 480 # Résolution de la grille (mètres) # Grille d&#39;observation ======================================================== # Coordonnées min/max en ligne avec images Landsat x.min &lt;- RES * extent(TGO)@xmin %/% RES x.max &lt;- RES * extent(TGO)@xmax %/% RES y.min &lt;- RES * extent(TGO)@ymin %/% RES y.max &lt;- RES * extent(TGO)@ymax %/% RES # Creation de la grille frame.points &lt;- SpatialPoints(expand.grid(seq(x.min, x.max, by=RES), seq(y.min, y.max, by=RES)), proj4string=UTM.31)[TGO] # Ajouter attribues PLOTID, xcoords et ycoords frame.points$xcoords &lt;- frame.points@coords[,1] frame.points$ycoords &lt;- frame.points@coords[,2] frame.points$PLOTID &lt;- paste0(str_pad(frame.points@xcoords, 7, &quot;left&quot;, &quot;0&quot;), &quot;_&quot;, str_pad(frame.points@ycoords, 7, &quot;left&quot;, &quot;0&quot;)) # Sauveguarder comme fichier Shapefile writeOGR(frame.points, dsn=OUT.DIR, layer=&quot;TGO_frame_480m&quot;, driver=&quot;ESRI Shapefile&quot;, overwrite=TRUE) "],
["02_training-plots.html", "", " 2.2.2 Parcelles d’entraînement 2.2.2.1 Échantillonnage Les parcelles d’entraînement sont sélectionnées de manière à couvrir la plus large gamme possible de couverture végétale sur l’ensemble du territoire du Togo. À cette fin, le NDVI comme indicateur du couvert végétal est attribué à tout les points du réseau d’échantillonnage et stratifiées en 10 classes NDVI. Un échantillon aléatoire spatialement équilibré de 1 500 échantillons est ensuite tiré de chaque strate NDVI (generalized random tesselation). 2.2.2.2 Couverture des houppiers Pour détérminer la couverture des houppiers une grille de 7 x 7 points est définit à l’intérieur de chaque parcelles d’échantillonage. Pour chaque point de ce grille, les photo-interprètes détérminent sur base des images de très haute résolution disponible en GoogleEarth, si le point touche l’houppier d’un arbre ou non. La couverture des houppiers est en suite détérminé par le nombre des points qui tombent sur un arbre par rapport au nombre total des points (n = 49). L’attribution est fait par les photo-interprètes en QGIS, avec l’image GoogleEarth comme carte de base. La source et la date d’acquistion de l’image GoogleEarth utilisé est également enregistré. Elle est obtenu par GoogleEarth Pro (plugin QGIS send2google_earth). Script R: 01_SSTS/02_BdD/_src/create-train-plots.R ############################################################################### # create-train-plots.R: Créer un ensemble de parcelles d&#39;entraînement # ----------------------------------------------------------------------------- # Bern University of Applied Sciences # Oliver Gardi, &lt;oliver.gardi@bfh.ch&gt; # 13 Mai 2020 # Préparation des variables =================================================== OUT.DIR &lt;- paste0(DIR.SST.BDD, &quot;/02_train-plots/empty&quot;) if(!dir.exists(OUT.DIR)) dir.create(OUT.DIR, recursive=TRUE) # Charger la grille d&#39;échantillonnage du SSTS frame.points &lt;- readOGR(paste0(DIR.SST.BDD, &quot;/01_reseau-SSTS/TGO_frame_480m.shp&quot;)) # Charger NDVI 2018 masquée (sans nuages, ombre, ...) ndvi.band &lt;- which(SST.LSBANDS == &quot;ndvi&quot;) ndvi &lt;- merge(raster(paste0(DIR.SST.DAT, &quot;/Landsat/p192/p192_2018_m.tif&quot;), band=ndvi.band), raster(paste0(DIR.SST.DAT, &quot;/Landsat/p193/p193_2018_m.tif&quot;), band=ndvi.band), raster(paste0(DIR.SST.DAT, &quot;/Landsat/p194/p194_2018_m.tif&quot;), band=ndvi.band)) # Extraire le NDVI pour chaque parcelle et le découper en 10 strates frame.points$ndvi &lt;- raster::extract(ndvi, frame.points) frame.points$ndvi_c &lt;- cut(frame.points$ndvi, 10, labels=paste0(&quot;s&quot;, 0:9)) # Echantillonnage des parcelles d&#39;entraînement ================================ # Initialiser le générateur de nombres aléatoires set.seed(RSEED) # Définition d&#39;échantillonnage (1500 échantillons par strate NDVI s0 - s9) Dsgn.grt &lt;- list(&quot;s0&quot;=list(panel=c(PanelOne=1500), seltype=&quot;Equal&quot;), &quot;s1&quot;=list(panel=c(PanelOne=1500), seltype=&quot;Equal&quot;), &quot;s2&quot;=list(panel=c(PanelOne=1500), seltype=&quot;Equal&quot;), &quot;s3&quot;=list(panel=c(PanelOne=1500), seltype=&quot;Equal&quot;), &quot;s4&quot;=list(panel=c(PanelOne=1500), seltype=&quot;Equal&quot;), &quot;s5&quot;=list(panel=c(PanelOne=1500), seltype=&quot;Equal&quot;), &quot;s6&quot;=list(panel=c(PanelOne=1500), seltype=&quot;Equal&quot;), &quot;s7&quot;=list(panel=c(PanelOne=1500), seltype=&quot;Equal&quot;), &quot;s8&quot;=list(panel=c(PanelOne=1500), seltype=&quot;Equal&quot;), &quot;s9&quot;=list(panel=c(PanelOne=1500), seltype=&quot;Equal&quot;) ) # Échantillonnage spatialement équilibrée (Generalized Random Tesselation) train.points &lt;- grts(sp.object=frame.points, # grille d&#39;échantillonnage src.frame=&quot;sp.object&quot;, # type de grille (objet spatiale) design=Dsgn.grt, # définition d&#39;échantillonnage stratum=&quot;ndvi_c&quot;, # colonne avec strate type.frame=&quot;finite&quot;, # type d&#39;échantillonnage DesignID=&quot;train&quot; # préfixe pour chaque point ) # Appliquer le système de référence des coordonnées proj4string(train.points) &lt;- proj4string(frame.points) # Mélanger les points d&#39;entraînement et ajouter un identifiant train.points &lt;- train.points[sample(1:nrow(train.points)), ] train.points$SAMPLEID &lt;- paste0(&quot;trn-&quot;, str_pad(string=1:nrow(train.points), width = 4, pad = &quot;0&quot;, side = &quot;left&quot;)) # Conversion des points en parcelles et ajouter des attributs ================= # Grille Landsat landsat.grid &lt;- raster(ndvi) values(landsat.grid) &lt;- 1 # Selectionner les pixels Landsat correspondantes et convertir en polygone train.plots &lt;- rasterToPolygons(mask(landsat.grid, train.points)) # Extraire les attributs des points d&#39;entraînement ... train.plots@data &lt;- over(train.plots, train.points[, c(&quot;PLOTID&quot;, &quot;SAMPLEID&quot;, &quot;xcoords&quot;, &quot;ycoords&quot;, &quot;ndvi&quot;, &quot;stratum&quot;)]) # ... et compléter avec autres attributs train.plots$ccov &lt;- as.character(NA) # couverture houppiers train.plots$img_src &lt;- train.plots$img_date &lt;- as.character(NA) # source et date d&#39;image train.plots$author &lt;- train.plots$mod_date &lt;- as.character(NA) # auteur et date # Sauvegarder parcelles sous format Shapefile et KML writeOGR(train.plots, dsn=OUT.DIR, layer=&quot;COV_parcelles&quot;, driver=&quot;ESRI Shapefile&quot;, overwrite=TRUE) writeKML(train.plots, kmlname=&quot;COV_parcelles&quot;, filename=paste0(OUT.DIR, &quot;/COV_parcelles.kml&quot;)) # Créer une grille d&#39;échantillon 7x7 dans chaque parcelle ===================== # Détérminer la grille grid.size &lt;- 7 res &lt;- res(landsat.grid)[1] offset &lt;- c(res/grid.size/2 + (0:(grid.size-1))*res/grid.size) # Diviser les parcelles pour un traitement parallèle subsets &lt;- split(train.plots, f=1:(CORES-1)) registerDoParallel(CORES-1) train.grids &lt;- foreach(subset=subsets, .combine=rbind, .multicombine=TRUE) %dopar% { # Créer un couche de points vides ... grids &lt;- SpatialPointsDataFrame(data.frame(x = 0, y = 0), data=data.frame(PLOTID = 0, SAMPLEID = 0, GRIDPOINT = 0))[-1,] # ... et ajoute la grille d&#39;échantillon pour chaque parcelle for(p in 1:length(subset)) { plot &lt;- subset[p,] ext &lt;- extent(plot) grids &lt;- bind(grids, SpatialPointsDataFrame(expand.grid(ext@xmin+offset, ext@ymin+offset), data=data.frame(PLOTID = plot$PLOTID, SAMPLEID = plot$SAMPLEID, GRIDPOINT = 1:grid.size^2))) } grids } # Appliquer le système de référence des coordonnées proj4string(train.grids) &lt;- proj4string(train.plots) # Ajouter un attribut &quot;arbre&quot; (1: oui, 0: non) train.grids$tree &lt;- as.integer(NA) # Sauveguarder les grille d&#39;échantillon sous format Shapefile writeOGR(train.grids, dsn=OUT.DIR, layer=&quot;COV_parcelles_grid&quot;, driver=&quot;ESRI Shapefile&quot;, overwrite=TRUE) # Diviser parcelles et grilles d&#39;échantillon en 10 sous-ensembles ============= # pour le traitement par différents photo-interprètes subsets &lt;- split(train.plots, f=1:10) for(i in 1:length(subsets)) { # Sauvegarder parcelles sous format Shapefile et KML writeOGR(subsets[[i]], dsn=OUT.DIR, layer=paste0(&quot;COV_parcelles_&quot;, i), driver=&quot;ESRI Shapefile&quot;, overwrite=TRUE) writeKML(subsets[[i]], kmlname=paste0(&quot;COV_parcelles_&quot;, i) , filename=paste0(OUT.DIR, &quot;/COV_parcelles_&quot;, i, &quot;.kml&quot;)) # Sauvegarder les grilles correspondantes sous format Shapefile subset.grids &lt;- train.grids[train.grids$PLOTID %in% subsets[[i]]$PLOTID,] writeOGR(subset.grids, dsn=OUT.DIR, layer=paste0(&quot;COV_parcelles_&quot;, i, &quot;_grid&quot;), driver=&quot;ESRI Shapefile&quot;, overwrite=TRUE) } "],
["03_validation-plots.html", "", " 2.2.3 Parcelles de validation 2.2.3.1 Échantillonnage La sélection des parcelles de validation est basée sur les cartes forêt/non-forêt, notamment leurs transitions entre les années 1987 – 2003 – 2015 – 2018. Tout d’abord les transitions sont déterminées pour chaque point du réseau d’échantillonnage. Ensuite un échantillon aléatoire stratifié est tirré avec une répartition de l’échantillon aux strates (transitions) qui est la valeur moyenne d’une répartition proportionnelle à la taille des strates et d’une répartion égale. 2.2.3.2 Occupation des terres L’occupation des terres et le changement de l’occupation des terres est déterminé sur base des images Landsat. Trois différentes catégories sont distingués: forêt (couverture houppier ≥ 30%) terre boisée (couverture houppier entre 10% et 30%) non-forêt (couverture houppier &lt; 10%) La couverture des houppiers est utilisé pour détérminer l’occupation des terres sur l’image Landsat de la date correspondate. À partir de cette référence, l’occupation des terres est détérminé pour les autres dates de référence 1987 – 2003 – 2015 – 2018. La figure suivante illustre la procédure: Script R: 01_SSTS/02_BdD/_src/create-val-plots.R ############################################################################### # create-val-points.R: Créer un ensemble de parcelles de validation # ----------------------------------------------------------------------------- # Bern University of Applied Sciences # Oliver Gardi, &lt;oliver.gardi@bfh.ch&gt; # 13 Mai 2020 # Préparation des variables =================================================== OUT.DIR &lt;- paste0(DIR.SST.BDD, &quot;/03_val-plots/empty&quot;) if(!dir.exists(OUT.DIR)) dir.create(OUT.DIR, recursive=TRUE) IN.DIR &lt;- paste0(DIR.MRV.MCF, &quot;/2_raw-maps/FC30/TGO&quot;) # Fusionner les cartes des années de référence (1987, 2003, 2015 et 2018) maps &lt;- merge(raster(paste0(IN.DIR, &quot;/TGO_1987_F30r.tif&quot;)), raster(paste0(IN.DIR, &quot;/TGO_2003_F30r.tif&quot;)), raster(paste0(IN.DIR, &quot;/TGO_2015_F30r.tif&quot;)), raster(paste0(IN.DIR, &quot;/TGO_2018_F30r.tif&quot;))) # Créer carte des changements (p.ex. FFFN pour déforestation entre 2015 et 2018) change.map &lt;- maps[[1]] + maps[[2]]*10 + maps[[3]]*100 + maps[[4]]*1000 # Charger la grille d&#39;échantillonnage du SSTS frame.points &lt;- readOGR(paste0(DIR.SST.BDD, &quot;/01_reseau-SSTS/TGO_frame_480m.shp&quot;)) # Extraire la transition pour chaque parcelle frame.points$trans &lt;- raster::extract(change.map, frame.points) # Allocation des points de validation ========================================= n &lt;- 4000 # la taille de l&#39;échantillon alloc &lt;- freq(change.map)[1:16,] # fréquence des transitions (p. ex FFFN) # Allocation proportionelle et égale alloc &lt;- cbind(alloc, prop=round(n*alloc[,&quot;count&quot;]/sum(alloc[,&quot;count&quot;])), equal=round(n/nrow(alloc))) # Allocation balancée (moyenne entre proportionelle et égale) alloc &lt;- cbind(alloc, balanced=round((alloc[,&quot;prop&quot;] + alloc[,&quot;equal&quot;])/ 2)) # Echantillonnage des parcelles de validation ================================ # Ajoute les attributs des parcelles d&#39;entraînement train.plots &lt;- readOGR(paste0(DIR.SST.BDD, &quot;/02_train-plots/assessed/COV_parcelles.shp&quot;)) frame.points &lt;- merge(frame.points, train.plots@data[,c(&quot;PLOTID&quot;, &quot;img_date&quot;, &quot;img_src&quot;, &quot;mod_date&quot;, &quot;author&quot;, &quot;ccov&quot;)], by=&quot;PLOTID&quot;, all.x=TRUE) # Créer une couche de points vide val.points &lt;- frame.points[0,] # Initialiser le générateur de nombres aléatoires set.seed(RSEED) # Pour chaque transition ... for(i in 1:nrow(alloc)) { strat.n &lt;- alloc[i, &quot;balanced&quot;] # nombre d&#39;échantillons à tirer sample.ids &lt;- NULL # vecteur pour les ID à échantillonner # Tout d&#39;abord, prend les parcelles avec ... ids.tp &lt;- which(!is.na(frame.points$trans) &amp; frame.points$trans==alloc[i, &quot;value&quot;] &amp; !is.na(frame.points$ccov)) # couverture houppier connue n.tp &lt;- min(length(ids.tp), strat.n) if (n.tp &gt; 0) sample.ids &lt;- c(sample.ids, sample(ids.tp, n.tp)) # ... et completer avec autres échantillons de la grille avec ids.r &lt;- which(!is.na(frame.points$trans) &amp; frame.points$trans==alloc[i, &quot;value&quot;] &amp; is.na(frame.points$ccov)) # couverture houppier inconnue n.r &lt;- min(length(ids.r), strat.n - n.tp) if (n.r &gt; 0) sample.ids &lt;- c(sample.ids, sample(ids.r, n.r)) # aléatoirement # Ajouter aux points de validation val.points &lt;- rbind(val.points, frame.points[sample.ids, ]) } # Mélanger les points de validation et ajouter un identifiant val.points &lt;- val.points[sample(1:nrow(val.points)), ] val.points$SAMPLEID &lt;- paste0(&quot;val-&quot;, str_pad(string=1:nrow(val.points), width = 4, pad = &quot;0&quot;, side = &quot;left&quot;)) # Conversion des points en parcelles et ajouter des attributs ================= # Grille Landsat landsat.grid &lt;- raster(change.map) values(landsat.grid) &lt;- 1 # Selectionner les pixels Landsat correspondantes et convertir en polygone val.plots &lt;- rasterToPolygons(mask(landsat.grid, val.points)) # Extraire les attributs des points d&#39;entraînement ... val.plots@data &lt;- over(val.plots, val.points[, c(&quot;PLOTID&quot;, &quot;SAMPLEID&quot;, &quot;xcoords&quot;, &quot;ycoords&quot;, &quot;trans&quot;, &quot;ccov&quot;, &quot;img_src&quot;, &quot;img_date&quot;, &quot;author&quot;, &quot;mod_date&quot;)]) # ... convertir couverture des houppiers disponibles en %, les autres NA val.plots$ccov &lt;- format(round(100*val.plots$ccov, 1)) val.plots$ccov[val.plots$ccov == &quot; NA&quot;]&lt;- NA # ... et compléter avec attributs occupation des terre 1987, 2003, 2015 et 2018 val.plots$lc_18 &lt;- val.plots$lc_15 &lt;- as.character(NA) val.plots$lc_03 &lt;- val.plots$lc_87 &lt;- as.character(NA) # Sauvegarder parcelles sous format Shapefile et KML writeOGR(val.plots, dsn=&quot;.&quot;, layer=&quot;UOT_parcelles&quot;, driver=&quot;ESRI Shapefile&quot;, overwrite=TRUE) writeKML(val.plots, kmlname=&quot;UOT_parcelles&quot;, filename=&quot;UOT_parcelles.kml&quot;) # Créer une grille d&#39;échantillon 7x7 dans chaque parcelle ===================== # Détérminer la grille grid.size &lt;- 7 res &lt;- res(landsat.grid)[1] offset &lt;- c(res/grid.size/2 + (0:(grid.size-1))*res/grid.size) # Diviser les parcelles pour un traitement parallèle subsets &lt;- split(val.plots, f=1:86) registerDoParallel(CORES-1) val.grids &lt;- foreach(subset=subsets, .combine=bind, .multicombine=TRUE) %dopar% { # Créer un couche de points vides ... grids &lt;- SpatialPointsDataFrame(data.frame(x = 0, y = 0), data=data.frame(PLOTID = 0, SAMPLEID = 0, GRIDPOINT = 0))[-1,] # ... et ajoute la grille d&#39;échantillon pour chaque parcelle for(p in 1:length(subset)) { plot &lt;- subset[p,] ext &lt;- extent(plot) grids &lt;- bind(grids, SpatialPointsDataFrame(expand.grid(ext@xmin+offset, ext@ymin+offset), data=data.frame(PLOTID = plot$PLOTID, SAMPLEID = plot$SAMPLEID, GRIDPOINT = 1:grid.size^2))) } grids } # Appliquer le système de référence des coordonnées proj4string(val.grids) &lt;- proj4string(val.plots) # fusionner avec les attributs (arbre, oui ou non?) déjà collectés train.grids &lt;- readOGR(paste0(DIR.SST.BDD, &quot;/02_train-plots/assessed/COV_parcelles_grid.shp&quot;)) val.grids &lt;- merge(val.grids, train.grids@data[, c(&quot;PLOTID&quot;, &quot;GRIDPOINT&quot;, &quot;tree&quot;)], all.x=TRUE) # Sauveguarder les grille d&#39;échantillon sous format Shapefile writeOGR(val.grids, dsn=OUT.DIR, layer=&quot;UOT_parcelles_grid&quot;, driver=&quot;ESRI Shapefile&quot;, overwrite=TRUE) # Diviser parcelles et grilles d&#39;échantillon en 10 sous-ensembles ============= # pour le traitement par différents photo-interprètes subsets &lt;- split(val.plots, f=1:10) for(i in 1:length(subsets)) { # Sauvegarder parcelles sous format Shapefile et KML writeOGR(subsets[[i]], dsn=OUT.DIR, layer=paste0(&quot;UOT_parcelles_&quot;, i), driver=&quot;ESRI Shapefile&quot;, overwrite=TRUE) writeKML(subsets[[i]], kmlname=paste0(&quot;UOT_parcelles_&quot;, i) , filename=paste0(&quot;UOT_parcelles_&quot;, i, &quot;.kml&quot;)) # Sauvegarder les grilles correspondantes sous format Shapefile subset.grids &lt;- val.grids[val.grids$PLOTID %in% subsets[[i]]$PLOTID,] writeOGR(subset.grids, dsn=OUT.DIR, layer=paste0(&quot;UOT_parcelles_&quot;, i, &quot;_grid&quot;), driver=&quot;ESRI Shapefile&quot;, overwrite=TRUE) } "],
["04_outil-QGIS.html", "", " 2.2.4 Outil QGIS Actuellement, les parcelles d’échantillonnage du SSTS ainsi que la grille de points pour déterminer la couverture des houppiers sont stockées sous forme des fichiers Shapefile. L’acquisition des attributs est effectuée par les photo-interprètes à l’aide d’un formulaire défini dans un projet QGIS. Un SSTS basé sur des fichiers limite la complexité de la structure des données (par exemple, l’enregistrement récurrent des mêmes parcelles sur des images de différentes années) ainsi que le travail parallèle de différents photo-interprètes sur les mêmes données. Actuellement, des travaux sont en cours pour transférer le SSTS vers une base de données géographiques (PostGIS) sur un serveur central au Ministère de l’Environnement et des Ressources Forestières (MERF) et pour définir un formulaire QGIS qui permet à plusieurs personnes de travailler simultanément sur les données. "],
["00_IFN.html", "3 Inventaire Forestier National", " 3 Inventaire Forestier National L’inventaire forestier national, combine les données collectées directement sur le terrain avec des références forestières provenant de différentes sources. Le seul ensemble de données collectées est actuellement le premier IFN-1 national inverntaire forestier qui a été réalisé par le GIZ en 2015/16. Il est utilisé dans le cadre du NRF-MRV pour déterminer les facteurs d’émission de la biomasse des arbres. Un deuxième inverntaire forestier national (IFN-2) sur base des parcelles permanentes installées par IFN-1 est en cours de planification. Ce deuxième inventaire sera realise par la direction de l’environnment du Mininstère de l’environnment, du developpement durable et de la protection de la nature. En outre, d’autres ensembles de données seront intégrés dans la base de données au cours des prochaines années: inventaires réalisés par les universités du Togo et autres acteurs inventaire des plantations de l’ODEF données sur les feux de végétation de l’ANGE données sur l’agriculture (superficie emblavées et le cheptel) de DCID et ITRA données démographiques de INSEED … Les données et les documents des enquêtes respectives sont structurés comme suit: 02_IFN # INVENTAIRE FORESTIER NATIONAL ============= ├── 01_IFN-1 # Premier IFN 2015/16 --------------------- ├── 01_data # données brutes ├── 02_aux # données auxiliaires, scripts, analyses └── 03_reports # rapports (méthode, résultats, ...) └── ... # Autres données (IFN-2, Plantations, Feu) "],
["01_IFN-1.html", "3.1 IFN-1 (2015/16)", " 3.1 IFN-1 (2015/16) Le premier inventaire forestier national a ete realisé par la GIZ et le MERF en 2015/16. 3.1.1 Méthode L’inventaire IFN-1 a installé xx parcelles permanentes au Togo. Les parcelles ont été installée … 3.1.2 Données 3.1.3 Résultats Les résultats … "],
["00_NRF-MRV.html", "4 Analyses NRF/MRV", " 4 Analyses NRF/MRV Le Togo a soumis son premier Niveau de Référence pour les Forêts (NRF v1.0) en Janvier 2020 (Soumissions du Togo sur la plateforme web REDD+ de la CCNUCC). Il est basé sur l’évolution de la couverture forestier entre 2003 – 2018 (pertes et gains des terres forestières à une couverture du houppier ≥ 30%) et le stockage de carbone dans la biomasse aérienne des arbres (incl. bois mort sur pied) Les données de base qu’on a utilisé pour effectuer ce travail sont: les images Landsat de l’archive USGS (1985 – 2019) et les données climatiques WorldClim version 2 pour la la cartographie de l’évolution des surfaces forestiers et la cartographie de la biomasse les données du SSTS (état 2019) sur la couverture du houppier et l’utilisation des terres pour la calibration et la validation des cartes sur l’évolution des surfaces forestier les données dendrométriques du IFN-1 (2015/16) pour déterminer le stockage de carbone dans la biomasse aérienne par parcelle et la calibration des cartes de la biomasse Ce chapitre décrit l’approche méthodologique et les outils techniques utilisés pour établir ce NRF. C’est un travail en cours. Le même approche sera utilisés a) pour améliorer le NRF avec des nouvelles données et/ou mèthodes et b) pour mettre à jour régulièrement les analyses dans le cadre du Monitoring, reporting et vérification (MRV). Chaque section commence par une description de la méthodologie utilisée et des références aux points clés dans le script R correspondant, suivie par une présentation exemplaire des résultats de cette étappe et enfin du code R commenté. Section 2.1 décrit l’acquisition et la préparation des données de télédétection dans le cadre du Système de Surveillance Terrestre par Satellite SSTS (Images Landsat, données WorldClim, …). Section 2.2 décrit la collecte de données d’entraînement et de validation dans le cadre du SSTS. Section 4.1 décrit les différentes étapes nécessaires pour la cartographie des surfaces forestières et leur évolution: la classification des séries de cartes, leur néttoyage et validation. Section 4.2 décrit les différentes étapes nécessaires pour la cartographie des la biomasse aérienne: l’évaluation des données de l’IFN, la calibration des cartes de biomasse leur néttoyage et l’analyse de l’évolution de la biomasse dans le temps. "],
["00_analyse-FCC.html", "4.1 Analyse surfaces forestiers", " 4.1 Analyse surfaces forestiers L’analyse des surfaces forestiers est fait par une classification supervisée. Les données du SSTS sur la couverture des houppiers (section 2.2.2.2) est utilisé pour calibrer un modèle de classification RandomForest sur base des images satellitaires Landsat et les données climatiques Worldclim v2. Tous les parcelles d’entraînement avec une couverture des houppiers ≥ 30% sont considérées comme forêt, les autres comme non-forêts. Le schéma suivant montre les étapes pour la production des cartes forêt/non-forêt sur toute la période 1986 – 2019: Vue que les données sur la couverture des houppier et seulement disponible pour les années récentes (à cause d’une disponibilité limité des images de très haute résolution), c’est seulement la carte forêt/non-forêt 2018 qui a été produit sur base des parcelles d’entraînement du SSTS. Cette carte de référence 2018 est utilisé comme base pour la calibration des modèles de classification pour les autres dates pour lesquelles des images Landsat sont disponible. Tout d’abord la carte de référence 2018 est utilisé pour calibrer la classification d’une carte forêt/non-forêt en 2003. C’est cette carte de référence 2003 qu’on a utilisé pour calibrer les autres cartes de 1986 – 2018. On a choisi de faire la classification de toute la serie des images sur la carte de référence 2003, parce que les images Landsat 2003 sont de très bonne qualité (et donc la carte est probablement aussi de bonne qualité) et parce que l’année 2003 se trouve au milieu de la période analysée. Si on prend directement la carte forêt/non-forêt 2018 comme référence pour la calibration de la série 1986 – 2018, on observe une généralisation de la surface forestier le plus on s’éloigne de la date 2018, donc un changement de la surface forestier qui est plutôt un artefact de la méthode que une changement d’occupation des terres. Dans la suite, la série des cartes forêts/non-forêts 1986 – 2018 est nettoyé par une lyssage temporelle, pixel par pixel, avec un filtre majoritaire (fenêtre coulissante d’une taille de 5). Finalement, une reforestation est seulement constaté si on a observé la forêt pour une période de 10 ans et plus. Les cartes nettoyés de 2003, 2015 et 2018 sont validés avec les données SSTS sur l’occupation des terres (Section 2.2.3.2) et les matrices d’erreurs sont utilisées pour détérminer la précision des cartes individuelles et des différents changements d’occupation des terres, en utilisant la méthode de Olofsson et al. (2014) "],
["01_create-FC-maps.html", "", " 4.1.1 Production des cartes Forêt/Non-Forêt La production de la carte forêt/non-forêt 2018 est basée sur la couverture des houppiers déterminée dans les parcelles d’entraînement, les images satellitaires Landsat (bandes et indices B, G, R, NIR, SWIR1, SWIR2, nbr, ndmi, ndvi, evi) et les données climatiques Worldclim (BIO1, BIO4, BIO12, BIO15). Dans une première étape, les variables Landsat et Worldclim sont extraites pour toutes les parcelles d’entraînement de la période de référence 1.1.2017 - 31.12.2018 et les parcelles d’entraînement sont classées comme “forestières” ou “non forestières” en fonction de leur couverture des houppiers (forêt ≥ 30% de couverture des houppiers). Par la suite, des cartes forêt/non-forêt 2018 sont produites pour chaque chemin WRS. Pour cela, la fonction classify.image() est utilisée, qui crée un modèle de classification en utilisant l’algorithme RandomForest et une carte correspondante. Pour le chemin WRS central p193, l’algorithme de classification est calibré uniquement sur la base des parcelles d’entraînement. Pour les chemins p192 et p194, des données d’entraînement supplémentaires basées sur la carte p193 sont utilisées pour assurer la calibration entre les chemins. Les cartes de référence générées pour 2018 sont maintenant utilisées pour générer les cartes correspondantes pour l’année 2003. Les données d’entraînement sont tirées de la carte de référence 2018, autour de la lisière des forêts. Pour les chemins p192 et p194, de nouves des données d’entraînement supplémentaires du chemin p193 sont utilisées. Sur la base des cartes forêt/non-forêt de 2003, les cartes de toutes les années avec des images Landsat disponibles sont produites selon la même procédure. Le calibrage de toute la série sur la base d’une année de référence garantit une différenciation consistante entre les classes forêt et non-forêt. Enfin, les chemins p192, p193 et p194 sont fusionnés pour les années clés 1987, 2003, 2015 et 2018. Example La figure suivante illustre la série des 13 cartes forêt/non-forêt brutes dans une région au sud de Kpalimé. Les pixels en rose vif sont des pixels dont les données sont manquantes (nuages, ombres, L7 SLC-off). Voir série des cartes nettoyées pour comparaison. Script R: 03_NRF-MRV/01_MCF/_src/01_create-FC-maps.R ############################################################################### # 01_create-fc-maps.R: créer des cartes brutes du couvert forestier # ----------------------------------------------------------------------------- # Bern University of Applied Sciences # Oliver Gardi, &lt;oliver.gardi@bfh.ch&gt; # 13 Mai 2020 # Définitions des variables =================================================== # Seuil de la couverture des houppiers (forêt vs. non-forêt) COV.FC &lt;- 30 # Nombre de pixels à considerer comme lisière forêt SAMPLE.DIST &lt;- 1 # Nombre de pixels non-NA (sera déterminé plus tard) N.PIXELS &lt;- NA # Part de pixels à prendre en compte pour la calibration des cartes SAMPLE.RATIO &lt;- 0.0025 # Part des pixels à prendre des autres chemins (calibration) CAL.RATIO &lt;- 0.75 # Bandes à utiliser pour la modélisation forêt vs. non-forêt PREDICTORS &lt;- c(&quot;B&quot;, &quot;G&quot;, &quot;R&quot;, &quot;NIR&quot;, &quot;SWIR1&quot;, &quot;SWIR2&quot;, &quot;nbr&quot;, &quot;ndmi&quot;, &quot;ndvi&quot;, &quot;evi&quot;, &quot;BIO1&quot;, &quot;BIO4&quot;, &quot;BIO12&quot;, &quot;BIO15&quot;) # Répertoires LANDSAT.DIR &lt;- DIR.SST.DAT.LST WORLDCLIM.DIR &lt;- DIR.SST.DAT.WC2 REF.DIR &lt;- DIR.MRV.MCF.REF RAW.DIR &lt;- DIR.MRV.MCF.RAW # Définitions des fonctions =================================================== # Charger un image Landsat ---------------------------------------------------- # # @param filename Chemin du fichier landsat # # @return Image Landsat avec bandes nommées # load.image &lt;- function(filename) { image &lt;- brick(paste0(LANDSAT.DIR, filename)) names(image) &lt;- SST.LSBANDS return(image) } # Tirer des points d&#39;entraînement d&#39;une carte autour de la lisière forêt ------ # # @param map Carte forêt/non-forêt à échantillonner # @param n Nombre d&#39;échantillons à tirer # # @return Points d&#39;échantillon avec aatribut forêt/non-forêt # sample.map &lt;- function(map, n) { tmp.src &lt;- tempfile(pattern = &quot;&quot;, fileext = &quot;.tif&quot;) # tmp carte forêt/non-forêt tmp.dst1 &lt;- tempfile(pattern = &quot;&quot;, fileext = &quot;.tif&quot;) # tmp lisière à l&#39;extérieur tmp.dst3 &lt;- tempfile(pattern = &quot;&quot;, fileext = &quot;.tif&quot;) # tmp lisière à l&#39;intérieur # Écrire la carte sur le disque map &lt;- writeRaster(map, tmp.src) # Forêt + lisière à l&#39;extérieur de la forêt system(paste(&quot;gdal_proximity.py&quot;, tmp.src, tmp.dst1, &quot;-values&quot;, FOREST, &quot;-use_input_nodata YES&quot;, &quot;-maxdist &quot;, SAMPLE.DIST, &quot;-fixed-buf-val&quot;, NONFOR)) dst1 &lt;- raster(tmp.dst1) NAvalue(dst1) &lt;- 65535 cat(&quot; &quot;) # Non-forêt + lisière à l&#39;intérieur de la forêt system(paste(&quot;gdal_proximity.py&quot;, tmp.src, tmp.dst3, &quot;-values&quot;, NONFOR, &quot;-use_input_nodata YES&quot;, &quot;-maxdist &quot;, SAMPLE.DIST, &quot;-fixed-buf-val&quot;, FOREST)) dst3 &lt;- raster(tmp.dst3) NAvalue(dst3) &lt;- 65535 # Masquer la carte avec les lisières map &lt;- mask(map, dst1) map &lt;- mask(map, dst3) # Supprimer les fichiers temporaires unlink(c(tmp.src, tmp.dst1, tmp.dst3)) # Echantillonnage stratifié des lisières forêt et non-forêt n.classes &lt;- length(unique(map)) cat(paste0(&quot; -Sampling map (n=&quot;, n.classes, &quot;*&quot;, round(n/n.classes), &quot;) ... &quot;)) sample.pts &lt;- sampleStratified(map, round(n/n.classes), sp=TRUE)[,-1] names(sample.pts) &lt;- &quot;CLASS&quot; cat(&quot;done\\n&quot;) return(sample.pts) } # Classification d&#39;une image ------------------------------------------- # # @param image Image Landsat à classifier # @param filename Nom de fichier pour la sauvegarde de la carte # @param bioclim Raster des variables bioclimatiques à utiliser # @param train.pts Points d&#39;entraînement # @param ref.map Carte de référence # @param n.ref.map Nombre de points à échantilloner # @param cal.map Carte de calibration (autre chemin WRS) # @param n.cal.map Nombre de points à échantilloner # @param mask Masque à utiliser pour ref.map et cal.map # @param preds Variables à utiliser pour le modèle # @param type Modèle de classification ou de regression # @param crossval Faire validation croisée (3 * 10-fold) # @param prob Produire également carte de probabilité # @param n.cores Nombre de processeurs à utiliser pour prédiction # # @return List avec les éléments # - model, # - carte forêt/non-forêt # - carte des probabilités (si disponible) # classify.image &lt;- function(image, filename, bioclim=NULL, train.pts=NULL, ref.map=NULL, n.ref.map=NULL, cal.map=NULL, n.cal.map=NULL, mask=NULL, preds=NULL, type=&quot;classification&quot;, crossval=FALSE, prob=FALSE, n.cores=8) { # Ouvrir le fichier journal txtfile &lt;- paste0(sub(&quot;[.]tif$&quot;, &quot;&quot;, filename), &quot;.txt&quot;) cat(&quot;-- Image classification: &quot;, basename(filename), &quot;/&quot;, date(), &quot; --\\n&quot;, file=txtfile) # Charger des points d&#39;entraînement --------------------- if(!is.null(train.pts)) { cat(&quot; -Loading training points ... &quot;) train.pts &lt;- train.pts[,1] # utiliser que la première colonne ... names(train.pts) &lt;- &quot;CLASS&quot; # ... et nommer &quot;CLASS&quot; set_ReplCRS_warn(FALSE) proj4string(train.pts) &lt;- proj4string(image) # Système de coordonnées CRS cat(&quot;done\\n&quot;) cat(&quot;Training points:&quot;, nrow(train.pts), &quot;\\n&quot;, file=txtfile, append=TRUE) } # Ajouter des points d&#39;une carte de référence ----------- if(!is.null(ref.map)) { cat(paste0(&quot; -Masking / buffering reference map ... \\n&quot;)) # ... couper/masquer avec l&#39;image ref.map &lt;- mask(crop(ref.map, image[[1]]), crop(image[[1]], ref.map)) # ... et masque additionelle (si disponible) if(!is.null(mask)) ref.map &lt;- mask(ref.map, mask) # Découper la carte de calibration (si disponible) if(!is.null(cal.map)) { tmp &lt;- extend(crop(cal.map, ref.map), ref.map) ref.map &lt;- mask(ref.map, tmp, inverse=TRUE) } cat(&quot; &quot;) # Tirer des points d&#39;échantillon ... ref.pts &lt;- sample.map(ref.map, n.ref.map) cat(&quot;Ref-map points: &quot;, nrow(ref.pts), &quot;/&quot;, ref.map@file@name, &quot;/&quot;, SAMPLE.DIST, &quot;px\\n&quot;, file=txtfile, append=TRUE) # ... et ajouter aux points d&#39;entraînement if(is.null(train.pts)) { train.pts &lt;- ref.pts } else { train.pts &lt;- rbind(train.pts, ref.pts) } } # Ajouter des points d&#39;une carte de calibration --------- if(!is.null(cal.map)) { cat(paste0(&quot; -Masking / buffering calibration map ... \\n&quot;)) # ... couper/masquer avec l&#39;image cal.map &lt;- mask(crop(cal.map, image[[1]]), crop(image[[1]], cal.map)) # ... et masque additionelle (si disponible) if(!is.null(mask)) cal.map &lt;- mask(cal.map, mask) cat(&quot; &quot;) # Tirer des points d&#39;échantillon ... cal.pts &lt;- sample.map(cal.map, n.cal.map) cat(&quot;Cal-map points: &quot;, nrow(cal.pts), &quot;from&quot;, cal.map@file@name, &quot;/&quot;, SAMPLE.DIST, &quot;px\\n&quot;, file=txtfile, append=TRUE) # ... et ajouter aux points d&#39;entraînement if(is.null(train.pts)) { train.pts &lt;- cal.pts } else { train.pts &lt;- rbind(train.pts, cal.pts) } } # Nombre total des points d&#39;entraînement cat(&quot;Total points: &quot;, nrow(train.pts), &quot;\\n&quot;, file=txtfile, append=TRUE) # Extraire les variables correspondantes ---------------- # Utiliser toutes les variables si non-spécifiées dans les paramètres if(is.null(preds)) { preds &lt;- names(image) if(!is.null(bioclim)) preds &lt;- c(preds, names(bioclim)) } # Extraire les variables Landsat ... cat(&quot; -Extracting pixel values for bands:&quot;, preds, &quot;... &quot;) train.pts &lt;- raster::extract(image, train.pts, sp=TRUE) # ... et Bioclim if(!is.null(bioclim)) train.pts &lt;- raster::extract(bioclim, train.pts, sp=TRUE) # Ignorer des lignes avec NAs train.dat &lt;- na.omit(train.pts@data)[, c(&quot;CLASS&quot;, preds)] # Calibration du modèle Random Forest ------------------- # Variable catégorielle -&gt; mode de classification, autrement -&gt; régression if(type==&quot;classification&quot;) train.dat[,1] &lt;- as.factor(train.dat[,1]) cat(&quot;done\\n&quot;) cat(&quot; -Calibrating RandomForest ... &quot;) sink(txtfile, append=TRUE) # Utiliser caret::train pour validation croisée (a besoin de beaucoup de temps) if(crossval) { map.model.cv &lt;- train(y = train.dat[,1], x = train.dat[,-1], method = &quot;rf&quot;, # RandomForest importance = TRUE, trControl = trainControl( method = &quot;repeatedcv&quot;, number = 10, # k-fold repeats = 3)) # répétitions print(map.model.cv) map.model &lt;- map.model.cv$finalModel print(map.model) cat(&quot;\\n&quot;) print(varImp(map.model, scale=FALSE)) # autrement randomForest directement } else { map.model &lt;- randomForest(y=train.dat[,1], x=train.dat[,-1], importance=TRUE) # , do.trace=100) # # Parallélisation de RandomForest : cpossible, mais confusion, err.rate, mse et rsq seront NULL # https://stackoverflow.com/questions/14106010/parallel-execution-of-random-forest-in-r # map.model &lt;- foreach(ntree=rep(100, 5), .combine=randomForest::combine, .multicombine=TRUE, .packages=&#39;randomForest&#39;) %dopar% { # randomForest(x=ref.pts[,!(names(ref.pts) == &quot;CLASS&quot;)], y=ref.pts$CLASS, importance=TRUE, ntree=ntree) # print(map.model) cat(&quot;\\n&quot;) print(varImp(map.model)) } sink() # Mesures des erreurs if(type==&quot;treecover&quot;) { cat(&quot;R2:&quot;, round(map.model$rsq[500], 2), &quot;RMSE:&quot;, round(sqrt(map.model$mse[500]), 2), &quot;\\n&quot;) } else { cat(&quot;OOB error rate:&quot;, round(map.model$err.rate[500,1], 2), &quot;\\n&quot;) } # Classification de la carte forêt/non-forêt ------------ dir.create(dirname(filename), recursive=TRUE, showWarnings=FALSE) cat(&quot; -Creating map ... &quot;) # empiler les couches Landsat et bioclim if(!is.null(bioclim)) image &lt;- stack(image, crop(bioclim, image)) # classifier l&#39;image sur différents procésseurs en parallèle beginCluster(n=n.cores) map &lt;- clusterR(image, predict, args=list(model=map.model)) endCluster() # sauvegarder la carte cat(&quot;writing map ... &quot;) # convertir en %, si c&#39;est une carte couverture houppier if(type==&quot;treecover&quot;) map &lt;- floor(map*100) map &lt;- writeRaster(map, filename=filename, format=&quot;GTiff&quot;, datatype=&quot;INT2U&quot;, overwrite=TRUE) cat(&quot;done\\n&quot;) # Calculer une carte de probabilité --------------------- if(prob==TRUE) { cat(&quot; -Creating probability map ... &quot;) beginCluster(n=n.cores) prob.map &lt;- clusterR(image, predict, args=list(model=map.model, type=&quot;prob&quot;)) endCluster() cat(&quot;writing map ... &quot;) writeRaster(prob.map, filename=sub(&quot;\\\\.tif&quot;, &quot;_prob.tif&quot;, filename), format=&quot;GTiff&quot;, overwrite=TRUE) cat(&quot;done\\n&quot;) } else { prob.map &lt;- NULL } cat(&quot;-- Done: &quot;, basename(filename), &quot;/&quot;, date(), &quot; --\\n&quot;, file=txtfile, append=TRUE) invisible(list( &quot;model&quot; = map.model, &quot;map&quot; = map, &quot;prob&quot; = prob.map )) } # COMMENCER LE TRAITEMENT ##################################################### # Charger images 2018 --------------------------------------------------------- ref.p192 &lt;- brick(paste0(LANDSAT.DIR, &quot;/p192/p192_2018_m.tif&quot;)) ref.p193 &lt;- brick(paste0(LANDSAT.DIR, &quot;/p193/p193_2018_m.tif&quot;)) ref.p194 &lt;- brick(paste0(LANDSAT.DIR, &quot;/p194/p194_2018_m.tif&quot;)) names(ref.p192) &lt;- names(ref.p193) &lt;- names(ref.p194) &lt;- SST.LSBANDS ref.images &lt;- list(p192=ref.p192, p193=ref.p193, p194=ref.p194) # Détérminer le nombre de pixels non-NA N.PIXELS &lt;- list(p192 = ncell(ref.p192[[&quot;B&quot;]]) - summary(ref.p192)[&quot;NA&#39;s&quot;,&quot;B&quot;], p193 = ncell(ref.p193[[&quot;B&quot;]]) - summary(ref.p193)[&quot;NA&#39;s&quot;,&quot;B&quot;], p194 = ncell(ref.p194[[&quot;B&quot;]]) - summary(ref.p194)[&quot;NA&#39;s&quot;,&quot;B&quot;]) # Charger variables bioclim bioclim.p192 &lt;- brick(paste0(WORLDCLIM.DIR, &quot;/p192/p192_bioclim.tif&quot;)) bioclim.p193 &lt;- brick(paste0(WORLDCLIM.DIR, &quot;/p193/p193_bioclim.tif&quot;)) bioclim.p194 &lt;- brick(paste0(WORLDCLIM.DIR, &quot;/p194/p194_bioclim.tif&quot;)) names(bioclim.p192) &lt;- names(bioclim.p193) &lt;- names(bioclim.p194) &lt;- SST.BIOCLIM bioclim &lt;- list(p192=bioclim.p192, p193=bioclim.p193, p194=bioclim.p194) # Charger points d&#39;entraînement ----------------------------------------------- train.plots &lt;- readOGR(paste0(DIR.SST.BDD.TPS, &quot;/COV_parcelles.shp&quot;)) train.plots &lt;- train.plots[!is.na(train.plots$ccov), # couverture des houppiers! c(&quot;PLOTID&quot;, &quot;ccov&quot;, &quot;img_date&quot;, &quot;author&quot;)] # Convertir les polygones des parcelles en points spatiaux (centroïdes) train.points &lt;- SpatialPointsDataFrame(gCentroid(train.plots, byid=TRUE), data.frame(author=train.plots$author, ccov=train.plots$ccov, img_date=as.Date(train.plots$img_date))) # Pour la calibration de l&#39;image 2018, sélectionner uniquement les points # d&#39;entraînement dont la date de l&#39;image GoogleEarth est entre le 1.1.2017 et le 31.12.2019 train.points &lt;- train.points[!is.na(train.points$img_date) &amp; train.points$img_date &gt; as.Date(&quot;2017-01-01&quot;) &amp; train.points$img_date &lt;= as.Date(&quot;2019-12-31&quot;), ] # Illustrer la répartition des observations pdf(paste0(REF.DIR, &quot;/training-pts_2018.pdf&quot;)) plot(train.points) dev.off() # Extraire les valeurs d&#39;image pour les points d&#39;entraînement (en parallèle) registerDoParallel(CORES) train.points &lt;- foreach(i=1:length(ref.images), .combine=rbind) %dopar% { pts &lt;- raster::extract(ref.images[[i]], train.points, sp=TRUE) pts &lt;- raster::extract(bioclim[[i]], pts, sp=TRUE) pts$image &lt;- names(ref.images[i]) pts[, c(&quot;author&quot;, &quot;image&quot;, &quot;ccov&quot;, SSTS.BANDS, SSTS.BIOCLIM)] } # Supprimer les lignes avec des NA train.points &lt;- train.points[!is.na(rowSums(train.points@data[,-(1:2)])), ] # Réorganiser le tableau des attributs (F10: forêt/non-forêt à 10% / F30: à 30%) train.points@data &lt;- cbind(train.points@data[,c(&quot;image&quot;, &quot;ccov&quot;)], F10=cut(train.points$ccov, breaks=c(0.0,0.1,1.0), labels=c(NONFOR, FOREST), right=FALSE, include.lowest=TRUE), F30=cut(train.points$ccov, breaks=c(0.0,0.3,1.0), labels=c(NONFOR, FOREST), right=FALSE, include.lowest=TRUE), train.points@data[,c(SSTS.LSBANDS, SSTS.BIOCLIM)]) # Séléction des variables explicatives ---------------------------------------- cov.varsel &lt;- rfe(y = train.points@data[train.points$image==&quot;p193&quot;, &quot;ccov&quot;], x = train.points@data[train.points$image==&quot;p193&quot;, PREDICTORS], sizes = c(4, 6, 8, 10), rfeControl = rfeControl( functions = rfFuncs, # utiliser RandomForest method = &quot;repeatedcv&quot;, # validation croisée number = 10, # 10-fold repeats = 3)) # 3 répétitions print(cov.varsel) predictors(cov.varsel) plot(cov.varsel, type=c(&quot;g&quot;, &quot;o&quot;)) # Carte de la couverture des houppiers pour 2018 (TODO) ----------------------- set.seed(RSEED) p193.2018.cov &lt;- classify.image(image = load.image(&quot;/p193/p193_2018_m.tif&quot;), bioclim = bioclim[[&quot;p193&quot;]], filename = paste0(REF.DIR, &quot;/p193_2018_COV_R.tif&quot;), train.pts = train.points[train.points$image == &quot;p193&quot;, &quot;ccov&quot;], preds = PREDICTORS, type = &quot;treecover&quot;, crossval = TRUE, n.cores = 32) # observé vs. prédit pdf(paste0(REF.DIR, &quot;/p193_2018_COV_R.pdf&quot;)) plot(p193.2018.cov[[&quot;model&quot;]]$y, p193.2018.cov[[&quot;model&quot;]]$predicted, xlim=c(0,1), ylim=c(0,1), main=&quot;Couverture houppier p193&quot;, xlab=&quot;Observation&quot;, ylab=&quot;Prédiction&quot;) abline(0,1) dev.off() # Cartes de référence du couvert forestier 2018 ------------------------------- # Chemin WRS p193 set.seed(RSEED) classify.image(image = load.image(&quot;/p193/p193_2018_m.tif&quot;), bioclim = bioclim[[&quot;p193&quot;]], filename = paste0(REF.DIR, &quot;/FC&quot;, COV.FC, &quot;/p193_2018_FC&quot;, COV.FC, &quot;_R.tif&quot;), train.pts = train.points[train.points$image == &quot;p193&quot;, paste0(&quot;F&quot;, COV.FC)], preds = PREDICTORS, prob = TRUE, crossval = TRUE, n.cores = 32) # Chemins WRS p192 and p194, utilisant p193 pour calibration set.seed(RSEED) registerDoParallel(CORES) foreach(path=c(&quot;p192&quot;, &quot;p194&quot;)) %dopar% { # traitement en parallèle train.pts &lt;- train.points[train.points$image == path, paste0(&quot;F&quot;, COV.FC)] classify.image(image = load.image(paste0(&quot;/&quot;, path, &quot;/&quot;, path, &quot;_2018_m.tif&quot;)), bioclim = bioclim[[path]], filename = paste0(REF.DIR, &quot;/FC&quot;, COV.FC, &quot;/&quot;, path, &quot;_2018_FC&quot;, COV.FC, &quot;_R.tif&quot;), train.pts = train.pts, # calibration avec carte p193 ... cal.map = raster(paste0(REF.DIR, &quot;/FC&quot;, COV.FC, &quot;/p193_2018_FC&quot;, COV.FC, &quot;_R.tif&quot;)), n.cal.map = max(2000, nrow(train.pts)/CAL.RATIO), # ... avec au moin 2000 points preds = PREDICTORS, mask = TGO, prob = TRUE, n.cores = 32) } # Cartes de référence du couvert forestier 2003 ----------------------------------------- # Chemin WRS p193 set.seed(RSEED) classify.image(image = load.image(&quot;/p193/p193_2003_m.tif&quot;), bioclim = bioclim[[&quot;p193&quot;]], filename = paste0(REF.DIR, &quot;/FC&quot;, COV.FC, &quot;/p193_2003_FC&quot;, COV.FC, &quot;_R.tif&quot;), # sur base de la carte de référence 2018 ref.map = raster(paste0(REF.DIR, &quot;/FC&quot;, COV.FC, &quot;/p193_2018_FC&quot;, COV.FC, &quot;_R.tif&quot;)), n.ref.map = 2 * SAMPLE.RATIO * N.PIXELS[[&quot;p193&quot;]], preds = PREDICTORS, mask = TGO, n.cores = 32) # Chemins WRS p192 and p194, utilisant p193 pour calibration set.seed(RSEED) registerDoParallel(CORES) foreach(path=c(&quot;p192&quot;, &quot;p194&quot;)) %dopar% { # traitement en parallèle classify.image(image = load.image(paste0(&quot;/&quot;, path, &quot;/&quot;, path, &quot;_2003_m.tif&quot;)), bioclim = bioclim[[path]], filename = paste0(REF.DIR, &quot;/FC&quot;, COV.FC, &quot;/&quot;, path, &quot;_2003_FC&quot;, COV.FC, &quot;_R.tif&quot;), # sur base de la carte de référence 2018 ... ref.map = raster(paste0(REF.DIR, &quot;/FC&quot;, COV.FC, &quot;/&quot;, path, &quot;_2018_FC&quot;, COV.FC, &quot;_R.tif&quot;)), n.ref.map = 2 * (1 - CAL.RATIO) * SAMPLE.RATIO * N.PIXELS[[path]], # ... et calibration avec carte p193 ... cal.map = raster(paste0(REF.DIR, &quot;/FC&quot;, COV.FC, &quot;/p193_2003_FC&quot;, COV.FC, &quot;_R.tif&quot;)), n.cal.map = 2 * CAL.RATIO * SAMPLE.RATIO * N.PIXELS[[path]], preds = PREDICTORS, mask = TGO, n.cores = 32) } # Cartes du couvert forestier brutes pour toutes les dates -------------------- # Chemin WRS p193 set.seed(RSEED) registerDoParallel(CORES) foreach(file=dir(paste0(LANDSAT.DIR, &quot;/p193&quot;), pattern=&quot;\\\\_[[:digit:]]+\\\\_m\\\\.tif&quot;)) %dopar% { classify.image(image = load.image(paste0(&quot;/p193/&quot;, file)), bioclim = bioclim[[&quot;p193&quot;]], filename = paste0(RAW.DIR, &quot;/FC&quot;, COV.FC, &quot;/p193/&quot;, sub(&quot;\\\\_m\\\\.tif$&quot;, paste0(&quot;_F&quot;, COV.FC, &quot;r.tif&quot;), file)), ref.map = raster(paste0(REF.DIR, &quot;/FC&quot;, COV.FC, &quot;/p193_2003_FC&quot;, COV.FC, &quot;_R.tif&quot;)), n.ref.map = SAMPLE.RATIO * N.PIXELS[[&quot;p193&quot;]], preds = PREDICTORS, mask = TGO, n.cores = 6) } # fusionner les deux tuiles p193_1990 merge(raster(paste0(RAW.DIR, &quot;/FC&quot;, COV.FC, &quot;/p193/p193_1990_1_F&quot;, COV.FC, &quot;r.tif&quot;)), raster(paste0(RAW.DIR, &quot;/FC&quot;, COV.FC, &quot;/p193/p193_1990_2_F&quot;, COV.FC, &quot;r.tif&quot;)), filename=paste0(RAW.DIR, &quot;/FC&quot;, COV.FC, &quot;/p193/p193_1990_F&quot;, COV.FC, &quot;r.tif&quot;), format=&quot;GTiff&quot;, datatype=&quot;INT2U&quot;, overwrite=TRUE) # Chemins WRS p192 and p194, utilisant ... set.seed(RSEED) registerDoParallel(CORES) foreach(file=c(dir(paste0(LANDSAT.DIR, &quot;/p192&quot;), pattern=&quot;\\\\_[[:digit:]]+\\\\_m\\\\.tif&quot;), dir(paste0(LANDSAT.DIR, &quot;/p194&quot;), pattern=&quot;\\\\_[[:digit:]]+\\\\_m\\\\.tif&quot;))) %dopar% { path &lt;- sub(&quot;\\\\_.*&quot;, &quot;&quot;, file) # ... carte p193 pour la calibration (s&#39;il existe, pour la même année) if(file.exists(paste0(RAW.DIR, &quot;/FC&quot;, COV.FC, &quot;/p193/&quot;, sub(&quot;\\\\_m\\\\.tif$&quot;, paste0(&quot;_F&quot;, COV.FC, &quot;r.tif&quot;), sub(path, &quot;p193&quot;, file))))) { cal.map &lt;- raster(paste0(RAW.DIR, &quot;/FC&quot;, COV.FC, &quot;/p193/&quot;, sub(&quot;\\\\_m\\\\.tif$&quot;, paste0(&quot;_F&quot;, COV.FC, &quot;r.tif&quot;), sub(path, &quot;p193&quot;, file)))) n.cal.map &lt;- CAL.RATIO * SAMPLE.RATIO * N.PIXELS[[path]] } else { cal.map &lt;- NULL n.cal.map &lt;- NULL } classify.image(image = load.image(paste0(&quot;/&quot;, path, &quot;/&quot;, file)), bioclim = bioclim[[path]], filename = paste0(RAW.DIR, &quot;/FC&quot;, COV.FC, &quot;/&quot;, path, &quot;/&quot;, sub(&quot;\\\\_m\\\\.tif$&quot;, paste0(&quot;_F&quot;, COV.FC, &quot;r.tif&quot;), file)), ref.map = raster(paste0(REF.DIR, &quot;/FC&quot;, COV.FC, &quot;/&quot;, path, &quot;_2003_FC&quot;, COV.FC, &quot;_R.tif&quot;)), n.ref.map = (1 - CAL.RATIO) * SAMPLE.RATIO * N.PIXELS[[path]], cal.map = cal.map, n.cal.map = n.cal.map, preds = PREDICTORS, mask = TGO, n.cores = 6) } # Fusionner les cartes des chemins p192, p193 et p194 pour dates clés ... ----- for(year in YEARS.REF) { merge(mask(crop(brick(paste0(RAW.DIR, &quot;/FC&quot;, COV.FC, &quot;/p193/p193_&quot;, year, &quot;_F&quot;, COV.FC, &quot;r.tif&quot;)),TGO), TGO), mask(crop(brick(paste0(RAW.DIR, &quot;/FC&quot;, COV.FC, &quot;/p192/p192_&quot;, year, &quot;_F&quot;, COV.FC, &quot;r.tif&quot;)),TGO), TGO), mask(crop(brick(paste0(RAW.DIR, &quot;/FC&quot;, COV.FC, &quot;/p194/p194_&quot;, year, &quot;_F&quot;, COV.FC, &quot;r.tif&quot;)),TGO), TGO), filename=paste0(RAW.DIR, &quot;/FC&quot;, COV.FC, &quot;/TGO/TGO_&quot;, year, &quot;_F&quot;, COV.FC, &quot;r.tif&quot;), overwrite=TRUE) } # ... et le cartes de référence 2018 et 2003 for(map in c(paste0(&quot;2018_FC&quot;, COV.FC, &quot;_R&quot;), paste0(&quot;2018_FC&quot;, COV.FC, &quot;_R_prob&quot;), paste0(&quot;2003_FC&quot;, COV.FC, &quot;_R&quot;))) { merge(mask(crop(brick(paste0(REF.DIR, &quot;/FC&quot;, COV.FC, &quot;/p193_&quot;, map, &quot;.tif&quot;)),TGO), TGO), mask(crop(brick(paste0(REF.DIR, &quot;/FC&quot;, COV.FC, &quot;/p192_&quot;, map, &quot;.tif&quot;)),TGO), TGO), mask(crop(brick(paste0(REF.DIR, &quot;/FC&quot;, COV.FC, &quot;/p194_&quot;, map, &quot;.tif&quot;)),TGO), TGO), filename=paste0(REF.DIR, &quot;/FC&quot;, COV.FC, &quot;/TGO_&quot;, map, &quot;.tif&quot;), overwrite=TRUE) } "],
["02_clean-FC-maps.html", "", " 4.1.2 Nettoyage des cartes brutes Les series temporelles des cartes forêt/non-forêt brutes sont lissées et filtrées pixel par pixel pour les raisons suivantes: Remplissage des données manquantes (nuages, ombres, L7 SLC-off), Lissage des bruits (pixels qui changent entre forêt et non-forêt plusieurs fois) Filtrer la régénération temporaire par les fourrés, observée sur les jachères. La première étappe, est la remplissage des données manquantes: les données manquantes entre deux observations de forêt deviennent forêt, celles entre deux observations de non-forêt deviennent non-forêt. Ensuite, une fenêtre coulissante de taille 5 est appliquée, c’est-à-dire qu’une observation est attribuée à la classe qui est observé le plus fréquemment dans la fenêtre qui inclu les deux observations précédentes et les deux suivantes. Pour l’évaluation de la deuxième et de l’avant-dernière observation, une fenêtre coulissante de taille 3 est appliquée. La première et la dernière observation ne sont pas ajustées, il faut les ignorer dans la suite. Les observations manquantes sont remplis dans la mesure possible. Toute la procédure est répétée jusqu’à ce qu’il n’y a plus de changements. Les années manquantes sont ajoutées pour créer une série annuelle. Les observations manquantes sont remplacées par la classe de l’observation précédente. Lorsque les observations initiales sont manquantes, elles sont remplacées par la classe de l’observation suivante. Dans les situations de régénération (forêt suit non-forêt), les 9 premières années sont marquées comme “reboisement potentiel”. Si la forêt disparaît à nouveau après moins de 10 ans, les observations sont remplacées par la classe non-forêt. Si la régénération reste, elle est considérée comme un reboisement après 10 ans. Après la nettoyage des séries temporelles pixel par pixel, une nettoyage spatiale est réalisé pour éliminer les surfaces forestières &lt; 0,5 hectares. Pour cela, une carte est créée de tous les pixels qui étaient une fois observées comme forêt sur toute la série des cartes. Sur cette carte, toutes les surfaces forestières ayant moins de 6 pixels liés (soit moins de 0,54 hectares) sont éliminées. Ce masque forestier est ensuite appliqué à toutes les cartes forêt/non-forêt de la série. Cela signifie que seuls les pixels forestier qui ont fait partie d’une surface forestière ≥ 0,54 hectares dans la série des cartes sont retenus comme forêt. Example La figure suivante montre les cartes forêt/non-forêt après la nettoyage temporelle et spatiale pour une région au sud de Kpalimé (voir série des cartes brutes pour comparaison). Script R: 03_NRF-MRV/01_MCF/_src/02_clean-fc-maps.R ############################################################################### # 02_clean-fc-maps.R: nettoyage des cartes brutes du couvert forestier # ----------------------------------------------------------------------------- # Bern University of Applied Sciences # Oliver Gardi, &lt;oliver.gardi@bfh.ch&gt; # 13 Mai 2020 # Définitions des variables =================================================== COV.FC &lt;- 30 RAW.DIR &lt;- DIR.MRV.MCF.RAW CLN.DIR &lt;- DIR.MRV.MCF.CLN # Définitions des fonctions =================================================== # Nettoyage temporel d&#39;une série d&#39;images ------------------------------------- # # @param path Chemin WRS # # @return -- # clean.temporal &lt;- function(path) { # Préparation des images -------------------------------- # Charger les cartes brutes maps &lt;- stack(dir(paste0(RAW.DIR, &quot;/FC&quot;, COV.FC, &quot;/&quot;, path), pattern=&quot;.*[[:digit:]]{4}\\\\_F.*\\\\.tif$&quot;, full.names=TRUE)) map.names &lt;- sub(&quot;r$&quot;, &quot;&quot;, names(maps)) # Noms de cartes à utiliser dans les colonnes de la matrice map.cols &lt;- sub(paste0(path, &quot;\\\\_&quot;), &quot;X&quot;, sub(&quot;\\\\_[[:alnum:]]+$&quot;, &quot;M&quot;, map.names)) # Names à utiliser pour les années sans carte no.map.cols &lt;- paste0(&quot;X&quot;, YEARS.ALL[!YEARS.ALL %in% gsub(&quot;[[:alpha:]]&quot;, &quot;&quot;, map.cols)], &quot;_&quot;) # Joindre et ordonner les noms de colonnes col.order &lt;- c(map.cols, no.map.cols)[order(c(map.cols, no.map.cols))] # Convertir les cartes en matrice (une carte par colonne, prend du temps) maps.values &lt;- values(maps) colnames(maps.values) &lt;- map.cols # Nettoyage parallèle des trajectoires des pixels ------- # Définir des sous-ensembles des pixels (lignes) pour le traitement parallèle nsubsets &lt;- CORES subsets &lt;- c(0, floor((1:nsubsets)*(nrow(maps.values)/nsubsets))) # Traitement en parallèle registerDoParallel(CORES) maps.values.clean &lt;- foreach(i=1:nsubsets, .combine=rbind) %dopar% { # 0. obtenir un sous-ensemble à partir de maps.values val &lt;- maps.values[(subsets[i]+1):subsets[i+1], ] # mettre tout en NA qui n&#39;est pas de la forêt ou non-forêt val[!(is.na(val) | val %in% c(FOREST,NONFOR))] &lt;- NA # 1. Supprimer les NA isolées ---- str &lt;- apply(val, 1, paste, collapse=&quot;&quot;) # convertir en chaînes de caractères str.c &lt;- gsub(&quot;NA&quot;, &quot;9&quot;, str) # remplacer NA avec 9 # Nettoyer jusqu&#39;à la convergence while(!identical(str, str.c)) { str &lt;- str.c # Mettre NA dans la classe correspondante (forêt ou non-forêt) str.c &lt;- gsub(paste0(&quot;^(.*&quot;, NONFOR, &quot;)9(9*&quot;, NONFOR, &quot;.*)$&quot;), paste0(&quot;\\\\1&quot;, NONFOR, &quot;\\\\2&quot;), str.c) str.c &lt;- gsub(paste0(&quot;^(.*&quot;, FOREST, &quot;)9(9*&quot;, FOREST, &quot;.*)$&quot;), paste0(&quot;\\\\1&quot;, FOREST, &quot;\\\\2&quot;), str.c) } # Reconvertir en matrice numérique val &lt;- matrix(as.numeric(unlist(strsplit(str.c, &quot;&quot;))), ncol=ncol(val), byrow=TRUE) val[val==9] &lt;- NA # remplacer les 9 avec NA colnames(val) &lt;- map.cols # 2. nettoyer les trajectoires avec fenêtre coulissante ---- val.c &lt;- val val.o &lt;- val[] &lt;- 0 iter &lt;- 0 # Nettoyer jusqu&#39;à la convergence while(!identical(val, val.c) &amp; !identical(val.o, val.c)) { iter &lt;- iter+1 message(&quot; -Clean modal: iteration &quot;, iter, &quot; ... &quot;, appendLF = FALSE) val.o &lt;- val # valeurs actuelles -&gt; anciennes valeurs val &lt;- val.c # valeurs nettoyées -&gt; valuers actuelles # 3ème - 3ème l&#39;année dernière : fenêtre modale taille 5 for(l in 3:(ncol(val)-2)) { val.c[,l] &lt;- apply(val[,(l-2):(l+2)], 1, modal, na.rm=TRUE, ties=&#39;NA&#39;) } # 2e et 2e l&#39;année dernière : fenêtre modale taille 3 for(l in c(2, ncol(val)-1)) { val.c[,l] &lt;- apply(val.c[,(l-1):(l+1)], 1, modal, na.rm=TRUE, ties=&#39;NA&#39;) } message(&quot;done&quot;) } val &lt;- val.c # valeurs nettoyées -&gt; valuers actuelles # 3. nettoyage final regexp ---- # ajouter des années sans observation (cartes) val &lt;- cbind(val, matrix(nrow=nrow(val), ncol=length(no.map.cols), dimnames=list(NULL, no.map.cols))) # ordonner correctement val &lt;- val[, col.order] str &lt;- apply(val, 1, paste, collapse=&quot;&quot;) # convertir en chaînes de caractères str.c &lt;- gsub(&quot;NA&quot;, &quot;9&quot;, str) # remplacer NA avec 9 # Nettoyer jusqu&#39;à la convergence while(!identical(str, str.c)) { str &lt;- str.c # 3.1 remplacer NA par la classe précédente str.c &lt;- gsub(paste0(NONFOR, &quot;9&quot;), paste0(NONFOR, NONFOR), str.c) str.c &lt;- gsub(paste0(FOREST, &quot;9&quot;), paste0(FOREST, FOREST), str.c) } str &lt;- &quot;&quot; # Nettoyer jusqu&#39;à la convergence while(!identical(str, str.c)) { str &lt;- str.c # 3.2 remplacer NA par la classe suivante str.c &lt;- gsub(paste0(&quot;9&quot;, NONFOR), paste0(NONFOR, NONFOR), str.c) str.c &lt;- gsub(paste0(&quot;9&quot;, FOREST), paste0(FOREST, FOREST), str.c) } str &lt;- &quot;&quot; # Nettoyer jusqu&#39;à la convergence while(!identical(str, str.c)) { str &lt;- str.c # 3.3 suppression des 1 isolés jusqu&#39;à une longueur de 10 (régénération qui est à nouveau perdue) str.c &lt;-gsub(paste0(&quot;^(.*&quot;, NONFOR, &quot;)&quot;, FOREST, &quot;(&quot;, FOREST, &quot;{0,9}&quot;, NONFOR, &quot;.*)$&quot;), paste0(&quot;\\\\1&quot;, NONFOR, &quot;\\\\2&quot;), str.c) } str &lt;- &quot;&quot; # Nettoyer jusqu&#39;à la convergence while(!identical(str, str.c)) { str &lt;- str.c # 3.4 considérer la régénération seulement comme une forêt à partir de 10 ans # avant qu&#39;elle ne soit considérée comme une régénération potentielle PREGEN str.c &lt;-gsub(paste0(&quot;^(.*&quot;,NONFOREST,PREGEN, &quot;{0,8})&quot;, FOREST, &quot;(.*)$&quot;), paste0(&quot;\\\\1&quot;, PREGEN, &quot;\\\\2&quot;), str.c) } str &lt;- &quot;&quot; # Reconvertir en matrice numérique val &lt;- matrix(as.numeric(unlist(strsplit(str.c, &quot;&quot;))), ncol=ncol(val), byrow=TRUE) val[val==9] &lt;- NA # remplacer les 9 avec NA colnames(val) &lt;- col.order val[,grepl(&quot;M$&quot;, colnames(val))] # et extraire les données pour les années des cartes } # Sauvegarder le résultat ------------------------------- values(maps) &lt;- maps.values.clean # supprimer la première et la dernière carte &quot;non nettoyée&quot; writeRaster(dropLayer(maps, c(1,nlayers(maps))), filename=paste0(CLN.DIR, &quot;/FC&quot;, COV.FC, &quot;/&quot;, path, &quot;/&quot;, map.names[2:(nlayers(maps)-1)], &quot;c.tif&quot;), bylayer=TRUE, format=&quot;GTiff&quot;, datatype=&quot;INT2U&quot;, overwrite=TRUE) # écrire des trajectoires dans un fichier texte maps.strings &lt;- apply(maps.values.clean, 1, paste, collapse=&quot;&quot;) sink(paste0(CLN.DIR, &quot;/FC&quot;, COV.FC, &quot;/&quot;, path, &quot;/Trajectories.txt&quot;)) print(table(maps.strings)) sink() } # Nettoyage spatiale d&#39;une série d&#39;images ------------------------------------- # # @description Supprimer les pixels de forêt/déforestion/régénération qui n&#39;ont # JAMAIS fait partie de &quot;forêt &gt; 0,5 ha&quot; depuis 2003 # # @param maps Série des cartes # @param exclude Cartes à ignorer # @param size Nombre minimal de pixels connectés # @param connectedness Nombre de pixels qui comptent comme connectés # # @return -- # # clean.spatial.forest &lt;- function(maps, exclude = NULL, size=6, connectedness=8){ tmp1 &lt;- tempfile(pattern = &quot;&quot;, fileext = &quot;.tif&quot;) tmp2 &lt;- tempfile(pattern = &quot;&quot;, fileext = &quot;.tif&quot;) fcc.map &lt;- apply(maps[[-exclude]][], 1, paste, collapse=&quot;&quot;) onceforest.map &lt;- raster(maps) onceforest.map[] &lt;- NA # créer une carte &quot;forêt une fois&quot; onceforest.map[grepl(paste0(&quot;^&quot;, NONFOR, &quot;*$&quot;), fcc.map)] &lt;- NONFOR onceforest.map[grepl(paste0(&quot;^.*&quot;, FOREST, &quot;.*$&quot;), fcc.map)] &lt;- FOREST onceforest.map[grepl(paste0(&quot;^.*&quot;, PREGEN, &quot;.*$&quot;), fcc.map)] &lt;- FOREST writeRaster(onceforest.map, tmp1) # supprimer les parcelles de forêt isolées &lt; xy ha system(paste0(&quot;gdal_sieve.py -st &quot;, size, &quot; -&quot;, connectedness, &quot; -nomask &quot;, tmp1, &quot; &quot;, tmp2)) onceforest.map.clean &lt;- raster(tmp2) onceforest.map.clean[onceforest.map.clean == -2147483648] &lt;- NA # masquer les cartes avec cette carte forestière nettoyée maps.clean &lt;- mask(maps, onceforest.map.clean, maskvalue=NONFOR, updatevalue=NONFOR) writeRaster(maps.clean, filename=paste0(CLN.DIR, &quot;/FC&quot;, COV.FC, &quot;/TGO/&quot;, names(maps.clean), &quot;f.tif&quot;), bylayer=TRUE, format=&quot;GTiff&quot;, datatype=&quot;INT2U&quot;, overwrite=TRUE) } # COMMENCER LE TRAITEMENT ##################################################### # changer l&#39;étendue de p192_2019 à l&#39;étendue des autres images p192 ----------- # TODO : à faire déjà dans 01_SSTS/01_data/_src/prep-Landsat.R extend(brick(paste0(RAW.DIR, &quot;/FC&quot;, COV.FC, &quot;/p192/p192_2019_F&quot;, COV.FC, &quot;r.tif&quot;)), raster(paste0(RAW.DIR, &quot;/FC&quot;, COV.FC, &quot;/p192/p192_2018_F&quot;, COV.FC, &quot;r.tif&quot;)), filename=paste0(RAW.DIR, &quot;/FC&quot;, COV.FC, &quot;/p192/p192_2019_F&quot;, COV.FC, &quot;rt.tif&quot;), format=&quot;GTiff&quot;, datatype=&quot;INT2U&quot;) file.rename(paste0(RAW.DIR, &quot;/FC&quot;, COV.FC, &quot;/p192/p192_2019_F&quot;, COV.FC, &quot;rt.tif&quot;), paste0(RAW.DIR, &quot;/FC&quot;, COV.FC, &quot;/p192/p192_2019_F&quot;, COV.FC, &quot;r.tif&quot;)) # Nettoyage temporel des chemins ---------------------------------------------- for(path in c(&quot;p192&quot;, &quot;p193&quot;, &quot;p194&quot;)) { clean.temporal(path) } # Fusionner les cartes p192, p193 et p194 pour les dates conjointes ----------- for(year in YEARS.JNT) { merge(mask(crop(brick(paste0(CLN.DIR, &quot;/FC&quot;, COV.FC, &quot;/p193/p193_&quot;, year, &quot;_F&quot;, COV.FC, &quot;c.tif&quot;)), TGO), TGO), mask(crop(brick(paste0(CLN.DIR, &quot;/FC&quot;, COV.FC, &quot;/p192/p192_&quot;, year, &quot;_F&quot;, COV.FC, &quot;c.tif&quot;)), TGO), TGO), mask(crop(brick(paste0(CLN.DIR, &quot;/FC&quot;, COV.FC, &quot;/p194/p194_&quot;, year, &quot;_F&quot;, COV.FC, &quot;c.tif&quot;)), TGO), TGO), filename=paste0(CLN.DIR, &quot;/FC&quot;, COV.FC, &quot;/TGO/TGO_&quot;, year, &quot;_F&quot;, COV.FC, &quot;c.tif&quot;), overwrite=TRUE) } # Spatial cleaning of results (only from 2003 onwards) ----------------------------------- # exclure la première couche (1987) pour la création du masque forêt/non-forêt clean.spatial.forest(maps = stack(dir(paste0(CLN.DIR, &quot;/FC&quot;, COV.FC, &quot;/TGO&quot;), pattern = &quot;c\\\\.tif&quot;, full.names = TRUE)), exclude = c(1), size = 6, connectedness = 8) "],
["03_validate-FC-maps.html", "", " 4.1.3 Validation des cartes La validation des cartes forêt/non-forêt se fait à l’aide de parcelles de validation sur lesquelles l’occupation du sol (forêt/terre boisée/non-forêt) a été déterminée par des photo-interprètes sur la base d’images Landsat pour les années 1987, 2003, 2015 et 2018. Pour ces parcelles, dans un premier temps, les classes correspondantes sont lues à partir des cartes. Ensuite, des matrices d’erreur sont générées, celles de la classification forêt/non-forestière pour les différentes années, ainsi que celles de l’évolution de la couverture terrestre sur différentes périodes. Ces matrices d’erreurs constituent la base de l’analyse de la précision des cartes. Example Le tableau suivant montre la matrice d’erreur des transitions de l’occupation du sol 2003 – 2018. Dans les colonnes sont les classes attribuées par les photo-interprètes, dans les lignes les classes selon les cartes forêt/non-forêt nettoyées. xFxF xFxN xNxF xNxN xFxF 536 52 80 105 xFxN 36 80 3 52 xNxF 23 0 73 29 xNxN 35 84 50 1175 MCF/03_validate-fc-maps.R ############################################################################### # 03_validate-fc-maps.R: validation des cartes forêt/non-forêt nettoyées # ----------------------------------------------------------------------------- # Bern University of Applied Sciences # Oliver Gardi, &lt;oliver.gardi@bfh.ch&gt; # 13 Mai 2020 # Définitions des variables =================================================== COV.FC &lt;- 30 REF.DIR &lt;- DIR.MRV.MCF.REF RAW.DIR &lt;- DIR.MRV.MCF.RAW CLN.DIR &lt;- DIR.MRV.MCF.CLN VAL.DIR &lt;- DIR.MRV.MCF.VAL TPS.DIR &lt;- DIR.SST.BDD.TPS VPS.DIR &lt;- DIR.SST.BDD.VPS ct &lt;- list() # liste des matrices d&#39;érreurs (tableaux de confusion) # Préparations ================================================================ # Charger cartes et points d&#39;entraînment -------- # Cartes brutes et cartes néttoyées maps &lt;- stack(c(paste0(RAW.DIR, &quot;/FC&quot;, COV.FC, &quot;/TGO/TGO_&quot;, YEARS.REF, &quot;_F&quot;, COV.FC, &quot;r.tif&quot;), paste0(CLN.DIR, &quot;/FC&quot;, COV.FC, &quot;/TGO/TGO_&quot;, YEARS.REF, &quot;_F&quot;, COV.FC, &quot;c.tif&quot;))) names(maps) &lt;- sub(&quot;TGO\\\\_&quot;, &quot;X&quot;, sub(&quot;\\\\_F[[:digit:]]{2}&quot;, &quot;&quot;, names(maps))) # Points d&#39;entraînement: Couverture des houppiers ... train.plots &lt;- readOGR(paste0(TPS.DIR, &quot;/COV_parcelles.shp&quot;)) train.points &lt;- SpatialPointsDataFrame(gCentroid(train.plots, byid=TRUE), # polygons -&gt; points data.frame(COV=train.plots$ccov)) # seulement COV # ... et classes forêt/non-forêt dans les cartes 2018 train.points &lt;- raster::extract(maps[[c(&quot;X2018r&quot;,&quot;X2018c&quot;)]], train.points, sp=TRUE) # supprimer lignes avec NA train.points &lt;- train.points[rowSums(is.na(train.points@data)) == 0, ] # charger points de validation ------------------ # Points de validation: classes forêt/non-forêt 1987, 2003, 2015, 2018 ... val.plots &lt;- readOGR(paste0(VPS.DIR, &quot;/UOT_parcelles.shp&quot;)) val.points &lt;- SpatialPointsDataFrame(gCentroid(val.plots, byid=TRUE), data.frame(author=sub(&quot;^.*\\\\/&quot;, &quot;&quot;, val.plots$author), V1987=as.numeric(substr(val.plots$lc_87, 1, 1)), V2003=as.numeric(substr(val.plots$lc_03, 1, 1)), V2015=as.numeric(substr(val.plots$lc_15, 1, 1)), V2018=as.numeric(substr(val.plots$lc_18, 1, 1)))) # ... et classes correspondantes dans les cartes val.points &lt;- raster::extract(maps, val.points, sp=TRUE) # supprimer lignes avec NA val.points &lt;- val.points[rowSums(is.na(val.points@data)) == 0, ] # Ajustements terres boisées (classe 2) --------- if(COV.FC == 30) { # terre boisée -&gt; non-forêt val.points$V1987[val.points$V1987 == 2] &lt;- NONFOR val.points$V2003[val.points$V2003 == 2] &lt;- NONFOR val.points$V2015[val.points$V2015 == 2] &lt;- NONFOR val.points$V2018[val.points$V2018 == 2] &lt;- NONFOR } if(COV.FC == 10) { # terre boisée -&gt; forêt val.points$V1987[val.points$V1987 == 2] &lt;- FOREST val.points$V2003[val.points$V2003 == 2] &lt;- FOREST val.points$V2015[val.points$V2015 == 2] &lt;- FOREST val.points$V2018[val.points$V2018 == 2] &lt;- FOREST } # Ajustements nuages/ombre (classe 4) ----------- val.points$V1987r &lt;- val.points$V1987c &lt;- val.points$V1987; val.points$V2003r &lt;- val.points$V2003c &lt;- val.points$V2003; val.points$V2015r &lt;- val.points$V2015c &lt;- val.points$V2015; val.points$V2018r &lt;- val.points$V2018c &lt;- val.points$V2018; # Prendre la classe dans les carte brutes ... val.points$V1987r[val.points$V1987 %in% c(2,4)] &lt;- val.points$X1987r[val.points$V1987 %in% c(2,4)] val.points$V2003r[val.points$V2003 %in% c(2,4)] &lt;- val.points$X2003r[val.points$V2003 %in% c(2,4)] val.points$V2015r[val.points$V2015 %in% c(2,4)] &lt;- val.points$X2015r[val.points$V2015 %in% c(2,4)] val.points$V2018r[val.points$V2018 %in% c(2,4)] &lt;- val.points$X2018r[val.points$V2018 %in% c(2,4)] # ... et les cartes nettoyées val.points$V1987c[val.points$V1987 %in% c(2,4)] &lt;- val.points$X1987c[val.points$V1987 %in% c(2,4)] val.points$V2003c[val.points$V2003 %in% c(2,4)] &lt;- val.points$X2003c[val.points$V2003 %in% c(2,4)] val.points$V2015c[val.points$V2015 %in% c(2,4)] &lt;- val.points$X2015c[val.points$V2015 %in% c(2,4)] val.points$V2018c[val.points$V2018 %in% c(2,4)] &lt;- val.points$X2018c[val.points$V2018 %in% c(2,4)] # Ajustements de la régénération potentielle ---- # changer &quot;régénération potentielle&quot; à la classe identifiée dans les parcelles de validation val.points$X1987c[val.points$X1987c == PREGEN] &lt;- val.points$V1987c[val.points$X1987c == PREGEN] val.points$X2003c[val.points$X2003c == PREGEN] &lt;- val.points$V2003c[val.points$X2003c == PREGEN] val.points$X2015c[val.points$X2015c == PREGEN] &lt;- val.points$V2015c[val.points$X2015c == PREGEN] val.points$X2018c[val.points$X2018c == PREGEN] &lt;- val.points$V2018c[val.points$X2018c == PREGEN] # Non-forêt où &quot;régénération potentielle&quot; dans la carte et &quot;terre boisée&quot; dans les parcelles de validation val.points$X1987c[val.points$X1987c == NONFOR] &lt;- val.points$V1987c[val.points$X1987c == 2] &lt;- NONFOR val.points$X2003c[val.points$X2003c == NONFOR] &lt;- val.points$V2003c[val.points$X2003c == 2] &lt;- NONFOR val.points$X2015c[val.points$X2015c == NONFOR] &lt;- val.points$V2015c[val.points$X2015c == 2] &lt;- NONFOR val.points$X2018c[val.points$X2018c == NONFOR] &lt;- val.points$V2018c[val.points$X2018c == 2] &lt;- NONFOR val.points@data &lt;- mutate_all(val.points@data, as.factor) # Matrices d&#39;erreurs ========================================================== # carte 2018 vs. carte référence 2018 ---------- ref.map &lt;- raster(paste0(REF.DIR, &quot;/FC&quot;, COV.FC, &quot;/TGO_2018_FC&quot;, COV.FC, &quot;_R.tif&quot;)) ct[[&quot;MAP_REF.18r&quot;]] &lt;- confusionMatrix(as.factor(maps$X2018r[]), as.factor(ref.map[])) pdf(paste0(VAL.DIR, &quot;/FC2018_vs_COV2018_FC&quot;, COV.FC, &quot;.pdf&quot;)) plot(factor(train.points$X2018r, labels=c(&quot;Forest&quot;, &quot;Non-Forest&quot;)) ~ train.points$COV, xlab=&quot;Tree Cover 2018&quot;, ylab=&quot;Forest Cover Map 2018&quot;) dev.off() pdf(paste0(VAL.DIR, &quot;/COV2018_vs_FC2018_FC&quot;, COV.FC, &quot;.pdf&quot;)) plot(train.points$COV ~ factor(train.points$X2018r, labels=c(&quot;Forest&quot;, &quot;Non-Forest&quot;)), xlab=&quot;Forest Cover Map 2018&quot;, ylab=&quot;Tree Cover 2018&quot;) dev.off() # Map validation / confusion matrices ---------------------------------- confMat &lt;- function(xtrans, vtrans) { levels &lt;- unique(c(xtrans, vtrans)) return(confusionMatrix(factor(xtrans, levels=levels[order(levels)]), factor(vtrans, levels=levels[order(levels)]))) } # val.points.bu &lt;- val.points # backup val.points &lt;- val.points[!val.points$author %in% c(&quot;7_Aklasson_TOLEBA&quot;, &quot;8_Yawo_KONKO&quot;, &quot;2_Mamalnassoh_ABIGUIME&quot;), ] # check for authors whose validation points reduce precision # print(confMat(paste0(&quot;x&quot;, tmp$X2003c, tmp$X2015c, tmp$X2018c), # paste0(&quot;x&quot;, tmp$VX2003, tmp$VX2015, tmp$VX2018))$overall) # # for(author in unique(tmp$author)) { # print(author) # print(confMat(paste0(&quot;x&quot;, tmp$X2003c[tmp$author != author], tmp$X2015c[tmp$author != author], tmp$X2018c[tmp$author != author]), # paste0(&quot;x&quot;, tmp$VX2003[tmp$author != author], tmp$VX2015[tmp$author != author], tmp$VX2018[tmp$author != author]))$overall) # } for(t in c(&quot;r&quot;, &quot;c&quot;)) { # confusion matrices for individual dates ct[[paste0(&quot;MAP_VAL.87&quot;, t)]] &lt;- confMat(paste0(val.points[[paste0(&quot;X1987&quot;, t)]], &quot;x&quot;, &quot;x&quot;, &quot;x&quot; ), paste0(val.points[[paste0(&quot;V1987&quot;, t)]], &quot;x&quot;, &quot;x&quot;, &quot;x&quot; )) ct[[paste0(&quot;MAP_VAL.03&quot;, t)]] &lt;- confMat(paste0(&quot;x&quot;, val.points[[paste0(&quot;X2003&quot;, t)]], &quot;x&quot;, &quot;x&quot; ), paste0(&quot;x&quot;, val.points[[paste0(&quot;V2003&quot;, t)]], &quot;x&quot;, &quot;x&quot; )) ct[[paste0(&quot;MAP_VAL.15&quot;, t)]] &lt;- confMat(paste0(&quot;x&quot;, &quot;x&quot;, val.points[[paste0(&quot;X2015&quot;, t)]], &quot;x&quot; ), paste0(&quot;x&quot;, &quot;x&quot;, val.points[[paste0(&quot;V2015&quot;, t)]], &quot;x&quot; )) ct[[paste0(&quot;MAP_VAL.18&quot;, t)]] &lt;- confMat(paste0(&quot;x&quot;, &quot;x&quot;, &quot;x&quot;, val.points[[paste0(&quot;X2018&quot;, t)]]), paste0(&quot;x&quot;, &quot;x&quot;, &quot;x&quot;, val.points[[paste0(&quot;V2018&quot;, t)]])) # confusion matrices for 2-date transitions ct[[paste0(&quot;MAP_VAL.87.03&quot;, t)]] &lt;- confMat(paste0(val.points[[paste0(&quot;X1987&quot;, t)]], val.points[[paste0(&quot;X2003&quot;, t)]], &quot;x&quot;, &quot;x&quot; ), paste0(val.points[[paste0(&quot;V1987&quot;, t)]], val.points[[paste0(&quot;V2003&quot;, t)]], &quot;x&quot;, &quot;x&quot; )) ct[[paste0(&quot;MAP_VAL.03.15&quot;, t)]] &lt;- confMat(paste0(&quot;x&quot;, val.points[[paste0(&quot;X2003&quot;, t)]], val.points[[paste0(&quot;X2015&quot;, t)]], &quot;x&quot; ), paste0(&quot;x&quot;, val.points[[paste0(&quot;V2003&quot;, t)]], val.points[[paste0(&quot;V2015&quot;, t)]], &quot;x&quot; )) ct[[paste0(&quot;MAP_VAL.15.18&quot;, t)]] &lt;- confMat(paste0(&quot;x&quot;, &quot;x&quot;, val.points[[paste0(&quot;X2015&quot;, t)]], val.points[[paste0(&quot;X2018&quot;, t)]]), paste0(&quot;x&quot;, &quot;x&quot;, val.points[[paste0(&quot;V2015&quot;, t)]], val.points[[paste0(&quot;V2018&quot;, t)]])) ct[[paste0(&quot;MAP_VAL.87.18&quot;, t)]] &lt;- confMat(paste0(val.points[[paste0(&quot;X1987&quot;, t)]], &quot;x&quot;, &quot;x&quot;, val.points[[paste0(&quot;X2018&quot;, t)]]), paste0(val.points[[paste0(&quot;V1987&quot;, t)]], &quot;x&quot;, &quot;x&quot;, val.points[[paste0(&quot;V2018&quot;, t)]])) ct[[paste0(&quot;MAP_VAL.03.18&quot;, t)]] &lt;- confMat(paste0(&quot;x&quot;, val.points[[paste0(&quot;X2003&quot;, t)]], &quot;x&quot;, val.points[[paste0(&quot;X2018&quot;, t)]]), paste0(&quot;x&quot;, val.points[[paste0(&quot;V2003&quot;, t)]], &quot;x&quot;, val.points[[paste0(&quot;V2018&quot;, t)]])) # confusion matrices for 3-date transition ct[[paste0(&quot;MAP_VAL.03.15.18&quot;, t)]] &lt;- confMat(paste0(&quot;x&quot;,val.points[[paste0(&quot;X2003&quot;, t)]], val.points[[paste0(&quot;X2015&quot;, t)]], val.points[[paste0(&quot;X2018&quot;, t)]]), paste0(&quot;x&quot;, val.points[[paste0(&quot;V2003&quot;, t)]], val.points[[paste0(&quot;V2015&quot;, t)]], val.points[[paste0(&quot;V2018&quot;, t)]])) # confusion matrices for 4-date transition ct[[paste0(&quot;MAP_VAL.87.03.15.18&quot;, t)]] &lt;- confMat(paste0(val.points[[paste0(&quot;X1987&quot;, t)]], val.points[[paste0(&quot;X2003&quot;, t)]], val.points[[paste0(&quot;X2015&quot;, t)]], val.points[[paste0(&quot;X2018&quot;, t)]]), paste0(val.points[[paste0(&quot;V1987&quot;, t)]], val.points[[paste0(&quot;V2003&quot;, t)]], val.points[[paste0(&quot;V2015&quot;, t)]], val.points[[paste0(&quot;V2018&quot;, t)]])) } # Validation RapidEye map 2015 -------------------------------- rey &lt;- raster(paste0(DATA.DIR, &quot;/RapidEye/TGO_30m.tif&quot;)) names(rey) &lt;- &quot;R2015&quot; val.points &lt;- raster::extract(rey, val.points, sp=TRUE) val.points$Rr2015 &lt;- 3 val.points$Rr2015[val.points$R2015 %in% c(11, 12, 16, 18, 19)] &lt;- 1 val.points$VR2015 &lt;- val.points$V2015 val.points$VR2015[val.points$V2015 == 2] &lt;- val.points$Rr2015[val.points$V2015 == 2] ct[[&quot;REY_VAL.15&quot;]] &lt;- confMat(paste0(&quot;x&quot;, &quot;x&quot;, val.points$Rr2015, &quot;x&quot; ), paste0(&quot;x&quot;, &quot;x&quot;, val.points$VR2015, &quot;x&quot; )) # Write confusion tables ------------------------------------- save(ct, file=paste0(FCC.VAL.DIR, &quot;/FC&quot;, COV.FC, &quot;_flex_ConfTab.RData&quot;)) # write error matrices to Excel File write.xlsx(lapply(ct, &quot;[[&quot;, &quot;table&quot;), file=paste0(FCC.VAL.DIR, &quot;/FC&quot;, COV.FC, &quot;_flex_ConfTab.xlsx&quot;), colnames=TRUE, overwrite=TRUE) "],
["04_FC-maps-accuracy.html", "", " 4.1.4 Analyse de la précision La précision des cartes est déterminée selon la procédure de Olofsson et al. (2014). Le script utilisé est basé sur une implémentation des méthodes dans OpenForis. Le calcul de la précision se fait sur la matrice d’erreur d’une part et les surfaces couvertes par les catégories correspondantes d’autre part. Avec les informations sur les surfaces, la matrice d’erreur est extrapolée en une matrice d’erreur proportionnelle, qui indique dans les cellules les proportions de la surface totale. Sur cette base les indices de précision sont calculés: concrètement la précision globale, le Kappa de Cohen, la précision du producteur et la précision de l’utilisateur, y compris la variance et l’intervalle de confiance à 95 %. D’autre part, la matrice d’erreur proportionnelle est extrapolée à la superficie totale pour obtenir des estimations de superficie des différentes catégories, y compris les erreurs types et les intervalles de confiance. Example Le nombre de pixels cartographiés et matrice d’erreur des transitions de l’occupation du sol 2003 – 2018. Dans les colonnes sont les classes attribuées par les photo-interprètes, dans les lignes les classes selon les cartes forêt/non-forêt nettoyées. Pixels xFxF xFxN xNxF xNxN xFxF 12 590 594 536 52 80 105 xFxN 2 509 971 36 80 3 52 xNxF 1 637 331 23 0 73 29 xNxN 46 599 650 35 84 50 1175 La matrice d’erreur proportionelle aux surfaces cartographiées. La somme de tous les cellules est 1 (100%). xFxF xFxN xNxF xNxN xFxF 0.138 0.013 0.021 0.027 xFxN 0.008 0.019 0.001 0.012 xNxF 0.005 0 0.015 0.006 xNxN 0.019 0.046 0.027 0.643 Les indices de précision globales sont: Précision globale: 81.5% (± 1.5% intervalle de confiance) Cohen’s Kappa: 59.3% Indices de précision et le surfaces estimées par catégories (± intervalle de confiance à 95%) Surface cartes Prop. cartes Proportion ajustée Surface ajustée Précision utilisateur Précision producteur xFxF 1’133’153 0.20 0.17 ± 0.01 969’621 ± 54’104 0.69 ± 0.03 0.81 ± 0.03 xFxN 225’897 0.04 0.08 ± 0.01 444’034 ± 60’300 0.47 ± 0.08 0.24 ± 0.04 xNxF 147’360 0.03 0.06 ± 0.01 363’320 ± 50’777 0.58 ± 0.09 0.24 ± 0.04 xNxN 4’193’969 0.74 0.69 ± 0.01 3’923’405 ± 81’517 0.87 ± 0.02 0.94 ± 0.01 MCF/04_fc-maps-accuracy.R ########################################################################## # NERF_Togo/FCC/7_fc-maps-accuracy.R: validate clean forest cover maps # ------------------------------------------------------------------------ # Bern University of Applied Sciences # Oliver Gardi, &lt;oliver.gardi@bfh.ch&gt; # 20 May 2019 # based on OpenForis implementation of Olofsson et al. (2014), written by # Antonia Ortmann, 20 October, 2014 # Source: https://github.com/openforis/accuracy-assessment/blob/master/Rscripts/error_matrix_analysis.R VALSET &lt;- &quot;FC30_flex&quot; # Function for estimating accuracies ---------------------------------- accuracy.estimate &lt;- function(areas.map, error.matrix, filename=NULL, pixelsize=30^2/10000) { # remove &quot;x&quot; from the category colnames(error.matrix) &lt;- rownames(error.matrix) &lt;- gsub(&quot;x&quot;, &quot;&quot;, colnames(error.matrix)) # match category order maparea &lt;- areas.map[match(rownames(error.matrix), names(areas.map))] ma &lt;- error.matrix dyn &lt;- names(maparea) # calculate the area proportions for each map class aoi &lt;- sum(maparea) propmaparea &lt;- maparea/aoi # convert the absolute cross tab into a probability cross tab ni. &lt;- rowSums(ma) # number of reference points per map class propma &lt;- as.matrix(ma/ni. * as.vector(propmaparea)) propma[is.nan(propma)] &lt;- 0 # for classes with ni. = 0 # estimate the accuracies now OA &lt;- sum(diag(propma)) # overall accuracy (Eq. 1 in Olofsson et al. 2014) pe &lt;- 0 # Agreement by chance ... for (i in 1:length(dyn)) { pe &lt;- pe + sum(propma[i,]) * sum(propma[,i]) } K &lt;- (OA - pe) / (1 - pe) # ... for Cohen&#39;s Kappa UA &lt;- diag(propma) / rowSums(propma) # user&#39;s accuracy (Eq. 2 in Olofsson et al. 2014) PA &lt;- diag(propma) / colSums(propma) # producer&#39;s accuracy (Eq. 3 in Olofsson et al. 2014) # estimate confidence intervals for the accuracies V_OA &lt;- sum(as.vector(propmaparea)^2 * UA * (1 - UA) / (ni. - 1), na.rm=T) # variance of overall accuracy (Eq. 5 in Olofsson et al. 2014) V_UA &lt;- UA * (1 - UA) / (rowSums(ma) - 1) # variance of user&#39;s accuracy (Eq. 6 in Olofsson et al. 2014) # variance of producer&#39;s accuracy (Eq. 7 in Olofsson et al. 2014) N.j &lt;- array(0, dim=length(dyn)) aftersumsign &lt;- array(0, dim=length(dyn)) for(cj in 1:length(dyn)) { N.j[cj] &lt;- sum(maparea / ni. * ma[, cj], na.rm=T) aftersumsign[cj] &lt;- sum(maparea[-cj]^2 * ma[-cj, cj] / ni.[-cj] * ( 1 - ma[-cj, cj] / ni.[-cj]) / (ni.[-cj] - 1), na.rm = T) } V_PA &lt;- 1/N.j^2 * ( maparea^2 * (1-PA)^2 * UA * (1-UA) / (ni.-1) + PA^2 * aftersumsign ) V_PA[is.nan(V_PA)] &lt;- 0 # proportional area estimation propAreaEst &lt;- colSums(propma) # proportion of area (Eq. 8 in Olofsson et al. 2014) AreaEst &lt;- propAreaEst * sum(maparea) # estimated area # standard errors of the area estimation (Eq. 10 in Olofsson et al. 2014) V_propAreaEst &lt;- array(0, dim=length(dyn)) for (cj in 1:length(dyn)) { V_propAreaEst[cj] &lt;- sum((as.vector(propmaparea) * propma[, cj] - propma[, cj] ^ 2) / ( rowSums(ma) - 1)) } V_propAreaEst[is.na(V_propAreaEst)] &lt;- 0 # produce result tables res &lt;- list() res$PREDICTED_PX &lt;- as.table(maparea) res$ERROR_MATRIX &lt;- ma res$ERROR_MATRIX_PROP &lt;- round(propma, 3) res$OVERALL_ACC &lt;- data.frame(accuracy=round(c(OA, K), 3), CI=round(c(1.96 * sqrt(V_OA), NA), 3), row.names=c(&quot;OA&quot;, &quot;Kappa&quot;)) res$CLASSES_ACC &lt;- data.frame(maparea=round(maparea * pixelsize, 3)) # in ha res$CLASSES_ACC$prop_maparea &lt;- round(propmaparea, 3) res$CLASSES_ACC$adj_proparea &lt;- round(propAreaEst, 3) res$CLASSES_ACC$CI_adj_proparea &lt;- round(1.96 * sqrt(V_propAreaEst), 3) res$CLASSES_ACC$adj_area &lt;- round(propAreaEst * aoi * pixelsize, 3) # in ha res$CLASSES_ACC$CI_adj_area &lt;- round(1.96 * sqrt(V_propAreaEst) * aoi * pixelsize, 3) # in ha res$CLASSES_ACC$UA &lt;- round(UA, 3) res$CLASSES_ACC$CI_UA &lt;- round(1.96 * sqrt(V_UA), 3) res$CLASSES_ACC$PA &lt;- round(PA, 3) res$CLASSES_ACC$CI_PA &lt;- round(1.96 * sqrt(V_PA), 3) # write results to Excel File if(!is.null(filename)) { write.xlsx(res, file=filename, col.names=TRUE, row.names=TRUE, overwrite=TRUE) } return(res) } # DO THE WORK ---------------------------------- # load the error matrices (R object ct) load(paste0(FCC.VAL.DIR, &quot;/ConfTab_&quot;, VALSET, &quot;.RData&quot;)) # load the predictions and convert &quot;potential regeneration (2)&quot; to &quot;non-forest (3)&quot; fc.2003 &lt;- brick(paste0(FCC.CLN.DIR, &quot;/FC30/TGO/TGO_2003_F30cf.tif&quot;)); fc.2003[fc.2003==2] &lt;- 3 fc.2015 &lt;- brick(paste0(FCC.CLN.DIR, &quot;/FC30/TGO/TGO_2015_F30cf.tif&quot;)); fc.2015[fc.2015==2] &lt;- 3 fc.2018 &lt;- brick(paste0(FCC.CLN.DIR, &quot;/FC30/TGO/TGO_2018_F30cf.tif&quot;)); fc.2018[fc.2018==2] &lt;- 3 # create 3-date transition map fcc &lt;- 100 * fc.2003 + 10 * fc.2015 + 1 * fc.2018 # get pixel counts (takes time) and separate for different dates / transitions freq &lt;- table(fcc[]) tmp &lt;- as.numeric(freq); names(tmp) &lt;- names(freq); freq &lt;- tmp freq.03.15.18 &lt;- c(freq[&quot;111&quot;], freq[&quot;113&quot;], freq[&quot;131&quot;], freq[&quot;133&quot;], freq[&quot;311&quot;], freq[&quot;313&quot;], freq[&quot;331&quot;], freq[&quot;333&quot;]) names(freq.03.15.18) &lt;- c(&quot;111&quot;, &quot;113&quot;, &quot;131&quot;, &quot;133&quot;, &quot;311&quot;, &quot;313&quot;, &quot;331&quot;, &quot;333&quot;); freq.03.15.18[is.na(freq.03.15.18)] &lt;- 0 freq.03.18 &lt;- c(sum(freq[&quot;111&quot;],freq[&quot;131&quot;], na.rm=T), sum(freq[&quot;113&quot;],freq[&quot;133&quot;], na.rm=T), sum(freq[&quot;311&quot;],freq[&quot;331&quot;], na.rm=T), sum(freq[&quot;313&quot;],freq[&quot;333&quot;], na.rm=T)) names(freq.03.18) &lt;- c(&quot;11&quot;, &quot;13&quot;, &quot;31&quot;, &quot;33&quot;); freq.03.18[is.na(freq.03.18)] &lt;- 0 freq.03.15 &lt;- c(sum(freq[&quot;111&quot;],freq[&quot;113&quot;], na.rm=T), sum(freq[&quot;131&quot;],freq[&quot;133&quot;], na.rm=T), sum(freq[&quot;311&quot;],freq[&quot;313&quot;], na.rm=T), sum(freq[&quot;331&quot;],freq[&quot;333&quot;], na.rm=T)) names(freq.03.15) &lt;- c(&quot;11&quot;, &quot;13&quot;, &quot;31&quot;, &quot;33&quot;); freq.03.15[is.na(freq.03.15)] &lt;- 0 freq.15.18 &lt;- c(sum(freq[&quot;111&quot;],freq[&quot;311&quot;], na.rm=T), sum(freq[&quot;113&quot;],freq[&quot;313&quot;], na.rm=T), sum(freq[&quot;131&quot;],freq[&quot;331&quot;], na.rm=T), sum(freq[&quot;133&quot;],freq[&quot;333&quot;], na.rm=T)) names(freq.15.18) &lt;- c(&quot;11&quot;, &quot;13&quot;, &quot;31&quot;, &quot;33&quot;); freq.15.18[is.na(freq.15.18)] &lt;- 0 freq.03 &lt;- c(sum(freq[&quot;111&quot;],freq[&quot;131&quot;], freq[&quot;113&quot;],freq[&quot;133&quot;], na.rm=T), sum(freq[&quot;311&quot;],freq[&quot;331&quot;], freq[&quot;313&quot;],freq[&quot;333&quot;], na.rm=T)) names(freq.03) &lt;- c(&quot;1&quot;, &quot;3&quot;); freq.03[is.na(freq.03)] &lt;- 0 freq.15 &lt;- c(sum(freq[&quot;111&quot;],freq[&quot;311&quot;], freq[&quot;113&quot;],freq[&quot;313&quot;], na.rm=T), sum(freq[&quot;131&quot;],freq[&quot;331&quot;], freq[&quot;133&quot;],freq[&quot;333&quot;], na.rm=T)) names(freq.15) &lt;- c(&quot;1&quot;, &quot;3&quot;); freq.15[is.na(freq.15)] &lt;- 0 freq.18 &lt;- c(sum(freq[&quot;111&quot;],freq[&quot;311&quot;], freq[&quot;131&quot;],freq[&quot;331&quot;], na.rm=T), sum(freq[&quot;113&quot;],freq[&quot;313&quot;], freq[&quot;133&quot;],freq[&quot;333&quot;], na.rm=T)) names(freq.18) &lt;- c(&quot;1&quot;, &quot;3&quot;); freq.18[is.na(freq.18)] &lt;- 0 # create accuracy maps ------------------------------------ accuracy.estimate(freq.18, ct$MAP_VAL.18c$table , filename=paste0(FCC.VAL.DIR, &quot;/&quot;, VALSET, &quot;_Acc_18.xlsx&quot;)) accuracy.estimate(freq.15, ct$MAP_VAL.15c$table , filename=paste0(FCC.VAL.DIR, &quot;/&quot;, VALSET, &quot;_Acc_15.xlsx&quot;)) accuracy.estimate(freq.03, ct$MAP_VAL.03c$table , filename=paste0(FCC.VAL.DIR, &quot;/&quot;, VALSET, &quot;_Acc_03.xlsx&quot;)) accuracy.estimate(freq.03.15, ct$MAP_VAL.03.15c$table , filename=paste0(FCC.VAL.DIR, &quot;/&quot;, VALSET, &quot;_Acc_03-15.xlsx&quot;)) accuracy.estimate(freq.15.18, ct$MAP_VAL.15.18c$table , filename=paste0(FCC.VAL.DIR, &quot;/&quot;, VALSET, &quot;_Acc_15-18.xlsx&quot;)) accuracy.estimate(freq.03.18, ct$MAP_VAL.03.18c$table , filename=paste0(FCC.VAL.DIR, &quot;/&quot;, VALSET, &quot;_Acc_03-18.xlsx&quot;)) accuracy.estimate(freq.03.15.18, ct$MAP_VAL.03.15.18c$table , filename=paste0(FCC.VAL.DIR, &quot;/&quot;, VALSET, &quot;_Acc_03-15-18.xlsx&quot;)) # calculate forest cover, loss and gains --------------------- res &lt;- data.frame(year = c(2003, 2015, 2018), total.ha = c(freq.03[&quot;1&quot;], freq.15[&quot;1&quot;], freq.18[&quot;1&quot;]) * 30^2/10000, defor.ha = c(NA, freq.03.15[&quot;13&quot;], freq.15.18[&quot;13&quot;]) * 30^2/10000, regen.ha = c(NA, freq.03.15[&quot;31&quot;], freq.15.18[&quot;31&quot;]) * 30^2/10000) # Example from Olofsson et al. (2014) # ----------------------------------- # ct &lt;- matrix(data=c(66, 0, 5, 4, # 0, 55, 8, 12, # 1, 0, 153, 11, # 2, 1, 9, 313), # nrow=4, # byrow=TRUE, # dimnames = list(Prediction=c(&quot;FN&quot;, &quot;NF&quot;, &quot;FF&quot;, &quot;NN&quot;), Reference=c(&quot;FN&quot;, &quot;NF&quot;, &quot;FF&quot;, &quot;NN&quot;))) # # px &lt;- c(FN=200000, NF=150000, FF=3200000, NN=6450000) # # accuracy.estimate(px, ct) "],
["05_analyse-FC-maps.html", "", " 4.1.5 Analyse des cartes Les cartes forêt/non-forêt nettoyées de 1987 à 2018 sont utilisées pour créer des cartes des changements forestiers. En utilisant la fonction fc() les pixels qui passent d’une situation “non-forêt” ou “régénération potentielle” à “régénération” sont enregistrés comme gains de surface forestière dans l’année correspondante, les pixels qui passent d’une situation “forêt” ou “régénération” à “non-forêt” sont enregistrés comme perte de surface forestière. Sur cette base, pour chaque année disponible dans la série, une compilation des surfaces forestières, de la surface régénérée et de la régénération potentielle, ainsi que des gains et pertes de forêts dans la période précédente est effectuée. Le tableau de la superficie forestière sert à son tour de base pour déterminer les changements annuelles de la surface forestière sur différentes périodes de temps à l’aide de la fonction fcc(). En plus des changements annuelles absolues, les taux de changement correspondants sont calculés selon la formule suivante : \\(r = (1/(t2 - t1)) \\times ln(A2/A1)\\) (Puyravaud, 2003). La fonction plot.fc() affiche graphiquement le tableau des surfaces forestières et montre comment la surface forestière et la régénération se développent. La fonction plot.fcc() crée un diagramme en barres des gains et des pertes annuelles de la surface forestières sur certaines périodes de temps. Enfin, des cartes de la perte de forêts, de la régénération potentielle et de la régénération sont créées, avec les années de changement observées comme valeurs de pixel. Example Tableau des superfaces forestières pour le Togo La surface forestière initiale (existant depuis 1987), la superficie forestière secondaire (régénérée depuis 1987), la surface de la régénération potentielle (&lt; 10 ans) et la déforestation et l’accroissement enregistrés dans la période précédente. Tous les chiffres sont exprimés en hectares. Année Surface f. totale Surface f. initiale Surface f. secondaire Régén. potentielle Pertes surface f. Gains surface f. 1987 1’265’377 1’265’377 0 133’038 — — 2003 1’359’051 1’193’731 165’320 90’972 -71’646 165’320 2005 1’321’963 1’166’156 155’807 122’718 -37’088 0 2007 1’281’909 1’136’373 145’536 161’787 -40’154 101 2015 1’290’948 1’059’301 231’647 156’237 -99’560 108’600 2017 1’290’615 1’035’724 254’891 169’171 -39’503 39’170 2018 1’280’513 1’019’489 261’024 188’715 -27’027 16’925 Changement annuelle de la superficie forestière au Togo, pour 2003 – 2015, 2015 – 2018 et toute la période 2003 – 2018, montrant les pertes et les gains brutes des forêts, et le changement net de la surface forestière en hectares par an et les taux correspondants en %/an. Période Pertes (ha/an) Gains (ha/an) Ch. net (ha/an) Pertes (%/an) Gains (%/an) Ch. net (%/an) 2003 – 2015 -14’734 9’058 -5’675 -1.2 0.6 -0.4 2015 – 2018 -22’177 18’699 -3’478 -1.8 1.4 -0.3 2003 – 2018 -16’222 10’986 -5’236 -1.3 0.8 -0.4 Changement de la surface forestière du Togo 2003 – 2018 Changement de la superficie forestière (à gauche) ansi que les gains et les pertes annuelles pour les périodes 2003 – 2015 et 2015 – 2018 (à droite). Complexité de l’évolution de la superficie forestière en prenant l’exemple d’une petite partie de la région des Plateaux. Pertes de forêts 2003 – 2018 (jaune à orange) et croissance des forêts au cours de la même période (bleu à blanc). Le fond noir est non-forêt sur toute la période. MCF/05_analyse-fc-maps.R ########################################################################## # NERF_Togo/FCC/8_analyse-fc-maps.R: analyze clean forest cover maps # ------------------------------------------------------------------------ # Bern University of Applied Sciences # Oliver Gardi, &lt;oliver.gardi@bfh.ch&gt; # 20 May 2019 COV.FC &lt;- 30 # Function for building forest cover table --------------------------------- fc &lt;- function(map, aoi=NULL) { if(!is.null(aoi)) map &lt;- mask(crop(map, aoi), aoi) # convert non-forest to 0 # potential regeneration to 1 # regeneration to 2 # forest to 3 tab.fc &lt;- map[] tab.fc[tab.fc == 3] &lt;- 0 tab.fc[tab.fc == 1] &lt;- 3 tab.fc[tab.fc == 2] &lt;- 1 for(i in 2:ncol(tab.fc)) { tab.fc[!is.na(tab.fc[,i]) &amp; tab.fc[,i] == 3 &amp; !is.na(tab.fc[,i-1]) &amp; tab.fc[,i-1] &lt; 3, i] &lt;- 2 # mark as regeneration when it was non-forest or regeneration before } tab.fcc &lt;- tab.fc[,1:ncol(tab.fc)-1] tab.fcc[] &lt;- 0 for(i in 1:ncol(tab.fcc)) { tab.fcc[tab.fc[,i] &lt;= 1 &amp; tab.fc[,i+1] &gt;= 2, i] &lt;- 1 # mark forest gain tab.fcc[tab.fc[,i] &gt;= 2 &amp; tab.fc[,i+1] &lt;= 1, i] &lt;- 2 # mark forest loss } dates &lt;- as.numeric(substr(names(map), 2, 5)) fc &lt;- data.frame(year = dates, initi.ha = colSums(tab.fc==3, na.rm=TRUE) * 30^2/10000, secon.ha = colSums(tab.fc==2, na.rm=TRUE) * 30^2/10000, poten.ha = colSums(tab.fc==1, na.rm=TRUE) * 30^2/10000) fc$total.ha &lt;- fc$initi.ha + fc$secon.ha fc$defor.ha &lt;- -c(colSums(tab.fcc==2, na.rm=TRUE) * 30^2/10000, NA) fc$regen.ha &lt;- c(colSums(tab.fcc==1, na.rm=TRUE) * 30^2/10000, NA) return(fc) } fcc &lt;- function(fc.tab, years=fc.tab$year) { for(i in 1:(length(years)-1)) { fc.tab$defor.ha.yr[fc.tab$year==years[i]] &lt;- sum(fc.tab$defor.ha[fc.tab$year &gt;= years[i] &amp; fc.tab$year &lt; years[i+1]])/(years[i+1]-years[i]) fc.tab$regen.ha.yr[fc.tab$year==years[i]] &lt;- sum(fc.tab$regen.ha[fc.tab$year &gt;= years[i] &amp; fc.tab$year &lt; years[i+1]])/(years[i+1]-years[i]) fc.tab$netch.ha.yr[fc.tab$year==years[i]] &lt;- fc.tab$defor.ha.yr[fc.tab$year==years[i]] + fc.tab$regen.ha.yr[fc.tab$year==years[i]] fc.tab$defor.pc.yr[fc.tab$year==years[i]] &lt;- round(100 * 1/(years[i+1]-years[i]) * log((fc.tab$total.ha[fc.tab$year==years[i+1]] - sum(fc.tab$regen.ha[fc.tab$year &gt;= years[i] &amp; fc.tab$year &lt; years[i+1]]))/fc.tab$total.ha[fc.tab$year==years[i]]), 3) fc.tab$regen.pc.yr[fc.tab$year==years[i]] &lt;- round(100 * 1/(years[i+1]-years[i]) * log((fc.tab$total.ha[fc.tab$year==years[i+1]] - sum(fc.tab$defor.ha[fc.tab$year &gt;= years[i] &amp; fc.tab$year &lt; years[i+1]]))/fc.tab$total.ha[fc.tab$year==years[i]]), 3) fc.tab$netch.pc.yr[fc.tab$year==years[i]] &lt;- round(100 * 1/(years[i+1]-years[i]) * log(fc.tab$total.ha[fc.tab$year==years[i+1]]/fc.tab$total.ha[fc.tab$year==years[i]]), 3) } return(fc.tab) } # Function for plotting evolution of forest cover ----------------------------------------- plot.fc &lt;- function(fc, zone, filename=NULL) { if(zone==&quot;TGO&quot;) { title &lt;- &quot;Togo: Évolution de la couverure forestière 2003 - 2018&quot; ylim &lt;- c(0, 1400000) ybreaks &lt;- seq(0, 1400000, 200000) ymbreaks &lt;- seq(0, 1400000, 100000) } else { title &lt;- paste0(zone, &quot;: Évolution de la couverure forestière 2003 - 2018&quot;) ylim &lt;- c(0, 500000) ybreaks &lt;- seq(0, 500000, 100000) ymbreaks &lt;- seq(0, 500000, 50000) } if(!is.null(filename)) pdf(filename) print( fc %&gt;% gather(variable, value, secon.ha, initi.ha) %&gt;% ggplot(aes(x = year, y=value, fill=variable)) + geom_area(position=position_stack(reverse=TRUE)) + scale_fill_manual(name = NULL, breaks=c(&quot;secon.ha&quot;, &quot;initi.ha&quot;), values=c(alpha(&quot;#009E73&quot;, 0.8), alpha(&quot;#00BFC4&quot;, 0.5)), labels=c(&quot;Régéneration&quot;, &quot;Forêt depuis 1987&quot;)) + xlab(&quot;Année&quot;) + ylab(&quot;Hectares&quot;) + ggtitle(title) + scale_x_continuous(minor_breaks = NULL, breaks=c(1991, 2000, 2005, 2010, 2015, 2018)) + scale_y_continuous(breaks=ybreaks, minor_breaks = ymbreaks) + coord_cartesian(ylim=ylim) + theme_light() + theme(legend.position=c(0.6,0.2), legend.box = &quot;horizontal&quot;, legend.justification=c(-0.2,1.2)) ) if(!is.null(filename)) dev.off() } # Function for plotting forest cover change ----------------------------------- plot.fcc &lt;- function(fc, zone=NULL, breaks=NULL, filename=NULL) { my.fcc &lt;- fcc(fc) fcc.breaks &lt;- my.fcc if(!is.null(breaks)) fcc.breaks &lt;- fcc(fc, breaks)[fc$year %in% breaks,] my.fcc$period &lt;- c(my.fcc$year[2:nrow(my.fcc)] - my.fcc$year[1:(nrow(my.fcc)-1)], NA) my.fcc$center &lt;- my.fcc$year + my.fcc$period/2 fcc.breaks$period &lt;- c(fcc.breaks$year[2:nrow(fcc.breaks)] - fcc.breaks$year[1:(nrow(fcc.breaks)-1)], NA) fcc.breaks$center &lt;- fcc.breaks$year + fcc.breaks$period/2 if(zone==&quot;TGO&quot;) { title &lt;- &quot;Togo: Changements bruts et nets des surfaces forestières 2003 - 2018&quot; ylim &lt;- c(-35000, 20000) ybreaks &lt;- seq(-35000, 20000, 5000) ymbreaks &lt;- seq(-35000, 20000, 2500) } else { title &lt;- paste0(zone, &quot;: Changements bruts et nets des surfaces forestières 2003 - 2018&quot;) ylim &lt;- c(-15000, 10000) ybreaks &lt;- seq(-15000, 10000, 5000) ymbreaks &lt;- seq(-15000, 10000, 2500) } if(!is.null(filename)) pdf(filename) fcc.breaks$netdefor.ha.yr &lt;- fcc.breaks$netch.ha.yr fcc.breaks$netdefor.ha.yr[fcc.breaks$netdefor.ha.yr &gt; 0] &lt;- 0 fcc.breaks$netregen.ha.yr &lt;- fcc.breaks$netch.ha.yr fcc.breaks$netregen.ha.yr[fcc.breaks$netregen.ha.yr &lt; 0] &lt;- 0 print( fcc.breaks[-nrow(fcc.breaks),] %&gt;% gather(variable, value, defor.ha.yr, regen.ha.yr, netdefor.ha.yr, netregen.ha.yr) %&gt;% ggplot(aes(y=value, x = center, width=period-0.7)) + geom_bar(data=. %&gt;% filter(variable %in% c(&quot;defor.ha.yr&quot;, &quot;regen.ha.yr&quot;)), aes(fill=variable), stat = &quot;identity&quot;) + geom_bar(data=. %&gt;% filter(variable %in% c(&quot;netdefor.ha.yr&quot;, &quot;netregen.ha.yr&quot;)), aes(fill=variable), stat = &quot;identity&quot;) + geom_errorbar(data=my.fcc[1:5,], aes(x=center, y=NULL, ymax=defor.ha.yr, ymin=defor.ha.yr, width=period-0.7), colour=alpha(&quot;#F8766D&quot;, 1)) + geom_hline(aes(yintercept=0)) + scale_fill_manual(name = NULL, breaks = c(&quot;defor.ha.yr&quot;, &quot;netdefor.ha.yr&quot;, &quot;netregen.ha.yr&quot;, &quot;regen.ha.yr&quot;), values=c(&quot;regen.ha.yr&quot; = alpha(&quot;#00BFC4&quot;, 0.4), &quot;netregen.ha.yr&quot; = &quot;#00BFC4&quot;, &quot;netdefor.ha.yr&quot; = &quot;#F8766D&quot;, &quot;defor.ha.yr&quot; = alpha(&quot;#F8766D&quot;, 0.4)), labels=c(&quot;Perte brutte&quot;, &quot;Perte nette&quot;, &quot;Gain net&quot;, &quot;Gain brut&quot;)) + xlab(&quot;Année&quot;) + ylab(&quot;Hectares par année&quot;) + ggtitle(title) + scale_x_continuous(minor_breaks = NULL, breaks=c(1991, 2000, 2005, 2010, 2015, 2018)) + scale_y_reverse(breaks=ybreaks, minor_breaks = ymbreaks) + coord_cartesian(ylim=ylim) + theme_light() + theme(legend.position=c(0,1), legend.box = &quot;horizontal&quot;, legend.justification=c(-0.2,1.2)) ) if(!is.null(filename)) dev.off() } # DO THE WORK ############################## # Load and rename forest-cover maps ---------------------------- maps &lt;- stack(dir(paste0(FCC.CLN.DIR, &quot;/FC&quot;, COV.FC, &quot;/TGO&quot;), pattern = &quot;cf.tif&quot;, full.names = TRUE)) names(maps) &lt;- paste0(&quot;X&quot;, substr(names(maps), 5, 8)) # Create forest cover tables for different periods ------------------------------------ fc.all &lt;- fc(maps) write.csv(fc.all, paste0(FCC.RES.DIR, &quot;/TGO/TGO_fc.csv&quot;), row.names=FALSE) fc.all &lt;- read.csv(paste0(FCC.RES.DIR, &quot;/TGO/TGO_fc.csv&quot;)) fc.cln &lt;- fc.all fc.cln &lt;- fc.cln[!fc.cln$year %in% c(1987, 1991, 2000), ] fcc(fc.cln) fcc(fc.cln, c(2003, 2018)) fcc(fc.cln, c(2003, 2015, 2017, 2018)) write.xlsx(list(&quot;Total&quot; = fcc(fc.cln)), file = paste0(FCC.RES.DIR, &quot;/TGO/TGO_fcc-all-dates.xlsx&quot;)) write.xlsx(list(&quot;Total&quot; = fcc(fc.cln, c(2003, 2018))), file = paste0(FCC.RES.DIR, &quot;/TGO/TGO_fcc-03-18.xlsx&quot;)) write.xlsx(list(&quot;Total&quot; = fcc(fc.cln, c(2003, 2015, 2018))), file = paste0(FCC.RES.DIR, &quot;/TGO/TGO_fcc-03-15-18.xlsx&quot;)) plot.fc(fc.cln, &quot;TGO&quot;, paste0(FCC.RES.DIR, &quot;/TGO/TGO_fc.pdf&quot;)) plot.fcc(fc = fc.cln, zone = &quot;TGO&quot;, breaks=c(2003, 2015, 2018), filename=paste0(FCC.RES.DIR, &quot;/TGO/TGO_fcc.pdf&quot;)) registerDoParallel(.env$numCores-1) foreach(i=1:length(TGO.reg)) %do% { region &lt;- TGO.reg[i,] # fc.all &lt;- fc(maps, aoi=region) # write.csv(fc.all, paste0(RESULTS.DIR, &quot;/tables/&quot;, region$NAME_1, &quot;_fc.csv&quot;), row.names=FALSE) fc.all &lt;- read.csv(paste0(RESULTS.DIR, &quot;/tables/&quot;, region$NAME_1, &quot;_fc.csv&quot;)) fc.cln &lt;- fc.all[!fc.all$year %in% c(1987, 1991, 2000), ] plot.fc(fc.cln, region$NAME_1, paste0(RESULTS.DIR, &quot;/figures/&quot;, region$NAME_1, &quot;_fc.pdf&quot;)) plot.fcc(fc.cln, region$NAME_1, breaks=c(2003, 2015, 2018), paste0(RESULTS.DIR, &quot;/figures/&quot;, region$NAME_1, &quot;_fcc.pdf&quot;)) write.xlsx(list(&quot;Total&quot; = fcc(fc.cln)), file = paste0(RESULTS.DIR, &quot;/tables/&quot;, region$NAME_1, &quot;_fcc-all-dates.xlsx&quot;)) write.xlsx(list(&quot;Total&quot; = fcc(fc.cln, c(2003, 2018))), file = paste0(RESULTS.DIR, &quot;/tables/&quot;, region$NAME_1, &quot;_fcc-03-18.xlsx&quot;)) write.xlsx(list(&quot;Total&quot; = fcc(fc.cln, c(2003, 2015, 2018))), file = paste0(RESULTS.DIR, &quot;/tables/&quot;, region$NAME_1, &quot;_fcc-03-15-18.xlsx&quot;)) } # creating GIS layers for forest loss and forest gain per date --------------------------------------------- forest.loss &lt;- raster(maps) forest.gain &lt;- raster(maps) # create empty rasters from stack forest.pote &lt;- raster(maps) for(i in 1:(nlayers(maps)-1)) { print(paste0(&quot;Checking deforestation / reforestation in year &quot;, maps.dates[i])) forest.loss[is.na(forest.loss) &amp; is.na(forest.gain) &amp; maps[[i]] == 1 &amp; maps[[i+1]] == 3] &lt;- maps.dates[i] # take the first date of forest loss observed and only if no regeneration before that forest.gain[maps[[i]] %in% c(2,3) &amp; maps[[i+1]] == 1] &lt;- maps.dates[i] forest.pote[maps[[i]] ==3 &amp; maps[[i+1]] == 2] &lt;- maps.dates[i] } # special layer for forest gain in areas where loss has been observed before loss.gain &lt;- raster(maps) loss.gain[!is.na(forest.loss) &amp; !is.na(forest.gain) &amp; forest.gain &gt; forest.loss] &lt;- forest.gain[!is.na(forest.loss) &amp; !is.na(forest.gain) &amp; forest.gain &gt; forest.loss] forest.2018 &lt;- maps$X2018 forest.2018[forest.2018==2] &lt;- 3 writeRaster(forest.2018, paste0(RESULTS.DIR, &quot;/maps/forest-2018.tif&quot;), datatype=&quot;INT2U&quot;, overwrite=TRUE) writeRaster(forest.loss, paste0(RESULTS.DIR, &quot;/maps/forest-loss.tif&quot;), datatype=&quot;INT2U&quot;, overwrite=TRUE) writeRaster(forest.gain, paste0(RESULTS.DIR, &quot;/maps/forest-gain.tif&quot;), datatype=&quot;INT2U&quot;, overwrite=TRUE) writeRaster(forest.pote, paste0(RESULTS.DIR, &quot;/maps/forest-potential.tif&quot;), datatype=&quot;INT2U&quot;, overwrite=TRUE) writeRaster(loss.gain, paste0(RESULTS.DIR, &quot;/maps/loss-gain.tif&quot;), datatype=&quot;INT2U&quot;, overwrite=TRUE) "],
["00_analyse-AGB.html", "4.2 Analyse biomasse aérienne", " 4.2 Analyse biomasse aérienne Comme pour le couvert forestier, la biomasse aérienne est également cartographiée à la résolution des images Landsat (30 x 30 mètres). Tout d’abord, les données d’inventaire IFN-1 des années 2015/16 sont évalués pour leur biomasse aérienne. La biomasse aérienne trouvé sur les parcelles d’échantillon IFN-1 sont ensuite utilisées pour calibrer un modèle de régression RandomForest sur base des images satellitaires Landsat de l’année 2015 et les données climatiques WorldClim. Ce modèle de biomasse est ensuite appliqué à l’ensemble de l’image Landsat et les données WorldClim pour obtenir une carte complète de la biomasse aérienne pour l’année de référence 2015. Le schéma ci-dessous montre le processus d’analyse. La carte de référence de la biomasse aérienne 2015 sert comme base pour déterminer des cartes correspondantes pour les années 2003 (début de la période de référence) et 2018 (fin de la période de référence). Finalement, les différences entre les cartes de la biomasse 2003 et 2018 servent comme facteurs d’émission pour les pixels déterminés comme déforestation ou reboisement par l’analyse des surfaces forestiers au cours de la même période (voir section 4.1). "],
["01_compile-IFN.html", "", " 4.2.1 Analyse des données IFN Description La détermination de la biomasse arborée est basée sur les données de l’inventaire forestier IFN-1 réalisé en 2015/16, en utilisant les données des parcelles (coordonnées du centre de la parcelle et strate de couverture du sol IFN), ainsi que les espèces d’arbres, le diamètre à hauteur poitrine (D à 1.3m) et la hauteur de tous les arbres avec DHP ≥ 10 cm enregistrés dans un rayon de 20 mètres autour du centre de la parcelle. Toutes les espèces d’arbres sont attribuées avec leur densité de bois basale ou infradensité, compilée par l’étude de la biomasse de Fonton et al (2018) en utilisant les bases de données GlobalWoodDensityDatabase de Zanne et al. (2009) et ICRAF Wood Density. La fonction allométrique de Chave et al. (2014) est utilisé pour estimer la biomasse aérienne des arbres sur base de la densité du bois \\(\\rho\\), du diamètre à hauteur poitrine \\(D\\) et de la hauteur totale \\(H\\) des arbres: \\(B_{aérienne} = 0.0673 (\\rho D^2 H)^{0.976}\\). Selon l’étude de Fonton et al. 2018 la fonction allométrique de Chave et al. (2014) est la fonction la plus approprié pour l’estimation de la biomasse aérienne des arbres au Togo. Par la suite, les biomasses des arbres en surface sont additionnées pour toutes les parcelles et converties en tonnes de matière sèche par hectare. Cela est fait séparément pour les arbres vivants et les arbres morts. Sur la base de la biomasse aérienne par hectare, la biomasse racinaire est estimée à l’aide du rapport racine-tige de Mokany et al (2006) pour les forêts sèches tropicales (0,563 ± 0,086 pour les forêts avec \\(B_{aérienne} ≤ 20\\) t/ha et 0,275 ± 0,003 pour les forêts avec \\(B_{aérienne} &gt; 20\\) t/ha). 4.2.1.1 Example Répartition spatiale des 945 placettes d’inventaire de l’IFN-1 (la taille du cercle correspond à la biomasse trouvée sur les placettes) et répartition de la biomasse aérienne dans les différentes strates de l’IFN (données en tonnes de matière sèche par hectare). Biomasse déterminée à partir des données IFN-1 (en tonnes de matière sèche par hectare) par strate IFN et par compartiment : biomasse aérienne des arbres, bois mort, biomasse racinaire des arbres, et biomasse totale des arbres. \\(B_{aérienne}\\) \\(B_{morte}\\) \\(B_{racinaire}\\) \\(B_{totale}\\) Strate IFN \\(n\\) \\(\\\\\\mu\\) \\(\\\\\\sigma\\) \\(\\\\\\mu\\) \\(\\\\\\sigma\\) \\(\\\\\\mu\\) \\(\\\\\\sigma\\) \\(\\\\\\mu\\) \\(\\\\\\sigma\\) Cultures_Jachères/Fourrées 108 20.0 31.7 2.3 8.6 6.6 8.4 28.9 40.6 Forêts claires/savanes boisées 251 50.1 33.5 2.1 5.2 14.4 8.7 66.6 42.6 Forêts denses 138 96.8 69.7 3.8 7.5 26.8 19.0 127.3 89.3 Forêts riveraines/marécageuses 101 84.1 66.1 3.5 5.9 23.3 18.0 110.8 84.9 Plantations forestières 24 23.3 19.6 0.1 0.1 7.9 4.8 31.2 24.1 Plantations fruitières et de palmiers 19 18.9 21.0 0.4 1.4 6.0 5.4 25.3 27.1 Savane arborée/savane arbustive 277 16.4 15.4 0.7 1.6 6.3 4.1 23.3 19.7 NA 27 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 Script R: 03_NRF-MRV/02_AGB/_src/01_compile-AGB.R ############################################################################### # 01_compile-AGB.R: Évaluation de la biomasse des données de l&#39;IFN # ----------------------------------------------------------------------------- # Bern University of Applied Sciences # Oliver Gardi, &lt;oliver.gardi@bfh.ch&gt; # 13 Mai 2020 # Définitions des variables =================================================== PLOT.SIZE &lt;- 20^2*pi # Taille de la parcelle en mètres carrés # Rapports racines-tige forêts tropicales sèches selon Mokany et al. (2006) # https://onlinelibrary.wiley.com/doi/abs/10.1111/j.1365-2486.2005.001043.x RSR.Y &lt;- 0.563 # moyenne pour la jeune forêt RSR.Y.SE &lt;- 0.086 # écart type pour la jeune forêt RSR.O &lt;- 0.275 # moyenne pour les forêts matures RSR.O.SE &lt;- 0.003 # écart type pour les forêts matures RSR.AGB &lt;- 20 # Seuil biomasse aérienne jeunes forêts &lt;-&gt; forêts matures # Lire et préparer les données IFN-1 ========================================== plots &lt;- read.xlsx(paste0(DIR.RAW.DAT, &quot;/IFN/IFN-Togo-2015.xlsx&quot;), &quot;placettes&quot;)[,1:4] names(plots) &lt;- c(&quot;PlotID&quot;, &quot;X&quot;, &quot;Y&quot;, &quot;LULC&quot;) trees &lt;- read.xlsx(&quot;~/Downloads/IFN-Togo-2015.xlsx&quot;, &quot;arbres&quot;)[,c(1,4:6,8)] names(trees) &lt;- c(&quot;PlotID&quot;, &quot;Species&quot;, &quot;Status&quot;, &quot;DBH&quot;, &quot;H&quot;) # Tableau avec les espèces et le nombre d&#39;observations species &lt;- as.data.frame(table(trees$Species)) names(species) &lt;- c(&quot;Species&quot;, &quot;Count&quot;) # fusionner avec les densités de bois basales --------------------------------- # Densité basale: rapport entre mass anhydre et volume vert (à l&#39;état saturé) # Source des données: étude biomasse de Fonton et al. 2018 fonton &lt;- read.csv2(paste0(DIR.RAW.DAT, &quot;/IFN/donnees_Tg_Fonton.csv&quot;), encoding=&quot;latin1&quot;)[,c(4,7)] fonton &lt;- aggregate(list(D=fonton$wsg), by=list(Species=fonton$NOM), FUN=modal) species &lt;- merge(species[,c(&quot;Species&quot;, &quot;Count&quot;)], fonton[,c(&quot;Species&quot;, &quot;D&quot;)], by=&quot;Species&quot;) species$Source &lt;- &quot;Fonton&quot; # fusionner avec les arbres dans l&#39;IFN-1 trees &lt;- merge(trees, species, by=&quot;Species&quot;, all.x=TRUE) # estimer la biomasse aérienne des arbres ------------------------------------ # en utilisant la fonction allométrique de Chave et al. (2014) # https://onlinelibrary.wiley.com/doi/abs/10.1111/gcb.12629 trees$AGB &lt;- 0.0673 * (trees$D * trees$DBH^2 * trees$H)^0.976 # distinguer la biomasse des arbres vivants et celle des arbres morts trees$AGBm &lt;- trees$AGBv &lt;- trees$AGB # Dupliquer les valeurs de la biomasse ... trees$AGBv[trees$Status != &quot;V&quot;] &lt;- 0 # ... et mettre à 0 aux endroits appropriés trees$AGBm[trees$Status == &quot;V&quot;] &lt;- 0 # biomasse aérienne et bois mort par parcelle (somme des arbres) -------------- plots &lt;- merge(plots, aggregate(trees[,c(&quot;AGBv&quot;, &quot;AGBm&quot;)], by=list(PlotID=trees$PlotID), # somme dbiomasse es arbres -&gt; à l&#39;héctare -&gt; en tonne FUN=function(x) sum(x) * 10000 / PLOT.SIZE / 1000), by=&quot;PlotID&quot;, all.x=TRUE) # mettre à 0 la biomasse et le bois mort pour les parcelles sans valeurs (NA) plots$AGBv[is.na(plots$AGBv)] &lt;- 0 plots$AGBm[is.na(plots$AGBm)] &lt;- 0 plots$AGB &lt;- plots$AGBv + plots$AGBm # estimer la biomasse racinaire par parcelle ---------------------------------- # avec les facteurs root-shoot de Mokany et al. (2006) # jeunes forêts plots$BGB[plots$AGBv &lt;= RSR.AGB] &lt;- plots$AGBv[plots$AGBv &lt;= RSR.AGB] * RSR.Y # forêts matures plots$BGB[plots$AGBv &gt; RSR.AGB] &lt;- plots$AGBv[plots$AGBv &gt; RSR.AGB] * RSR.O # Biomasse totale = biomasse aérienne (vivant et mort) + biomasse racinaire plots$BM &lt;- plots$AGB + plots$BGB # Sauvegarder le résultat et production des figures et tableaux =============== # Note: sur Mac utiliser fileEncoding = &quot;macintosh&quot; write.csv(plots, paste0(DIR.MRV.AGB.REF, &quot;/IFN-plots.csv&quot;), row.names = FALSE) #, fileEncoding = &quot;macintosh&quot;) # distribution des biomasses par strate IFN ------------------------------------ # boxplot biomasse aérienne pdf(paste0(DIR.MRV.AGB.REF, &quot;/AGB-vs-LULC.pdf&quot;)) par(mar=c(11,5,1,1), cex.axis=0.7) boxplot(plots$AGBv~plots$LULC, las=2, ylab=&quot;AGB (t/ha)&quot;, xlab=NULL) dev.off() # boxplot bois mort pdf(paste0(AGB.REF.DIR, &quot;/AGBm-vs-LULC.pdf&quot;)) par(mar=c(11,5,1,1), cex.axis=0.7) boxplot(plots$AGBm~plots$LULC, las=2, ylab=&quot;Bmort (t/ha)&quot;, xlab=NULL) dev.off() # tableau biomass per LU/LC category (moyenne et écart type) bm.lulc.tab &lt;- plots %&gt;% group_by(LULC) %&gt;% # grouper par strate summarise(n=length(AGB), # definir colonnes et calcul des valeurs AGBv.mean=mean(AGBv), AGBv.sd=sd(AGBv), BGB.mean=mean(BGB), BGB.sd=sd(BGB), AGBm.mean=mean(AGBm), AGBm.sd=sd(AGBm), BM.mean =mean(BM), BM.sd =sd(BM)) write.csv(bm.lulc.tab, paste0(AGB.REF.DIR, &quot;/AGB_LULC.csv&quot;), row.names = FALSE) #, fileEncoding = &quot;macintosh&quot;) # différences biomasse par type de conversion LU/LC --------------------------- dbm.lulc.tab &lt;- bm.lulc.tab %&gt;% expand(FROM = nesting(LULC, AGB.mean, AGB.sd, BGB.mean, BGB.sd, BM.mean, BM.sd), TO = nesting(LULC, AGB.mean, AGB.sd, BGB.mean, BGB.sd, BM.mean, BM.sd)) %&gt;% mutate(from = FROM$LULC, to = TO$LULC, dAGB.mean = TO$AGB.mean - FROM$AGB.mean, dAGB.sd=sqrt(FROM$AGB.sd^2 + TO$AGB.sd^2), dBGB.mean = TO$BGB.mean - FROM$BGB.mean, dBGB.sd=sqrt(FROM$BGB.sd^2 + TO$BGB.sd^2), dBM.mean = TO$BM.mean - FROM$BM.mean, dBM.sd =sqrt(FROM$BM.sd^2 + TO$BM.sd^2)) %&gt;% select(c(from, to, dAGB.mean, dAGB.sd, dBGB.mean, dBGB.sd, dBM.mean, dBM.sd)) write.csv(dbm.lulc.tab, paste0(AGB.REF.DIR, &quot;/AGB_LULC-diff.csv&quot;), row.names = FALSE) #, fileEncoding = &quot;macintosh&quot;) "],
["02_create-AGB-maps.html", "", " 4.2.2 Calibration et prédiction Description A partir de la biomasse aérienne déterminée sur les parcelles IFN en 2015/16 (t/ha matière sèche), un modèle de biomasse est calibré sur bases de l’image satellitaire Landsat de 2015 (bandes et indices B, G, R, NIR, SWIR1, SWIR2, nbr, ndmi, ndvi, evi) et les données climatiques Worldclim (BIO1, BIO4, BIO12, BIO15). Ce sont les mêmes variables qui ont déjà été utilisées pour la classification forêt/non-forêt. Dans une première étape, les variables sont extraites, c’est-à-dire que les valeurs des pixels sous les rayons des parcelles sont lues à partir des images Landsat et Worldclim et moyennées en conséquence. Ensuite, les variable sont mis en correspandance avec les biomasses aériennes à travers un modèle de régression RandomForest. Ce modèle est utilisé pour créer une carte de la biomasse aérienne pour l’année 2015. En supposant que la biomasse sur la plupart des pixels reste constante, la carte de la biomasse aérienne 2015 est utilisée comme base pour calibrer des cartes pour les années 2003 et 2018 (début et fin de la période de référence) avec la même méthode: 0,1% des pixels de la carte de la biomasse 2015 (environ 100 000 pixels) sont utilisés comme pixels d’entraînement pour la calibration d’un modèle RandomForest qui est ensuite utilisé pour la production de la carte. 4.2.2.1 Example Répartition spatiale des 945 placettes d’inventaire de l’IFN-1 (la taille du cercle correspond à la biomasse trouvée sur les placettes) et carte de la biomasse aérienne résultant. Le diagramme de dispersion montre la correlation entre la biomasse sur la carte et celle trouvé sur les parcelles de l’IFN (\\(R^2 = 70.7%\\)). La carte sous-estime les fortes biomasses (effet de saturation). Utilisation de la carte de biomasse 2015 comme référence pour la calibration des cartes 2003 et 2018. Le détail montre une région au sud de Kpalimé. Script R: 03_NRF-MRV/02_AGB/_src/02_create-AGB-maps.R ############################################################################### # 02_create-AGB-maps.R: Modélisation et création des cartes de biomasse # ----------------------------------------------------------------------------- # Bern University of Applied Sciences # Oliver Gardi, &lt;oliver.gardi@bfh.ch&gt; # 13 Mai 2020 # Définitions des variables =================================================== AGB.STRATA &lt;- 10 # Nombre de strates de biomasse pour l&#39;échantillonnage N.PIXELS &lt;- NA # Nombre de pixels non-NA (sera déterminé plus tard) SAMPLE.RATIO &lt;- 0.001 # Rapport des pixels non-NA à échantilloner CAL.RATIO &lt;- 1 # Rapport des pixels à échantillonner pour la calibration # Bandes à utiliser pour la modélisation forêt vs. non-forêt PREDICTORS &lt;- c(&quot;B&quot;, &quot;G&quot;, &quot;R&quot;, &quot;NIR&quot;, &quot;SWIR1&quot;, &quot;SWIR2&quot;, &quot;nbr&quot;, &quot;ndmi&quot;, &quot;ndvi&quot;, &quot;evi&quot;, &quot;BIO1&quot;, &quot;BIO4&quot;, &quot;BIO12&quot;, &quot;BIO15&quot;) # Répertoires LANDSAT.DIR &lt;- DIR.SST.DAT.LST WORLDCLIM.DIR &lt;- DIR.SST.DAT.WC2 REF.DIR &lt;- DIR.MRV.AGB.REF # Définitions des fonctions =================================================== # Charger un image Landsat ---------------------------------------------------- # # @param filename Chemin du fichier landsat # # @return Image Landsat avec bandes nommées # load.image &lt;- function(filename) { image &lt;- brick(paste0(LANDSAT.DIR, filename)) names(image) &lt;- SST.LSBANDS return(image) } # Prédiction d&#39;une carte biomasse --------------------------------------------- # # @param image Image Landsat à classifier # @param filename Nom de fichier pour la sauvegarde de la carte # @param bioclim Raster des variables bioclimatiques à utiliser # @param train.dat Tableau des données d&#39;entraînement # @param ref.map Carte de référence # @param n.ref.map Nombre de points à échantilloner # @param cal.map Carte de calibration (autre chemin WRS) # @param n.cal.map Nombre de points à échantilloner # @param mask Masque à utiliser pour ref.map et cal.map # @param preds Variables à utiliser pour le modèle # @param type Modèle de classification ou de regression # @param crossval Faire validation croisée (3 * 10-fold) # @param bias.corr Faire une correction linéaire du bias # @param n.cores Nombre de processeurs à utiliser pour prédiction # # @return List avec les éléments # - model RandomForest, # - model linéaire du correction du bias # - carte biomasse # agb.map &lt;- function(image, filename, bioclim=NULL, train.dat=NULL, ref.map=NULL, n.ref.map=NULL, cal.map=NULL, n.cal.map=NULL, mask=NULL, preds=NULL, crossval=FALSE, bias.corr=TRUE, n.cores=8) { # Ouvrir le fichier journal name &lt;- sub(&quot;[.]tif$&quot;, &quot;&quot;, filename) txtfile &lt;- paste0(sub(&quot;[.]tif$&quot;, &quot;&quot;, filename), &quot;.txt&quot;) cat(&quot;-- Biomass map: &quot;, basename(filename), &quot;/&quot;, date(), &quot; --\\n&quot;, file=txtfile) # Tirer des données d&#39;entraînement d&#39;une carte de référence ------- if(!is.null(ref.map)) { cat(paste0(&quot; -Masking / buffering reference map ... \\n&quot;)) # ... couper/masquer avec l&#39;image ref.map &lt;- mask(crop(ref.map, image[[1]]), crop(image[[1]], ref.map)) # ... et masque additionelle (si disponible) if(!is.null(mask)) ref.map &lt;- mask(ref.map, mask) # Découper la carte de calibration (si disponible) if(!is.null(cal.map)) { tmp &lt;- extend(crop(cal.map, ref.map), ref.map) ref.map &lt;- mask(ref.map, tmp, inverse=TRUE) } cat(&quot; &quot;) # Tirer des points d&#39;échantillon (stratifié selon classe biomasse) ... cat(paste0(&quot; -Sampling map (n=&quot;, AGB.STRATA, &quot;*&quot;, round(n.ref.map/AGB.STRATA), &quot;) ... &quot;)) ref.pts &lt;- sampleStratified(cut(ref.map, AGB.STRATA), round(n.ref.map/AGB.STRATA), sp=TRUE)[,-1] names(ref.pts) &lt;- &quot;AGB&quot; # ... extraire les valeurs spectrales et bioclimatique correspondantes ... cat(&quot;extracting values ... \\n&quot;) ref.dat &lt;- cbind(AGB=raster::extract(ref.map, ref.pts, df=TRUE)[,-1], raster::extract(image, ref.pts, df=TRUE)[,-1], raster::extract(bioclim, ref.pts, df=TRUE)[,-1]) cat(&quot;Ref-map points: &quot;, nrow(ref.dat), &quot;/&quot;, ref.map@file@name, file=txtfile, append=TRUE) # ... et ajouter aux données d&#39;entraînement (si disponible) if(is.null(train.dat)) { train.dat &lt;- ref.dat } else { train.dat &lt;- rbind(train.dat, ref.dat) } } # Ajouter des points d&#39;une carte de calibration ------------------- if(!is.null(cal.map)) { cat(paste0(&quot; -Masking calibration map ... \\n&quot;)) # ... couper/masquer avec l&#39;image cal.map &lt;- mask(crop(cal.map, image[[1]]), crop(image[[1]], cal.map)) # ... et masque additionelle (si disponible) if(!is.null(mask)) cal.map &lt;- mask(cal.map, mask) # Tirer des points d&#39;échantillon (stratifié selon classe biomasse) ... cat(paste0(&quot; -Sampling map (n=&quot;, AGB.STRATA, &quot;*&quot;, round(n.cal.map/AGB.STRATA), &quot;) ... &quot;)) cal.pts &lt;- sampleStratified(cut(cal.map, AGB.STRATA), round(n.cal.map/AGB.STRATA), sp=TRUE)[,-1] names(cal.pts) &lt;- &quot;AGB&quot; # ... extraire les valeurs spectrales et bioclimatique correspondantes ... cat(&quot;extracting values ... \\n&quot;) cal.dat &lt;- cbind(AGB=raster::extract(cal.map, cal.pts, df=TRUE)[,-1], raster::extract(image, cal.pts, df=TRUE)[,-1], raster::extract(bioclim, cal.pts, df=TRUE)[,-1]) # ... et ajouter aux points d&#39;entraînement if(is.null(train.dat)) { train.dat &lt;- cal.dat } else { train.dat &lt;- rbind(train.dat, cal.dat) } cat(&quot;Cal-map points: &quot;, nrow(cal.dat), &quot;from&quot;, cal.map@file@name, file=txtfile, append=TRUE) } # Nombre total des points d&#39;entraînement cat(&quot;Total points: &quot;, nrow(train.dat), &quot;\\n&quot;, file=txtfile, append=TRUE) # Calibration du modèle Random Forest ----------------------------- # Utiliser toutes les variables si non-spécifiées dans les paramètres if(is.null(preds)) { preds &lt;- names(image) if(!is.null(bioclim)) preds &lt;- c(preds, names(bioclim)) } # calibrer RandomForest (mode de régression) cat(&quot; -Calibrating RandomForest ... &quot;) sink(txtfile, append=TRUE) # Utiliser caret::train pour validation croisée (a besoin de beaucoup de temps) if(crossval) { map.model.cv &lt;- train(y = train.dat[,1], x = train.dat[,preds], method = &quot;rf&quot;, # RandomForest importance = TRUE, trControl = trainControl( method = &quot;repeatedcv&quot;, number = 10, # k-fold repeats = 3)) # répétitions print(map.model.cv) map.model &lt;- map.model.cv$finalModel print(map.model) cat(&quot;\\n&quot;) print(varImp(map.model, scale=FALSE)) # autrement randomForest directement } else { map.model &lt;- randomForest(y=train.dat[,1], x=train.dat[,preds], importance=TRUE) # , do.trace=100) print(map.model) cat(&quot;\\n&quot;) print(varImp(map.model)) } sink() # Mesures des erreurs cat(&quot;R2:&quot;, round(map.model$rsq[500], 2), &quot;RMSE:&quot;, round(sqrt(map.model$mse[500]), 2), &quot;\\n&quot;) # Model linéaire de correction du bias if(bias.corr) { bc &lt;- lm(train.dat$AGB ~ map.model$predicted) sink(paste0(name, &quot;_bc.txt&quot;), split=TRUE) print(summary(bc)) sink() save(bc, file=paste0(name, &quot;_bc.RData&quot;)) } else { bc &lt;- NULL } # Diagramme de dispersion prédiction vs. observation pdf(paste0(name, &quot;.pdf&quot;)) plot(train.dat$AGB ~ map.model$predicted, ylab=&quot;AGB (tDM/ha)&quot;, xlab=&quot;Predicted AGB (tDM/ha)&quot;) abline(0,1, lty=2) if(bias.corr) { abline(bc, lty=3, col=&quot;red&quot;) } dev.off() # Classification de la carte biomasse ----------------------------- dir.create(dirname(filename), recursive=TRUE, showWarnings=FALSE) cat(&quot; -Creating map ... &quot;) # empiler les couches Landsat et bioclim if(!is.null(bioclim)) image &lt;- stack(image, crop(bioclim, image)) # classifier l&#39;image sur différents procésseurs en parallèle beginCluster(n=n.cores) map &lt;- clusterR(image, predict, args=list(model=map.model)) endCluster() # appliquer la correction du bias (si disponible) if(bias.corr) { cat(&quot;Applying linear bias correction ...&quot;) map &lt;- calc(map, fun=function(x){bc$coefficients[1] + bc$coefficients[2]*x}) } # sauvegarder la carte cat(&quot;writing map ... &quot;) map &lt;- writeRaster(map, filename=filename, format=&quot;GTiff&quot;, datatype=&quot;INT2U&quot;, overwrite=TRUE) cat(&quot;done\\n&quot;) cat(&quot;-- Done: &quot;, basename(filename), &quot;/&quot;, date(), &quot; --\\n&quot;, file=txtfile, append=TRUE) invisible(list( &quot;rf.model&quot; = map.model, &quot;bc.model&quot; = bc, &quot;map&quot; = map )) } # COMMENCER LE TRAITEMENT ##################################################### # Carte de référence 2015 ===================================================== # Charger images 2015 --------------------------------------------------------- ref.p192 &lt;- brick(paste0(IMAGES.DIR, &quot;/p192/p192_2015_m.tif&quot;)) ref.p193 &lt;- brick(paste0(IMAGES.DIR, &quot;/p193/p193_2015_m.tif&quot;)) ref.p194 &lt;- brick(paste0(IMAGES.DIR, &quot;/p194/p194_2015_m.tif&quot;)) names(ref.p192) &lt;- names(ref.p193) &lt;- names(ref.p194) &lt;- BANDS ref.images &lt;- list(p192=ref.p192, p193=ref.p193, p194=ref.p194) # Détérminer le nombre de pixels non-NA N.PIXELS &lt;- list(p192 = ncell(ref.p192[[&quot;B&quot;]]) - summary(ref.p192)[&quot;NA&#39;s&quot;,&quot;B&quot;], p193 = ncell(ref.p193[[&quot;B&quot;]]) - summary(ref.p193)[&quot;NA&#39;s&quot;,&quot;B&quot;], p194 = ncell(ref.p194[[&quot;B&quot;]]) - summary(ref.p194)[&quot;NA&#39;s&quot;,&quot;B&quot;]) # Charger variables bioclim bioclim.p192 &lt;- brick(paste0(IMAGES.DIR, &quot;/p192/p192_bioclim.tif&quot;)) bioclim.p193 &lt;- brick(paste0(IMAGES.DIR, &quot;/p193/p193_bioclim.tif&quot;)) bioclim.p194 &lt;- brick(paste0(IMAGES.DIR, &quot;/p194/p194_bioclim.tif&quot;)) names(bioclim.p192) &lt;- names(bioclim.p193) &lt;- names(bioclim.p194) &lt;- BIOCLIM bioclim &lt;- list(p192=bioclim.p192, p193=bioclim.p193, p194=bioclim.p194) # Charger les données d&#39;inventaire -------------------------------------------- plots &lt;- read.csv(paste0(REF.DIR, &quot;/IFN-plots.csv&quot;)) # , fileEncoding=&quot;macintosh&quot;) # Convertir en points spatiales en utilisant les coordonnées coordinates(plots) &lt;- ~X+Y proj4string(plots) &lt;- utm.31 # Figure de distribution spatiale des points pdf(paste0(REF.DIR, &quot;/IFN-plots_location.pdf&quot;), width=3.5, height=7) par(mar=c(1,1,1,1)) plot(spTransform(TGO, utm.31), col=&quot;lightyellow&quot;) plot(plots, add=TRUE, col=&quot;black&quot;, pch=16, cex=0.3) plot(plots, add=TRUE, col=&quot;darkgreen&quot;, pch=1, cex=plots$AGB/100) dev.off() # Convertir les polygones des parcelles en points spatiaux (centroïdes) plots.poly &lt;- SpatialPolygonsDataFrame(gBuffer(plots, byid=TRUE, width=20), plots@data) # Extraire les valeurs spéctrales pour les parcelles IFN ---------------------- registerDoParallel(CORES) # ... pour chaque image Landsat (chemin WRS) ... x &lt;- foreach(i=1:length(ref.images), .combine=cbind) %:% # ... et pour chaque couche spéctrale (bandes et indices) ... foreach(j=1:nlayers(ref.images[[i]]), .combine=cbind) %dopar% { # ... calculer la valeur moyenne pondérée au dessous de la parcelle IFN raster::extract(ref.images[[i]][[j]], plots.poly, weights=TRUE, fun=mean, df=TRUE)[,2] } # Extraire les valeurs bioclimatiques pour les parcelles IFN (en parallèle) x2 &lt;- foreach(i=1:length(bioclim), .combine=cbind) %:% foreach(j=1:nlayers(bioclim[[i]]), .combine=cbind) %dopar% { raster::extract(bioclim[[i]][[j]], plots, df=TRUE)[,2] } # Séparer les données d&#39;entraînement selon les différents chemins WRS # (13 variables spéctrales et 19 variables bioclim pour chaque chemin WRS) dat.p192 &lt;- na.omit(as.data.frame(cbind(plots$AGB, x[,1:13], x2[,1:19]))) dat.p193 &lt;- na.omit(as.data.frame(cbind(plots$AGB, x[,14:26], x2[,20:38]))) dat.p194 &lt;- na.omit(as.data.frame(cbind(plots$AGB, x[,27:39], x2[,39:57]))) names(dat.p192) &lt;- names(dat.p193) &lt;- names(dat.p194) &lt;- c(&quot;AGB&quot;, SSTS.LSBANDS, SSTS.BIOCLIM) train.data &lt;- list(p192=dat.p192, p193=dat.p193, p194=dat.p194) # Séléction des variables explicatives ---------------------------------------- # TODO: inclure la carte de régénération comme prédicteur ! agb.varsel &lt;- rfe(y = dat.p193[,1], # biomasse (1ère colonne) x = dat.p193[,-1], # prédicteurs (reste) sizes = c(4, 6, 8, 10), rfeControl = rfeControl( functions = rfFuncs, # utiliser RandomForest method = &quot;repeatedcv&quot;, # validation croisée number = 10, # 10-fold repeats = 3)) # 3 répétitions sink(paste0(REF.DIR, &quot;/p193_2015_AGB_varsel-fin.txt&quot;), split=TRUE) predictors(agb.varsel) print(agb.varsel) sink() pdf(paste0(REF.DIR, &quot;/p193_2015_AGB_varsel-fin.pdf&quot;)) plot(var.sel, type=c(&quot;g&quot;, &quot;o&quot;)) dev.off() # Cartes de référence biomasse aérienne 2015 ------------------------------- # Chemin WRS p193 set.RSEED(RSEED) p193.2015.agb &lt;- agb.map(image = load.image(&quot;/p193/p193_2015_m.tif&quot;), bioclim = bioclim[[&quot;p193&quot;]], filename = paste0(REF.DIR, &quot;/p193_2015_AGB_R.tif&quot;), train.dat = train.data[[&quot;p193&quot;]], preds = PREDICTORS, crossval = TRUE, bias.corr = FALSE, n.cores = 32) # Chemins WRS p192 and p194, utilisant p193 pour calibration set.RSEED(RSEED) p192.2015.agb &lt;- agb.map(image = load.image(&quot;/p192/p192_2015_m.tif&quot;), bioclim = bioclim[[&quot;p192&quot;]], filename = paste0(REF.DIR, &quot;/p192_2015_AGB_R.tif&quot;), train.dat = train.data[[&quot;p192&quot;]], # calibration avec carte p193 ... cal.map = raster(paste0(REF.DIR, &quot;/p193_2015_AGB_R.tif&quot;)), # ... avec au moin 200 points n.cal.map = max(200, nrow(train.data[[&quot;p192&quot;]])*CAL.RATIO), preds = PREDICTORS, crossval = FALSE, bias.corr = FALSE, n.cores = 32, mask = TGO) set.RSEED(RSEED) p194.2015.agb &lt;- agb.map(image = load.image(&quot;/p194/p194_2015_m.tif&quot;), bioclim = bioclim[[&quot;p194&quot;]], filename = paste0(REF.DIR, &quot;/p194_2015_AGB_R.tif&quot;), train.dat = train.data[[&quot;p194&quot;]], # calibration avec carte p193 ... cal.map = raster(paste0(REF.DIR, &quot;/p193_2015_AGB_R.tif&quot;)), # ... avec au moin 200 points n.cal.map = max(200, nrow(train.data[[&quot;p194&quot;]])*CAL.RATIO), preds = PREDICTORS, crossval = FALSE, bias.corr = FALSE, n.cores = 32, mask = TGO) # Fusionner les cartes des chemins p192, p193 et p194 agb.2015 &lt;- mask(crop(mosaic(raster(paste0(REF.DIR, &quot;/p192_2015_AGB_R.tif&quot;)), raster(paste0(REF.DIR, &quot;/p193_2015_AGB_R.tif&quot;)), raster(paste0(REF.DIR, &quot;/p194_2015_AGB_R.tif&quot;)), fun=mean), # valeur moyenne pour les superpositions TGO), TGO, filename=paste0(REF.DIR, &quot;/TGO_2015_AGB_R.tif&quot;), overwrite=TRUE) # Sauvgarder un image de la carte biomasse 2015 pdf(paste0(REF.DIR, &quot;/TGO_2015_AGB_R.pdf&quot;), width=3.5, height=7) par(mar=c(1,1,1,1)) plot(agb.2015, axes=FALSE, col=brewer.pal(9, &quot;YlGn&quot;), zlim=c(0,220)) plot(spTransform(TGO, utm.31), add=TRUE) dev.off() # Correction de la sous-estimation des valeurs biomasse élevés ---------------- # Diagramme de dispersion carte biomasse 2015 vs. observation parcelles IFN plots.poly$AGB.pred &lt;- raster::extract(agb.2015, plots.poly, weights=TRUE, fun=mean, df=TRUE)[,2] pdf(paste0(REF.DIR, &quot;/AGB-model_2015.pdf&quot;)) plot(plots.poly$AGB.pred ~ plots.poly$AGB, xlab=&quot;Biomasse aérienne IFN (t/ha)&quot;, ylab=&quot;Carte AGB 2015 (t/ha)&quot;, xlim=c(0,350), ylim=c(0,350)) abline(0, 1, lty=&quot;dashed&quot;) dev.off() # Correction pour réduire la sous-estimation des valeurs élevés cv &lt;- train(AGB ~ AGB.pred, data=plots.poly@data[!is.na(plots.poly$AGB.pred),], method = &quot;lm&quot;, # régression linéaire trControl = trainControl( method = &quot;cv&quot;, # validation croisée number = 10)) # 10-fold model &lt;- lm(AGB ~ AGB.pred, data=plots.poly@data[!is.na(plots.poly$AGB.pred),]) sink(paste0(REF.DIR, &quot;/TGO_2015_AGB_Rc.txt&quot;), split=TRUE) print(cv) summary(model) sink() agb.2015 &lt;- writeRaster(model$coefficients[&quot;(Intercept)&quot;] + model$coefficients[&quot;AGB.pred&quot;] * agb.2015, paste0(REF.DIR, &quot;/TGO_2015_AGB_R.tif&quot;), overwrite=TRUE) # Diagramme de dispersion carte biomasse 2015 corrigé vs. observation parcelles IFN agb.pred.c &lt;- raster::extract(agb.2015, plots.poly, weights=TRUE, fun=mean, df=TRUE)[,2] pdf(paste0(REF.DIR, &quot;/AGB-model_2015_c.pdf&quot;)) plot(agb.pred ~ plots.poly$AGB, xlab=&quot;Biomasse aérienne IFN (t/ha)&quot;, ylab=&quot;Carte AGB 2015 (t/ha)&quot;, xlim=c(0,350), ylim=c(0,350)) abline(0, 1, lty=&quot;dashed&quot;) dev.off() # Carte biomasse 2003 (sur base de la carte 2015) ============================= # Chemins WRS p193 set.RSEED(RSEED) p193.2003.agb &lt;- agb.map(image = load.image(&quot;/p193/p193_2003_m.tif&quot;), bioclim = bioclim[[&quot;p193&quot;]], filename = paste0(REF.DIR, &quot;/p193_2003_AGB_R.tif&quot;), train.dat = NULL, ref.map = raster(paste0(REF.DIR, &quot;/TGO_2015_AGB_R.tif&quot;)), n.ref.map = SAMPLE.RATIO * N.PIXELS[[&quot;p193&quot;]], preds = PREDICTORS, crossval = FALSE, bias.corr = TRUE, n.cores = 32) # Chemins WRS p192 and p194, utilisant p193 pour calibration set.RSEED(RSEED) p192.2003.agb &lt;- agb.map(image = load.image(&quot;/p192/p192_2003_m.tif&quot;), bioclim = bioclim[[&quot;p192&quot;]], filename = paste0(REF.DIR, &quot;/p192_2003_AGB_R.tif&quot;), train.dat = NULL, ref.map = raster(paste0(REF.DIR, &quot;/TGO_2015_AGB_R.tif&quot;)), n.ref.map = SAMPLE.RATIO * N.PIXELS[[&quot;p192&quot;]] / (1 + CAL.RATIO), cal.map = raster(paste0(REF.DIR, &quot;/p193_2003_AGB_R.tif&quot;)), n.cal.map = SAMPLE.RATIO * N.PIXELS[[&quot;p192&quot;]] / (1 + 1/CAL.RATIO), preds = PREDICTORS, crossval = FALSE, bias.corr = FALSE, n.cores = 32, mask = TGO) set.RSEED(RSEED) p194.2003.agb &lt;- agb.map(image = load.image(&quot;/p194/p194_2003_m.tif&quot;), bioclim = bioclim[[&quot;p194&quot;]], filename = paste0(REF.DIR, &quot;/p194_2003_AGB_R.tif&quot;), train.dat = NULL, ref.map = raster(paste0(REF.DIR, &quot;/TGO_2015_AGB_R.tif&quot;)), n.ref.map = SAMPLE.RATIO * N.PIXELS[[&quot;p194&quot;]] / (1 + CAL.RATIO), cal.map = raster(paste0(REF.DIR, &quot;/p193_2003_AGB_R.tif&quot;)), n.cal.map = SAMPLE.RATIO * N.PIXELS[[&quot;p194&quot;]] / (1 + 1/CAL.RATIO), preds = PREDICTORS, crossval = FALSE, bias.corr = FALSE, n.cores = 32, mask = TGO) # Fusionner les cartes des chemins p192, p193 et p194 agb.2003 &lt;- mask(crop(mosaic(raster(paste0(REF.DIR, &quot;/p192_2003_AGB_R.tif&quot;)), raster(paste0(REF.DIR, &quot;/p193_2003_AGB_R.tif&quot;)), raster(paste0(REF.DIR, &quot;/p194_2003_AGB_R.tif&quot;)), fun=mean), TGO), TGO, filename=paste0(REF.DIR, &quot;/TGO_2003_AGB_R.tif&quot;), overwrite=TRUE) # Carte biomasse 2003 (sur base de la carte 2015) ============================= # Chemins WRS p193 set.RSEED(RSEED) p193.2018.agb &lt;- agb.map(image = load.image(&quot;/p193/p193_2018_m.tif&quot;), bioclim = bioclim[[&quot;p193&quot;]], filename = paste0(REF.DIR, &quot;/p193_2018_AGB_R.tif&quot;), train.dat = NULL, ref.map = raster(paste0(REF.DIR, &quot;/TGO_2015_AGB_R.tif&quot;)), n.ref.map = SAMPLE.RATIO * N.PIXELS[[&quot;p193&quot;]], preds = PREDICTORS, crossval = FALSE, bias.corr = TRUE, n.cores = 32) # Chemins WRS p192 and p194, utilisant p193 pour calibration set.RSEED(RSEED) p192.2018.agb &lt;- agb.map(image = load.image(&quot;/p192/p192_2018_m.tif&quot;), bioclim = bioclim[[&quot;p192&quot;]], filename = paste0(REF.DIR, &quot;/p192_2018_AGB_R.tif&quot;), train.dat = NULL, ref.map = raster(paste0(REF.DIR, &quot;/TGO_2015_AGB_R.tif&quot;)), n.ref.map = SAMPLE.RATIO * N.PIXELS[[&quot;p192&quot;]] / (1 + CAL.RATIO), cal.map = raster(paste0(REF.DIR, &quot;/p193_2018_AGB_R.tif&quot;)), n.cal.map = SAMPLE.RATIO * N.PIXELS[[&quot;p192&quot;]] / (1 + 1/CAL.RATIO), preds = PREDICTORS, crossval = FALSE, bias.corr = FALSE, n.cores = 32, mask = TGO) set.RSEED(RSEED) p194.2018.agb &lt;- agb.map(image = load.image(&quot;/p194/p194_2018_m.tif&quot;), bioclim = bioclim[[&quot;p194&quot;]], filename = paste0(REF.DIR, &quot;/p194_2018_AGB_R.tif&quot;), train.dat = NULL, ref.map = raster(paste0(REF.DIR, &quot;/TGO_2015_AGB_R.tif&quot;)), n.ref.map = SAMPLE.RATIO * N.PIXELS[[&quot;p194&quot;]] / (1 + CAL.RATIO), cal.map = raster(paste0(REF.DIR, &quot;/p193_2018_AGB_R.tif&quot;)), n.cal.map = SAMPLE.RATIO * N.PIXELS[[&quot;p194&quot;]] / (1 + 1/CAL.RATIO), preds = PREDICTORS, crossval = FALSE, bias.corr = FALSE, n.cores = 32, mask = TGO) # Fusionner les cartes des chemins p192, p193 et p194 agb.2018 &lt;- mask(crop(mosaic(raster(paste0(REF.DIR, &quot;/p192_2018_AGB_R.tif&quot;)), raster(paste0(REF.DIR, &quot;/p193_2018_AGB_R.tif&quot;)), raster(paste0(REF.DIR, &quot;/p194_2018_AGB_R.tif&quot;)), fun=mean), TGO), TGO, filename=paste0(REF.DIR, &quot;/TGO_2018_AGB_R.tif&quot;), overwrite=TRUE) "],
["00_analyse-ER.html", "4.3 Analyse émissions/séquestrations", " 4.3 Analyse émissions/séquestrations Description Le calcul des émissions de CO2 dues à la déforestation et à la séquestration par le reboisement au cours de la période de référence 2003–2018 est basé sur les cartes forêt/non-forêt néttoyées de 2003 et 2018, ainsi que sur les cartes de la biomasse aérienne des années correspondantes. Dans une première étape, la biomasse racinaire est calculée sur la base des cartes de la biomasse aérienne, en utilisant les rapports racine-tige de Mokany et al (2006) pour les forêts sèches tropicales. Le calcul des émissions de CO2, et fait sur base des cartes de biomasse (aérienne et racinaire) de l’année 2003, en considerant que les pixels qui ont été classés comme forêt en 2003 et comme non-forêt en 2018. La biomasse restant après la déforestation est pris en compte par la soustraction de la biomasse moyenne des pixels non-forêt en 2018. Pour les pixels où une différence négative en résulte (augmentation de la biomasse malgré la déforestation), celle-ci est fixée à 0. La conversion de la biomasse en CO2 se fait au moyen d’une teneur en carbone de la biomasse de 0,47 (GIEC, 2006) et du rapport des poids moléculaires CO2/C de 44/12. La même procédure est utilisée pour calculer la séquestration due au reboisement entre 2003 et 2018. La biomasse des pixels reboisés de 2003 est soustraite de celle de 2018. Lorsque la différence est négative (diminution de la biomasse malgré le reboisement), elle est fixée à 0. 4.3.0.1 Example Les résultats sont évalués pour l’ensemble du territoire du Togo et les différents régions: La superficie annuelle déboisée (ha/a), la perte moyenne de biomasse et de CO2 due à la déforestation par hectare déboisé (tCO2/ha) et la perte annuelle totale sur la zone analysée (tCO2/a). L’évaluation de la croissance de la biomasse et de la séquestration du CO2 due au reboisement est effectuée de manière analogue. Déforestation Reboisement Total Région ha/a tCO2/ha tCO2/a ha/a tCO2/ha tCO2/a ha/a tCO2/a TGO -15’060 -62.7 -944’475 9’824 21.8 213’885 -5’236 -730’590 Centre -4’683 -95.2 -445’750 2’883 26.5 76’489 -1’799 -369’261 Kara -1’545 -56.7 -87’631 694 28.7 19’896 -851 -67’735 Maritime -3’273 -1.6 -5’306 2’467 8.8 21’784 -806 16’479 Plateaux -5’284 -65.8 -347’859 3’666 22.7 83’157 -1’618 -264’702 Savanes -275 -52.4 -14’399 114 32.9 3’745 -161 -10’654 Script R: 03_NRF-MRV/02_AGB/_src/04_analyze-ER.R ############################################################################### # 04_analyze-ER.R: Analyse des émissions et séquestrations CO2 # ----------------------------------------------------------------------------- # Bern University of Applied Sciences # Oliver Gardi, &lt;oliver.gardi@bfh.ch&gt; # 13 Mai 2020 # Définitions des variables =================================================== # Rapports racines-tige forêts tropicales sèches selon Mokany et al. (2006) # https://onlinelibrary.wiley.com/doi/abs/10.1111/j.1365-2486.2005.001043.x RSR.Y &lt;- 0.563 # moyenne pour la jeune forêt RSR.Y.SE &lt;- 0.086 # écart type pour la jeune forêt RSR.O &lt;- 0.275 # moyenne pour les forêts matures RSR.O.SE &lt;- 0.003 # écart type pour les forêts matures RSR.AGB &lt;- 20 # Seuil biomasse aérienne jeunes forêts &lt;-&gt; forêts matures # Teneur en carbone de la biomasse (valeur défault IPCC 2006) # https://www.ipcc-nggip.iges.or.jp/public/2006gl/pdf/4_Volume4/V4_04_Ch4_Forest_Land.pdf#page=48 C.RATIO &lt;- 0.47 # Répétoires des cartes biomasse et des résultats AGB.REF.DIR &lt;- DIR.MRV.AGB.REF AGB.RES.DIR &lt;- DIR.MRV.AGB.RES # Analyse des cartes de biomasse aérienne ===================================== # Biomasse aérienne par strate IFN (carte RapidEye) --------------------------- agb.2015 &lt;- raster(paste0(AGB.REF.DIR, &quot;/TGO_2015_AGB_R.tif&quot;)) strata &lt;- raster(paste0(DIR.RAW.DAT, &quot;/RapidEye/TGO_30m.tif&quot;)) zonal(agb.2015, strata, fun=&quot;mean&quot;) zonal(agb.2015, strata, fun=&quot;sd&quot;) # Distribution des biomasses aériennes (densité) ------------------------------ # Charger cartes biomasse aérienne 2003, 2015 et 2018 files &lt;- dir(AGB.REF.DIR, pattern=&quot;^TGO.*\\\\.tif$&quot;, full.names=TRUE) maps &lt;- stack(files) names(maps) &lt;- substr(names(maps), 5, 8) # Sélectionner les années 2003 et 2018 maps.dat &lt;- as.data.frame(na.omit(maps[[c(&quot;X2003&quot;, &quot;X2018&quot;)]][])) # production de la figure dens.plot &lt;- maps.dat %&gt;% gather(variable, value, X2003, X2018) %&gt;% ggplot(aes(x=value, color=variable, linetype=variable)) + geom_density() + theme_light() pdf(paste0(AGB.RES.DIR, &quot;/AGB-densities_03-18.pdf&quot;)) print(dens.plot) dev.off() # Analyze des émissions et séquestrations CO2 2003-2018 ======================= # charger cartes de biomasse 2003 et 2018 et calcule de la biomasse racinaire agb.2003 &lt;- raster(paste0(AGB.REF.DIR, &quot;/TGO_2003_AGB_R.tif&quot;)) bgb.2003 &lt;- agb.2003 * RSR.O bgb.2003[agb.2003 &lt;= RSR.AGB] &lt;- agb.2003[agb.2003 &lt;= RSR.AGB] * RSR.Y agb.2018 &lt;- raster(paste0(AGB.REF.DIR, &quot;/TGO_2018_AGB_R.tif&quot;)) bgb.2018 &lt;- agb.2018 * RSR.O bgb.2018[agb.2018 &lt;= RSR.AGB] &lt;- agb.2018[agb.2018 &lt;= RSR.AGB] * RSR.Y # charger cartes des surfaces forestiers 2003 et 2018 fc.2003 &lt;- raster(paste0(DIR.MRV.MCF.CLN, &quot;/FC30/TGO/TGO_2003_F30cf.tif&quot;)) fc.2018 &lt;- raster(paste0(DIR.MRV.MCF.CLN, &quot;/FC30/TGO/TGO_2018_F30cf.tif&quot;)) # Carte des pertes de biomasse ------------------------------------------------ # Masque forêt en 2003 et non-forêt en 2018 defor &lt;- fc.2003 == FOREST &amp; fc.2018 != FOREST # moyenne biomasse non-forêt (aérienne et racinaire) nf.agb &lt;- mean(agb.2018[fc.2018 == 3], na.rm=TRUE) nf.bgb &lt;- mean(bgb.2018[fc.2018 == 3], na.rm=TRUE) # Perte de biomasse due à la déforestation = biomasse 2003 - moyenne biomasse non-forêt defor.agb &lt;- mask(agb.2003, defor, maskvalue=0) - nf.agb defor.bgb &lt;- mask(bgb.2003, defor, maskvalue=0) - nf.bgb # Mettre à 0, là ou la perte est négative defor.agb[defor.agb &lt; 0] &lt;- 0 defor.bgb[defor.bgb &lt; 0] &lt;- 0 # Carte des gains de biomasse ------------------------------------------------- # Masque non-forêt en 2003 et forêt en 2018 regen &lt;- fc.2003 != FOREST &amp; fc.2018 == FOREST # Gain de biomasse due à la régénération = biomasse 2018 - biomasse 2003 regen.agb.2003 &lt;- mask(agb.2003, regen, maskvalue=0) regen.bgb.2003 &lt;- mask(bgb.2003, regen, maskvalue=0) regen.agb.2018 &lt;- mask(agb.2018, regen, maskvalue=0) regen.bgb.2018 &lt;- mask(bgb.2018, regen, maskvalue=0) regen.agb &lt;- regen.agb.2018 - regen.agb.2003 regen.bgb &lt;- regen.bgb.2018 - regen.bgb.2003 # Mettre à 0, là ou le gain est négative regen.agb[regen.agb &lt; 0] &lt;- 0 regen.bgb[regen.bgb &lt; 0] &lt;- 0 # Calculer émissions ets séquestrations pour une région ----------------------- # # @param defor Carte des pixels déforestés # @param defor.agb Carte des pertes de biomasse aérienne # @param defor.bgb Carte des pertes de biomasse racinaire # @param regen Carte des pixels régénérés # @param regen.agb Carte des gains de biomasse aérienne # @param regen.bgb Carte des gains de biomasse racinaire # @param aoi Zone d&#39;intérêt (région) # @param years Nombre d&#39;années entre les observations # # @return List avec les éléments # - defor.area Surface déforestée # - defor.area.a Surface déforestée par année # - defor.agb.ha Perte de biomasse aérienne par hectare déforestée # - defor.agb.a Perte de biomasse aérienne par année # - defor.bgb.ha ... même chose pour la biomasse racinaire ... # - defor.bgb.a # - defor.co2.ha ... et converti en CO2eq ... # - defor.co2.a # - regen.area Surface régénérée # - regen.area.a Surface régénérée par année # - regen.agb.ha Gain de biomasse aérienne par hectare régénérée # - regen.agb.a Gain de biomasse aérienne par année # - regen.bgb.ha ... même chose pour la biomasse racinaire ... # - regen.bgb.a # - regen.co2.ha ... et converti en CO2eq ... # - regen.co2.a # # emissions &lt;- function(defor, defor.agb, defor.bgb, regen, regen.agb, regen.bgb, aoi=NULL, years=15) { # couper la zone d&#39;intérêt if(!is.null(aoi)) { defor &lt;- mask(crop(defor, aoi), aoi) defor.agb &lt;- mask(crop(defor.agb, aoi), aoi) defor.bgb &lt;- mask(crop(defor.agb, aoi), aoi) regen &lt;- mask(crop(regen, aoi), aoi) regen.agb &lt;- mask(crop(regen.agb, aoi), aoi) regen.bgb &lt;- mask(crop(regen.bgb, aoi), aoi) } # convertir biomasse aérienne et racinaire en CO2eq defor.co2 &lt;- (defor.agb + defor.bgb) * C.RATIO * 44/12 # déterminer la surface (taille d&#39;un pixel Landsat = 30mx30m) defor.area &lt;- sum(defor[], na.rm=TRUE) * 30^2 / 10000 defor.area.a &lt;- defor.area / years # moyenne des pertes (par hectare) defor.agb.ha &lt;- mean(defor.agb[], na.rm=TRUE) defor.bgb.ha &lt;- mean(defor.bgb[], na.rm=TRUE) defor.co2.ha &lt;- mean(defor.co2[], na.rm=TRUE) # la même chose pour la régénération regen.co2 &lt;- (regen.agb + regen.bgb) * C.RATIO * 44/12 regen.area &lt;- sum(regen[], na.rm=TRUE) * 30^2 / 10000 regen.area.a &lt;- regen.area / years regen.agb.ha &lt;- mean(regen.agb[], na.rm=TRUE) regen.bgb.ha &lt;- mean(regen.bgb[], na.rm=TRUE) regen.co2.ha &lt;- mean(regen.co2[], na.rm=TRUE) return(list( defor.area = defor.area, defor.area.a = defor.area.a, defor.agb.ha = defor.agb.ha, defor.agb.a = defor.agb.ha * defor.area.a, defor.bgb.ha = defor.bgb.ha, defor.bgb.a = defor.bgb.ha * defor.area.a, defor.co2.ha = defor.co2.ha, defor.co2.a = defor.co2.ha * defor.area.a, regen.area = regen.area, regen.area.a = regen.area.a, regen.agb.ha = regen.agb.ha, regen.agb.a = regen.agb.ha * regen.area.a, regen.bgb.ha = regen.bgb.ha, regen.bgb.a = regen.bgb.ha * regen.area.a, regen.co2.ha = regen.co2.ha, regen.co2.a = regen.co2.ha * regen.area.a )) } # COMMENCER LE TRAITEMENT ##################################################### # Analyse des pertes de biomasse et CO2 dà cause de la déforestation # et les gains à cause de la régénération pour Togo et les régions ------------ # traitement en parallèle registerDoParallel(CORES) res &lt;- foreach(i=0:length(TGO.REG), .combine=rbind) %dopar% { if(i==0) { # Analyse pour l&#39;ensemble du Togo data.frame(reg = &quot;TGO&quot;, emissions(defor, defor.agb, defor.bgb, regen, regen.agb, regen.bgb)) } else { # Analyse pour une région region &lt;- TGO.reg[i,] data.frame(reg = region$NAME_1, emissions(defor, defor.agb, defor.bgb, regen, regen.agb, regen.bgb, aoi=region)) } } # sauvegarder le tableau des résultats write.csv(res, paste0(AGB.RES.DIR, &quot;/NERF-Results.csv&quot;), row.names=FALSE) "]
]
