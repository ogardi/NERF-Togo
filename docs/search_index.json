[
["index.html", "Manuel NERF/NRV Togo Préface", " Manuel NERF/NRV Togo Oliver Gardi, Fifonsi Dangbo, Sophie, Bakabima February 2020 Préface Blah, blah "],
["1-x_organisation.html", "1 Organisation du travail 1.1 Logiciel et serveur 1.2 Structure des fichiers", " 1 Organisation du travail 1.1 Logiciel et serveur La cartographie des surfaces forestier et de la biomasse aérienne des arbres se fait avec le logiciel R, dirigé à travers RStudio. Les scripts R dépendent des fois des outils GDAL disponible dans l’environnement, à installer pour les systèmes Linux avec apt-get install python-gdal. Le traitement et l’analyse des images satellitaires demande des ressources intensives en terme de stockage de données (plusieurs TB) et capacité de calcul (plusieurs processeurs en parallèle). Pour le moment l’infrastructure informatique centrale de la BFH-HAFL est utilisé. L’interface R-Studio est disponible à http://r.gro1.bfh.science. Pour le transfer des fichiers on peut s’également connecter avec SFTP, pour controler le serveur avec SSH. 1.2 Structure des fichiers Le repertoire de base NERF_Togo est structuré comme suivant: NERF_Togo ├── data # Données de base │ ├── GADM # frontières administratives │ ├── IFN # inventaire forestier national │ ├── Landsat # images satellitaires │ ├── SRTM # données topographiques │ └── Worldclim # données climatiques └── NERF_v1 # Répétoire projet ├── input # données spécifiques du projet │ ├── 1_images # images pre-traités │ ├── 2_train-plots # données d&#39;entraînement │ └── 3_val-plots # données de validation ├── R # scripts │ ├── 0_set-up.R # initialisation │ ├── 1_prepare-images.R # préparation des images │ ├── AGB # cartographie biomasse │ │ ├── 2_compile-IFN.R # évaluer inventaire │ │ ├── 3_create-AGB-maps.R # calibration des cartes │ │ ├── 4_clean-AGB-maps.R # nettoyage des cartes │ │ └── 5_analyze-AGB.R # analyse des cartes │ └── FCC # cartographie surfaces forestiers │ ├── 2_create-train-points.R # création des point d&#39;entraînement │ ├── 3_create-fc-maps.R # calibration des cartes │ ├── 4_clean-fc-maps.R # nettoyage des cartes │ ├── 5_create-val-points.R # création des points de validation │ ├── 6_validate-fc-maps.R # validation des cartes │ ├── 7_fc-maps-accuracy.R # analyse de la précision des cartes │ └── 8_analyse-fc-maps.R # analyse des cartes ├── output # résultats │ ├── 1_forest-cover # cartes surfaces forestiers │ │ ├── 1_ref-maps # cartes référence │ │ ├── 2_raw-maps # cartes brutes │ │ ├── 3_clean-maps # cartes nettoyées │ │ ├── 4_validation # résultats validation │ │ └── 5_results # résultats surfaces │ └── 2_biomass # cartes biomasse │ ├── 1_ref-maps # cartes de référence │ └── 5_results # résultats biomasse └── report # rapport / manuel "],
["2-x_nouveau-projet.html", "2 Pour commencer 2.1 Création d’un nouveau projet 2.2 Acquisition des images", " 2 Pour commencer 2.1 Création d’un nouveau projet Pour la création d’un nouveau projet, le plus simple est de copier un projet existant, normalement le projet sur lequel on aimeriait continuer le travail. On peut faire ça à travers RStudio, SFTP ou directement sur le server par exemple cp -r 2019_NERF_v1 2020_MNV_v0. Après on efface les résultat de l’ancien projet rm -r 2020_MNV_v0/output/*. Les rapports on peut laisser, mais il faut les rédiger à la fin. 2.2 Acquisition des images Ouvrir le site USGS Earthexplorer. Dans la fenêtre Search Criteria il faut selectionner la période pour laquelle on cherche des images (Nov - Jan). Dans la fenêtre Data Sets, il faut sélectionner les produits Landsat Level-2 (Surface Reflectance). Dans la fenêtre Additional Criteria il faut choisir les scènes (chemin 192: 054-056 / chemin 193: 052-055 / chemin 194: 052-053). On prend le date sur les images de tout le chemin sont de meilleure qualité. On copie les identifier des images à télécharger dans un fichier text. Ensuite on ouvre le site USGS ESPA pour commander les scènes. On charge le fichier text avec les scènes et on commande les bandes surface reflectances et les indices. Pour commander des images, il faut qu’on a un compte USGS. Une fois on est notifié par eMail que les images sont prêts, on les téléchargent manuellement ou tous ensemble avec le USGS bulkdownloader et la commande download_espa_order.py -u oliver.gardi@gmail.com -o ALL -d NERF_Togo/data/Landsat. On dézip les images et les rangent dans le répétoire data/Landsat sous la scène et l’année correspondante. Pour des images de l’hiver 2019/20, l’année correspondante est 2020. "],
["3-0_set-up.html", "3 Préparation 3.1 Définition des variables", " 3 Préparation 3.1 Définition des variables Description Le traitement des images ce fait par executer les scripts R, un après l’autre. Tout d’abord on ouvre et execute le script 0_set-up.R pour charger les librairies et définir des variables qu’on utilise également dans les autres scripts. 0_set-up.R ################################################################ # NERF_Togo/0_set-up.R: Loading libraries and defining functions # -------------------------------------------------------------- # Bern University of Applied Sciences # Oliver Gardi, &lt;oliver.gardi@bfh.ch&gt; # 13 November 2019 Sys.setlocale(&quot;LC_CTYPE&quot;, &quot;de_CH.UTF-8&quot;) # set working directory setwd(&quot;~/NERF_Togo/NERF_v1/R&quot;) # Hidden environment for variables and functions .env = new.env() # Libraries ----------------- library(&quot;sp&quot;) # Classes and methods for spatial data library(&quot;rgdal&quot;) # Bindings for the Geospatial Data Abstraction Library library(&quot;rgeos&quot;) # Interface to Geometry Engine - Open Source library(&quot;raster&quot;) # Geographic Data Analysis and Modeling library(&quot;randomForest&quot;) # Breiman and Cutler&#39;s random forests for classification and regression library(&quot;RStoolbox&quot;) # for terrain correction (topCor) library(&quot;caret&quot;) # for confusion matrix library(&quot;openxlsx&quot;) # pour directement lire des fichiers excel (xlsx) library(&quot;dplyr&quot;) # pour cr??er des tableaux crois??s library(&quot;tidyr&quot;) library(&quot;ggplot2&quot;) library(&quot;foreach&quot;) library(&quot;doParallel&quot;) library(&quot;gdalUtils&quot;) library(&quot;stringr&quot;) library(&quot;maptools&quot;) # kmlPolygon # Years ---------------------------------- # .env$YEARS &lt;- c(1987, 1991, 2000, 2003, 2005, 2007, 2010, 2013, 2015, 2017, 2018) .env$PERIOD &lt;- 1985:2019 .env$JNT.YEARS &lt;- c(1987, 2003, 2005, 2007, 2015, 2017, 2018) .env$VAL.YEARS &lt;- c(1987, 2003, 2015, 2018) .env$REF.YEARS &lt;- c( 2003, 2015, 2018) # Directories ---------------------------------- .env$DATA.DIR &lt;- &quot;../../data&quot; .env$INPUT.DIR &lt;- &quot;../input&quot; .env$IMAGES.DIR &lt;- paste0(.env$INPUT.DIR, &quot;/1_images&quot;) .env$TRNPTS.DIR &lt;- paste0(.env$INPUT.DIR, &quot;/2_train-plots&quot;) .env$VALPTS.DIR &lt;- paste0(.env$INPUT.DIR, &quot;/3_val-plots&quot;) .env$INVENT.DIR &lt;- paste0(.env$INPUT.DIR, &quot;/4_IFN&quot;) .env$OUTPUT.DIR &lt;- &quot;../output&quot; .env$FCC.DIR &lt;- paste0(.env$OUTPUT.DIR, &quot;/1_forest-cover&quot;) .env$FCC.REF.DIR &lt;- paste0(.env$FCC.DIR, &quot;/1_ref-maps&quot;) .env$FCC.RAW.DIR &lt;- paste0(.env$FCC.DIR, &quot;/2_raw-maps&quot;) .env$FCC.CLN.DIR &lt;- paste0(.env$FCC.DIR, &quot;/3_clean-maps&quot;) .env$FCC.VAL.DIR &lt;- paste0(.env$FCC.DIR, &quot;/4_validation&quot;) .env$FCC.RES.DIR &lt;- paste0(.env$FCC.DIR, &quot;/5_results&quot;) .env$AGB.DIR &lt;- paste0(.env$OUTPUT.DIR, &quot;/2_biomass&quot;) .env$AGB.REF.DIR &lt;- paste0(.env$AGB.DIR, &quot;/1_ref-maps&quot;) .env$AGB.RAW.DIR &lt;- paste0(.env$AGB.DIR, &quot;/2_raw-maps&quot;) .env$AGB.CLN.DIR &lt;- paste0(.env$AGB.DIR, &quot;/3_clean-maps&quot;) .env$AGB.VAL.DIR &lt;- paste0(.env$AGB.DIR, &quot;/4_validation&quot;) .env$AGB.RES.DIR &lt;- paste0(.env$AGB.DIR, &quot;/5_results&quot;) .env$BANDS &lt;- c(&quot;B&quot;, &quot;G&quot;, &quot;R&quot;, &quot;NIR&quot;, &quot;SWIR1&quot;, &quot;SWIR2&quot;, &quot;evi&quot;, &quot;msavi&quot;, &quot;nbr&quot;, &quot;nbr2&quot;, &quot;ndmi&quot;, &quot;ndvi&quot;, &quot;savi&quot;) .env$BIOCLIM &lt;- paste0(&quot;BIO&quot;, 1:19) # CRS, AOI, Extents ------------------- .env$utm.30 &lt;- crs(&quot;+proj=utm +zone=30 +datum=WGS84 +units=m +no_defs +ellps=WGS84 +towgs84=0,0,0&quot;) .env$utm.31 &lt;- crs(&quot;+proj=utm +zone=31 +datum=WGS84 +units=m +no_defs +ellps=WGS84 +towgs84=0,0,0&quot;) # Boundary of Togo .env$TGO &lt;- spTransform(readOGR(paste0(.env$DATA.DIR, &quot;/GADM/gadm36_TGO_0.shp&quot;)), .env$utm.31) .env$TGO.reg &lt;- spTransform(readOGR(paste0(.env$DATA.DIR, &quot;/GADM/gadm36_TGO_1.shp&quot;)), .env$utm.31) .env$TGO.ext &lt;- extent(151155, 373005, 670665, 1238175) # define extent by xmin, xmax, ymin and ymax # Cloud and water masks for Landsat 4-7 and Landsat 8 ------------------------ # see Landsat 4-7 and Landsat 8 Surface Reflectance Product Guides .env$qa.water &lt;- c( 68, 132, # Landsat 4-7 324, 388, 836, 900, 1348 # Landsat 8 ) .env$qa.shadow &lt;- c( 72, 136, 328, 392, 840, 904, 1350 ) .env$qa.ice &lt;- c( 80, 112, 144, 176, 336, 368, 400, 432, 848, 880, 912, 944, 1352 ) .env$qa.cloud &lt;- c( 96, 112, 160, 176, 224, 352, 368, 416, 432, 480, 864, 880, 928, 944, 992 ) # Misc ------------------- # prepare for parallel computing .env$numCores &lt;- detectCores() attach(.env) "],
["3-1_prepare-images.html", "3.2 Prétraitement des images", " 3.2 Prétraitement des images Description Le premier traitement est la préparation des nouveaux images. On ouvre le script 1_prepare-images.R et on ajoute les nouveaux images dans la liste des images: p192.2019 = list( paste0(DATA.DIR, &quot;/Landsat/192_054/2019/LC081920542018122301T1-SC20190405164258/&quot;), paste0(DATA.DIR, &quot;/Landsat/192_055/2019/LC081920552018122301T1-SC20190405163359/&quot;), paste0(DATA.DIR, &quot;/Landsat/192_056/2019/LC081920562018122301T1-SC20190405163342/&quot;)) ... p193.2019 = list( paste0(DATA.DIR, &quot;/Landsat/193_052/2019/LC081930522019021601T1-SC20190405183839/&quot;), paste0(DATA.DIR, &quot;/Landsat/193_053/2019/LC081930532019021601T1-SC20190405181518/&quot;), paste0(DATA.DIR, &quot;/Landsat/193_054/2019/LC081930542019021601T1-SC20190405183609/&quot;), paste0(DATA.DIR, &quot;/Landsat/193_055/2019/LC081930552019021601T1-SC20190405181507/&quot;)) ... p194.2019 = list( paste0(DATA.DIR, &quot;/Landsat/194_052/2019/LC081940522019012201T1-SC20190405172019/&quot;), paste0(DATA.DIR, &quot;/Landsat/194_053/2019/LC081940532019012201T1-SC20190405172055/&quot;)) Le script définit une fonction pour stacker, masquer et couper les images Landsat chemin par chemin. Par défaut, les images qui ont déjà été traité (filename existe déjà) ne sont plus traité (overwrite=FALSE). prepare.image(in.image.dirs, ext=NULL, filename=NULL, overwrite=FALSE, log=TRUE) À la fin du script 1_prepare-images.R, où c’est noté # DO THE WORK ---------, on commence avec le traitement des images. Les images prétraités sont sauveguarder dans le répétoire input/1_images du projet, ensemble avec des Thumbnails des chemins. Dans une prochaîne étape, les images sont néttoyées de l’eau, nuages et ombres en utilisant les bandes Landsat de qualité des pixels. Finalement également les données topographiques (SRTM) et climatiques (Worldclim v2) sont préparés. Example Chemin 193 de l’année 2019 (composé de 4 scènes Landsat du 16 Février 2019) 1_prepare-images.R #################################################################### # NERF_Togo/1_prepare-images.R: cleaning and stacking Landsat images # ------------------------------------------------------------------ # Bern University of Applied Sciences # Oliver Gardi, &lt;oliver.gardi@bfh.ch&gt; # 13 November 2019 ## @knitr load.images # load WRS scenes WRS &lt;- readOGR(paste0(DATA.DIR, &quot;/Landsat/WRS2/WRS2_descending.shp&quot;)) # Selection of images to prepare and to merge ------------------------------------------- in.image.list &lt;- list( # Path 192 p192.1986 = list(paste0(DATA.DIR, &quot;/Landsat/192_054/1986/LT051920541986011301T1-SC20190405164223/&quot;), paste0(DATA.DIR, &quot;/Landsat/192_055/1986/LT051920551986011301T1-SC20190405164227/&quot;), paste0(DATA.DIR, &quot;/Landsat/192_056/1986/LT051920561986011301T1-SC20190405164153/&quot;)), p192.1987 = list(paste0(DATA.DIR, &quot;/Landsat/192_054/1987/LT051920541986123101T1-SC20190405164150/&quot;), paste0(DATA.DIR, &quot;/Landsat/192_055/1987/LT051920551986123101T1-SC20190405163521/&quot;), paste0(DATA.DIR, &quot;/Landsat/192_056/1987/LT051920561986123101T1-SC20190405164444/&quot;)), p192.1991 = list(paste0(DATA.DIR, &quot;/Landsat/192_054/1991/LT041920541991010301T1-SC20190405164201/&quot;), paste0(DATA.DIR, &quot;/Landsat/192_055/1991/LT041920551991010301T1-SC20190405165911/&quot;), paste0(DATA.DIR, &quot;/Landsat/192_056/1991/LT041920561991010301T1-SC20190405163911/&quot;)), p192.2001 = list(paste0(DATA.DIR, &quot;/Landsat/192_054/2001/LE071920542000121301T1-SC20190405165521/&quot;), paste0(DATA.DIR, &quot;/Landsat/192_055/2001/LE071920552000121301T1-SC20190405165645/&quot;), paste0(DATA.DIR, &quot;/Landsat/192_056/2001/LE071920562000121301T1-SC20190405164029/&quot;)), p192.2003 = list(paste0(DATA.DIR, &quot;/Landsat/192_054/2003/LE071920542003010401T1-SC20190520111322/&quot;), paste0(DATA.DIR, &quot;/Landsat/192_055/2003/LE071920552003010401T1-SC20190520100402/&quot;), paste0(DATA.DIR, &quot;/Landsat/192_056/2003/LE071920562003010401T1-SC20190520100206/&quot;)), p192.2005 = list(paste0(DATA.DIR, &quot;/Landsat/192_054/2005/LE071920542004122401T1-SC20190405165520/&quot;), paste0(DATA.DIR, &quot;/Landsat/192_055/2005/LE071920552004122401T1-SC20190405164050/&quot;), paste0(DATA.DIR, &quot;/Landsat/192_056/2005/LE071920562004122401T1-SC20190405164030/&quot;)), p192.2007 = list(paste0(DATA.DIR, &quot;/Landsat/192_054/2007/LE071920542006123001T1-SC20190406034211/&quot;), paste0(DATA.DIR, &quot;/Landsat/192_055/2007/LE071920552006123001T1-SC20190406034231/&quot;), paste0(DATA.DIR, &quot;/Landsat/192_056/2007/LE071920562006123001T1-SC20190406034202/&quot;)), p192.2011 = list(paste0(DATA.DIR, &quot;/Landsat/192_054/2011/LE071920542011011001T1-SC20190406034214/&quot;), paste0(DATA.DIR, &quot;/Landsat/192_055/2011/LE071920552011011001T1-SC20190406034114/&quot;), paste0(DATA.DIR, &quot;/Landsat/192_056/2011/LE071920562011011001T1-SC20190406034155/&quot;)), p192.2013 = list(paste0(DATA.DIR, &quot;/Landsat/192_054/2013/LE071920542013013101T1-SC20190406034224/&quot;), paste0(DATA.DIR, &quot;/Landsat/192_055/2013/LE071920552013013101T1-SC20190406034046/&quot;), paste0(DATA.DIR, &quot;/Landsat/192_056/2013/LE071920562013013101T1-SC20190406034057/&quot;)), p192.2015 = list(paste0(DATA.DIR, &quot;/Landsat/192_054/2015/LC081920542015011301T1-SC20190405163446/&quot;), paste0(DATA.DIR, &quot;/Landsat/192_055/2015/LC081920552015011301T1-SC20190405163723/&quot;), paste0(DATA.DIR, &quot;/Landsat/192_056/2015/LC081920562015011301T1-SC20190405164231/&quot;)), p192.2017 = list(paste0(DATA.DIR, &quot;/Landsat/192_054/2017/LC081920542017021901T1-SC20190405163339/&quot;), paste0(DATA.DIR, &quot;/Landsat/192_055/2017/LC081920552017021901T1-SC20190405163342/&quot;), paste0(DATA.DIR, &quot;/Landsat/192_056/2017/LC081920562017021901T1-SC20190405163222/&quot;)), p192.2018 = list(paste0(DATA.DIR, &quot;/Landsat/192_054/2018/LC081920542018010501T1-SC20190405164304/&quot;), paste0(DATA.DIR, &quot;/Landsat/192_055/2018/LC081920552018010501T1-SC20190405163402/&quot;), paste0(DATA.DIR, &quot;/Landsat/192_056/2018/LC081920562018010501T1-SC20190405163250/&quot;)), p192.2019 = list(paste0(DATA.DIR, &quot;/Landsat/192_054/2019/LC081920542018122301T1-SC20190405164258/&quot;), paste0(DATA.DIR, &quot;/Landsat/192_055/2019/LC081920552018122301T1-SC20190405163359/&quot;), paste0(DATA.DIR, &quot;/Landsat/192_056/2019/LC081920562018122301T1-SC20190405163342/&quot;)), # # Path 193 p193.1985 = list(paste0(DATA.DIR, &quot;/Landsat/193_052/1985/LT051930521985030601T1-SC20190520100259/&quot;), paste0(DATA.DIR, &quot;/Landsat/193_053/1985/LT051930531985030601T1-SC20190520100324/&quot;), paste0(DATA.DIR, &quot;/Landsat/193_054/1985/LT051930541985030601T1-SC20190520100340/&quot;), paste0(DATA.DIR, &quot;/Landsat/193_055/1985/LT051930551985030601T1-SC20190520100140/&quot;)), p193.1987 = list(paste0(DATA.DIR, &quot;/Landsat/193_052/1987/LT051930521987012301T1-SC20190405182322/&quot;), paste0(DATA.DIR, &quot;/Landsat/193_053/1987/LT051930531987012301T1-SC20190405182335/&quot;), paste0(DATA.DIR, &quot;/Landsat/193_054/1987/LT051930541987012301T1-SC20190405182331/&quot;), paste0(DATA.DIR, &quot;/Landsat/193_055/1987/LT051930551987012301T1-SC20190405182328/&quot;)), # images of two different dates! p193.1990.1 = list(paste0(DATA.DIR, &quot;/Landsat/193_052/1990/LT051930521989112801T1-SC20190520100201/&quot;), paste0(DATA.DIR, &quot;/Landsat/193_053/1990/LT051930531989112801T1-SC20190520100233/&quot;)), p193.1990.2 = list(paste0(DATA.DIR, &quot;/Landsat/193_054/1991/LT041930541991011001T1-SC20190402043117/&quot;), paste0(DATA.DIR, &quot;/Landsat/193_055/1991/LT041930551991011001T1-SC20190402042453/&quot;)), p193.2000 = list(paste0(DATA.DIR, &quot;/Landsat/193_052/2000/LE071930522000020401T1-SC20190520100729/&quot;), paste0(DATA.DIR, &quot;/Landsat/193_053/2000/LE071930532000020401T1-SC20190520100345/&quot;), paste0(DATA.DIR, &quot;/Landsat/193_054/2000/LE071930542000020401T1-SC20190402045232/&quot;), paste0(DATA.DIR, &quot;/Landsat/193_055/2000/LE071930552000020401T1-SC20190402043121/&quot;)), p193.2003 = list(paste0(DATA.DIR, &quot;/Landsat/193_052/2003/LE071930522002122601T1-SC20190405182352/&quot;), paste0(DATA.DIR, &quot;/Landsat/193_053/2003/LE071930532002122601T1-SC20190405182309/&quot;), paste0(DATA.DIR, &quot;/Landsat/193_054/2003/LE071930542002122601T1-SC20190405182226/&quot;), paste0(DATA.DIR, &quot;/Landsat/193_055/2003/LE071930552002122601T1-SC20190405190255/&quot;)), p193.2005 = list(paste0(DATA.DIR, &quot;/Landsat/193_052/2005/LE071930522005021701T1-SC20190405190117/&quot;), paste0(DATA.DIR, &quot;/Landsat/193_053/2005/LE071930532005021701T1-SC20190405190003/&quot;), paste0(DATA.DIR, &quot;/Landsat/193_054/2005/LE071930542005021701T1-SC20190405182210/&quot;), paste0(DATA.DIR, &quot;/Landsat/193_055/2005/LE071930552005021701T1-SC20190405190021/&quot;)), p193.2007 = list(paste0(DATA.DIR, &quot;/Landsat/193_052/2007/LE071930522007012201T1-SC20190405182221/&quot;), paste0(DATA.DIR, &quot;/Landsat/193_053/2007/LE071930532007012201T1-SC20190405182607/&quot;), paste0(DATA.DIR, &quot;/Landsat/193_054/2007/LE071930542007012201T1-SC20190405182139/&quot;), paste0(DATA.DIR, &quot;/Landsat/193_055/2007/LE071930552007012201T1-SC20190405182418/&quot;)), p193.2009 = list(paste0(DATA.DIR, &quot;/Landsat/193_052/2009/LE071930522009012701T1-SC20190405182143/&quot;), paste0(DATA.DIR, &quot;/Landsat/193_053/2009/LE071930532009012701T1-SC20190405182301/&quot;), paste0(DATA.DIR, &quot;/Landsat/193_054/2009/LE071930542009012701T1-SC20190405182103/&quot;), paste0(DATA.DIR, &quot;/Landsat/193_055/2009/LE071930552009012701T1-SC20190405182754/&quot;)), p193.2013 = list(paste0(DATA.DIR, &quot;/Landsat/193_052/2013/LE071930522013022301T1-SC20190405182200/&quot;), paste0(DATA.DIR, &quot;/Landsat/193_053/2013/LE071930532013022301T1-SC20190405182213/&quot;), paste0(DATA.DIR, &quot;/Landsat/193_054/2013/LE071930542013022301T1-SC20190405182152/&quot;), paste0(DATA.DIR, &quot;/Landsat/193_055/2013/LE071930552013022301T1-SC20190405182331/&quot;)), p193.2015 = list(paste0(DATA.DIR, &quot;/Landsat/193_052/2015/LC081930522015010401T1-SC20190405181512/&quot;), paste0(DATA.DIR, &quot;/Landsat/193_053/2015/LC081930532015010401T1-SC20190405181751/&quot;), paste0(DATA.DIR, &quot;/Landsat/193_054/2015/LC081930542015010401T1-SC20190402042510/&quot;), paste0(DATA.DIR, &quot;/Landsat/193_055/2015/LC081930552015010401T1-SC20190402042446/&quot;)), p193.2017 = list(paste0(DATA.DIR, &quot;/Landsat/193_052/2017/LC081930522017012501T1-SC20190405181511/&quot;), paste0(DATA.DIR, &quot;/Landsat/193_053/2017/LC081930532017012501T1-SC20190405181440/&quot;), paste0(DATA.DIR, &quot;/Landsat/193_054/2017/LC081930542017012501T1-SC20190405181458/&quot;), paste0(DATA.DIR, &quot;/Landsat/193_055/2017/LC081930552017012501T1-SC20190405181444/&quot;)), p193.2018 = list(paste0(DATA.DIR, &quot;/Landsat/193_052/2018/LC081930522018011201T1-SC20190405181524/&quot;), paste0(DATA.DIR, &quot;/Landsat/193_053/2018/LC081930532018011201T1-SC20190405181459/&quot;), paste0(DATA.DIR, &quot;/Landsat/193_054/2018/LC081930542018011201T1-SC20190405181510/&quot;), paste0(DATA.DIR, &quot;/Landsat/193_055/2018/LC081930552018011201T1-SC20190405181442/&quot;)), p193.2019 = list(paste0(DATA.DIR, &quot;/Landsat/193_052/2019/LC081930522019021601T1-SC20190405183839/&quot;), paste0(DATA.DIR, &quot;/Landsat/193_053/2019/LC081930532019021601T1-SC20190405181518/&quot;), paste0(DATA.DIR, &quot;/Landsat/193_054/2019/LC081930542019021601T1-SC20190405183609/&quot;), paste0(DATA.DIR, &quot;/Landsat/193_055/2019/LC081930552019021601T1-SC20190405181507/&quot;)), # # Path 194 p194.1986 = list(paste0(DATA.DIR, &quot;/Landsat/194_052/1986/LT051940521986011101T1-SC20190405172804/&quot;), paste0(DATA.DIR, &quot;/Landsat/194_053/1986/LT051940531986011101T1-SC20190405172758/&quot;)), p194.1987 = list(paste0(DATA.DIR, &quot;/Landsat/194_052/1987/LT051940521986122901T1-SC20190405172903/&quot;), paste0(DATA.DIR, &quot;/Landsat/194_053/1987/LT051940531986122901T1-SC20190405174433/&quot;)), p194.1997 = list(paste0(DATA.DIR, &quot;/Landsat/194_052/1997/LT051940521997021001T1-SC20190405181746/&quot;), paste0(DATA.DIR, &quot;/Landsat/194_053/1997/LT051940531997021001T1-SC20190405173130/&quot;)), p194.2000 = list(paste0(DATA.DIR, &quot;/Landsat/194_052/2000/LE071940522000012601T1-SC20190405172721/&quot;), paste0(DATA.DIR, &quot;/Landsat/194_053/2000/LE071940532000012601T1-SC20190405172733/&quot;)), p194.2003 = list(paste0(DATA.DIR, &quot;/Landsat/194_052/2003/LE071940522002121701T1-SC20190405172823/&quot;), paste0(DATA.DIR, &quot;/Landsat/194_053/2003/LE071940532002121701T1-SC20190405172739/&quot;)), p194.2005 = list(paste0(DATA.DIR, &quot;/Landsat/194_052/2005/LE071940522004122201T1-SC20190405172700/&quot;), paste0(DATA.DIR, &quot;/Landsat/194_053/2005/LE071940532004122201T1-SC20190405172612/&quot;)), p194.2007 = list(paste0(DATA.DIR, &quot;/Landsat/194_052/2007/LT051940522007010501T1-SC20190405172919/&quot;), paste0(DATA.DIR, &quot;/Landsat/194_053/2007/LT051940532007010501T1-SC20190405172216/&quot;)), p194.2010 = list(paste0(DATA.DIR, &quot;/Landsat/194_052/2010/LE071940522010012101T1-SC20190405172745/&quot;), paste0(DATA.DIR, &quot;/Landsat/194_053/2010/LE071940532010012101T1-SC20190405173304/&quot;)), p194.2012 = list(paste0(DATA.DIR, &quot;/Landsat/194_052/2012/LE071940522012011101T1-SC20190405173146/&quot;), paste0(DATA.DIR, &quot;/Landsat/194_053/2012/LE071940532012011101T1-SC20190405172236/&quot;)), # Indices not available! # p194.2013 = list(paste0(DATA.DIR, &quot;/Landsat/194_052/2013/LE071940522012122801T1-SC20190405172704/&quot;), # paste0(DATA.DIR, &quot;/Landsat/194_053/2013/LE071940532012122801T1-SC20190405172717/&quot;)), p194.2015 = list(paste0(DATA.DIR, &quot;/Landsat/194_052/2015/LC081940522015012701T1-SC20190405172055/&quot;), paste0(DATA.DIR, &quot;/Landsat/194_053/2015/LC081940532015012701T1-SC20190405172042/&quot;)), p194.2017 = list(paste0(DATA.DIR, &quot;/Landsat/194_052/2017/LC081940522016123101T1-SC20190405172058/&quot;), paste0(DATA.DIR, &quot;/Landsat/194_053/2017/LC081940532016123101T1-SC20190405172040/&quot;)), p194.2018 = list(paste0(DATA.DIR, &quot;/Landsat/194_052/2018/LC081940522017121801T1-SC20190405172038/&quot;), paste0(DATA.DIR, &quot;/Landsat/194_053/2018/LC081940532017121801T1-SC20190405174114/&quot;)), p194.2019 = list(paste0(DATA.DIR, &quot;/Landsat/194_052/2019/LC081940522019012201T1-SC20190405172019/&quot;), paste0(DATA.DIR, &quot;/Landsat/194_053/2019/LC081940532019012201T1-SC20190405172055/&quot;)) ) # create scene list ---------- # sink(paste0(INPUT.DIR, &quot;/Landsat/scenes.txt&quot;) # for(image.dirs in in.image.list) { # # for(image.dir in image.dirs) { # # cat(sub(&quot;_ANG[.]txt&quot;, &quot;&quot;, dir(image.dir)[1]),&quot;\\n&quot;) # } # } # sink() # Function for processing and merging a set of Landsat images ----------------------------------------------------------- ## @knitr prepare.image prepare.image &lt;- function(in.image.dirs, ext=NULL, filename=NULL, overwrite=FALSE, log=TRUE) { # load file, if filename already exists and overwrite=FALSE if(!is.null(filename) &amp;&amp; file.exists(filename) &amp;&amp; overwrite==FALSE) { message(&quot;- loading from file &quot;, filename) out.image &lt;- stack(filename) } else { # open logfile if(!is.null(filename) &amp; log==TRUE) { dir.create(dirname(filename), recursive = TRUE, showWarnings = FALSE) logfile &lt;- file(sub(&quot;\\\\.[[:alnum:]]+$&quot;, &quot;.log&quot;, filename), open=&quot;wt&quot;) sink(logfile, type=&quot;output&quot;) sink(logfile, type=&quot;message&quot;) message(date()) } # list for the imported and cleaned images images &lt;- list() qas &lt;- list() # read and process the individual images for(image.dir in in.image.dirs) { image.sensor &lt;- substr(basename(image.dir), 0,4) if(image.sensor==&quot;LC08&quot;) { image &lt;- stack(grep(&quot;^.*_(pixel_qa|band2|band3|band4|band5|band6|band7|evi|msavi|nbr|nbr2|ndmi|ndvi|savi).tif$&quot;, dir(image.dir, full.names=TRUE), value=TRUE)) } else { image &lt;- stack(grep(&quot;^.*_(pixel_qa|band1|band2|band3|band4|band5|band7|evi|msavi|nbr|nbr2|ndmi|ndvi|savi).tif$&quot;, dir(image.dir, full.names=TRUE), value=TRUE)) } image.bands &lt;- sub(&quot;^.*_&quot;, &quot;&quot;, names(image)) # check the number and order of stack-layers if(nlayers(image)!=14 || (image.sensor==&quot;LC08&quot; &amp;&amp; image.bands!=c(&quot;qa&quot;, &quot;band2&quot;, &quot;band3&quot;, &quot;band4&quot;, &quot;band5&quot;, &quot;band6&quot;, &quot;band7&quot;, &quot;evi&quot;, &quot;msavi&quot;, &quot;nbr&quot;, &quot;nbr2&quot;, &quot;ndmi&quot;, &quot;ndvi&quot;, &quot;savi&quot;)) || (image.sensor!=&quot;LC08&quot; &amp;&amp; image.bands!=c(&quot;qa&quot;, &quot;band1&quot;, &quot;band2&quot;, &quot;band3&quot;, &quot;band4&quot;, &quot;band5&quot;, &quot;band7&quot;, &quot;evi&quot;, &quot;msavi&quot;, &quot;nbr&quot;, &quot;nbr2&quot;, &quot;ndmi&quot;, &quot;ndvi&quot;, &quot;savi&quot;))) stop(&quot;image does not have the correct number/order of bands (qa, B, G, R, NIR, SWIR1, SWIR2, evi, msavi, nbr, nbr2, ndmi, ndvi, savi)&quot;) image.name &lt;- substr(names(image)[1], 1, 40) image.scene &lt;- paste0(substr(image.name, 11, 13), &quot;_&quot;, substr(image.name, 14, 16)) image.date &lt;- substr(image.name, 18, 21) image.path &lt;- as.numeric(substr(image.scene, 1, 3)) image.row &lt;- as.numeric(substr(image.scene, 5, 7)) message(&quot;- &quot;, image.name, &quot;: &quot;, appendLF = FALSE) # image &lt;- image[[1]] # crop and mask the image with extent (if any) if(!is.null(ext)) { message(&quot;crop ext ... &quot;, appendLF = FALSE) image &lt;- crop(image, ext) } # crop/mask with WRS message(&quot;crop/mask WRS2 ... &quot;, appendLF = FALSE) wrs &lt;- spTransform(WRS[WRS$PATH==image.path &amp; WRS$ROW==image.row, ], CRS=crs(image)) image &lt;- mask(crop(image, wrs), wrs) # extract and mask with quality band qa &lt;- image[[1]] image &lt;- dropLayer(image, 1) # message(&quot;mask cloud/shadow ... &quot;, appendLF = FALSE) # if(image.sensor==&quot;LC08&quot;) { # # Landsat 8: values 392 - 480 / 840 - 880 / 904 - 992 are medium to high confidence cloud/ice/shadow # # https://prd-wret.s3-us-west-2.amazonaws.com/assets/palladium/production/s3fs-public/atoms/files/LSDS-1368_%20L8_Surface-Reflectance-Code-LASRC-Product-Guide.pdf (p. 22) # qa[qa %in% c(392, 400, 416, 432, 480, # 840, 848, 864, 880, # 904, 912, 928, 944, 992)] &lt;- NA # # } else { # # Landsat 4-7: values ≥ 136 are medium to high confidence cloud/ice/shadow # # https://prd-wret.s3-us-west-2.amazonaws.com/assets/palladium/production/s3fs-public/atoms/files/LSDS-1370_L4-7_Surface%20Reflectance-LEDAPS-Product-Guide.pdf # qa[qa &gt;= 136] &lt;- NA # } # image &lt;- mask(image, qa) # remove non-valid reflectance values message(&quot;clean sr ... &quot;, appendLF = FALSE) for(i in 1:6) { image[[i]] &lt;- reclassify(image[[i]], cbind(-Inf, 0, NA), right=FALSE) image[[i]] &lt;- reclassify(image[[i]], cbind(10000, Inf, NA), right=TRUE) } # remove non-valid index values for(i in 7:13) { image[[i]] &lt;- reclassify(image[[i]], cbind(-Inf, -10000, NA), right=FALSE) image[[i]] &lt;- reclassify(image[[i]], cbind(10000, Inf, NA), right=TRUE) } # set whole stack to NA where one single layer is NA m &lt;- sum(image) image &lt;- mask(image, m) names(image) &lt;- BANDS[-c(length(BANDS)-1:0)] # write out the individual images # message(&quot;writing scene ... &quot;, appendLF = FALSE) # filename.scene &lt;- sub(&quot;complete&quot;, paste0(&quot;r0&quot;, image.row), sub(paste0(&quot;p&quot;, image.path, &quot;_&quot;), paste0(&quot;p&quot;, image.path, &quot;_r0&quot;, image.row, &quot;_&quot;), filename)) # image &lt;- writeRaster(image, filename = filename.scene, overwrite = TRUE, datatype=&quot;INT2S&quot;, options=c(&quot;COMPRESS=NONE&quot;)) # qa &lt;- writeRaster(qa, filename = sub(&quot;[.]tif$&quot;, paste0(&quot;_qa&quot;, image.sensor, &quot;.tif&quot;), filename.scene), overwrite = TRUE, datatype=&quot;INT2S&quot;, options=c(&quot;COMPRESS=NONE&quot;)) # message(&quot;done&quot;) images[[length(images)+1]] &lt;- image qas[[length(qas)+1]] &lt;- qa } # merge the images in the list message(&quot;- merging scenes ... &quot;, appendLF = FALSE) out.image &lt;- do.call(merge, images) out.qa &lt;- do.call(merge, qas) # write it to a file if (!is.null(filename) &amp;&amp; (!file.exists(filename) || overwrite == TRUE)) { message(&quot;writing to file &quot;, filename, &quot; ... &quot;, appendLF = FALSE) out.image &lt;- writeRaster(out.image, filename = filename, overwrite = overwrite, datatype=&quot;INT2S&quot;, options=c(&quot;COMPRESS=NONE&quot;)) names(out.image) &lt;- BANDS[-c(length(BANDS)-1:0)] out.qa &lt;- writeRaster(out.qa, filename = sub(&quot;[.]tif$&quot;, paste0(&quot;_qa&quot;, image.sensor, &quot;.tif&quot;), filename), overwrite = overwrite, datatype=&quot;INT2S&quot;) } } message(&quot;done&quot;) print(out.image) # close the logfile if(!is.null(filename) &amp; log==TRUE) { sink(type=&quot;output&quot;) sink(type=&quot;message&quot;) } # return image invisibly invisible(out.image) } # DO THE WORK --------- ## @knitr execute # be careful, can easily fill the tmp directory! Maybe only for a part of the images and then restart R # extent of Togo + 5km buffer TGO.ext.30 &lt;- extent(spTransform(TGO, utm.30)) + 10000 # go through all scenes (path/row) ... registerDoParallel(.env$numCores-1) #foreach(i=1:length(in.image.list)) %dopar% { # 1) %do% { foreach(i=1:13) %dopar% { # 1) %do% { in.image.dirs &lt;- in.image.list[[i]] name &lt;- unlist(strsplit(names(in.image.list[i]), &quot;[.]&quot;)) path &lt;- name[1] year &lt;- name[2] tile &lt;- name[3] if(path == &quot;p194&quot;) tmp.ext &lt;- TGO.ext.30 else tmp.ext &lt;- TGO.ext out.image.dir &lt;- paste0(OUTPUT.DIR, &quot;/1_images/&quot;, path) if(!dir.exists(out.image.dir)) dir.create(out.image.dir) if(is.na(tile)) { filename &lt;- paste0(out.image.dir, &quot;/&quot;, path, &quot;_&quot;, year, &quot;.tif&quot;) } else { filename &lt;- paste0(out.image.dir, &quot;/&quot;, path, &quot;_&quot;, year, &quot;_&quot;, tile, &quot;.tif&quot;) } message(&quot;Processing &quot;, path, &quot;_&quot;, year) prepare.image(in.image.dirs, ext = tmp.ext, filename = filename, overwrite=FALSE, log=TRUE) } # remove temporary files tmp_dir &lt;- tempdir() files &lt;- list.files(tmp_dir, full.names = T, recursive=T) file.remove(files) # merge logfiles ------------------- for(dir in dir(paste0(OUTPUT.DIR, &quot;/1_images/&quot;), full.names=TRUE)) { path &lt;- basename(dir) system(paste0(&quot;tail -n +1 &quot;, dir, &quot;/*.log &gt; &quot;, dir, &quot;/&quot;, path, &quot;.tmp&quot;)) system(paste0(&quot;rm &quot;, dir, &quot;/*.log&quot;)) system(paste0(&quot;mv &quot;, dir, &quot;/&quot;, path, &quot;.tmp &quot;, dir, &quot;/&quot;, path, &quot;.log&quot;)) } # Reprojection of p194 images from UTM 30 to UTM 31 -------------------------- in.dir &lt;- &quot;../output/1_images/p194&quot; # warp p194 UTM 30 images to UTM 31 foreach(image=dir(in.dir, pattern=&quot;.*[.]tif$&quot;)) %dopar% { image &lt;- paste0(in.dir, &quot;/&quot;, image) image.utm30 &lt;- sub(&quot;[.]tif$&quot;, &quot;utm30.tif&quot;, image) file.rename(image, image.utm30) system(paste(&quot;gdalwarp&quot;, image.utm30, &quot;-t_srs &#39;+proj=utm +zone=31 +datum=WGS84&#39;&quot;, &quot;-tr 30 30&quot;, &quot;-te 147255 1017495 222165 1238265&quot;, image, &quot;-ot &#39;Int16&#39;&quot;, &quot;-overwrite&quot;)) file.remove(image.utm30) } # cloud/shadow mask the images ---------------------------- registerDoParallel(.env$numCores-1) foreach(file= c(dir(&quot;../output/1_images/p192&quot;, pattern=&quot;.*\\\\_[[:digit:]]+\\\\.tif$&quot;, full.names = TRUE), dir(&quot;../output/1_images/p193&quot;, pattern=&quot;.*\\\\_[[:digit:]]+\\\\.tif$&quot;, full.names = TRUE), dir(&quot;../output/1_images/p194&quot;, pattern=&quot;.*\\\\_[[:digit:]]+\\\\.tif$&quot;, full.names = TRUE))) %dopar% { qa &lt;- raster(dir(dirname(file), pattern=gsub(&quot;\\\\_&quot;, &quot;\\\\_&quot;, sub(&quot;\\\\.tif&quot;, &quot;_qa*&quot;, basename(file))), full.names=TRUE)) image &lt;- mask(brick(file), qa %in% c(qa.cloud, qa.shadow, qa.water, qa.ice), maskvalue=TRUE) writeRaster(image, sub(&quot;\\\\.tif&quot;, &quot;_m.tif&quot;, file), overwrite = TRUE, datatype=&quot;INT2S&quot;, options=c(&quot;COMPRESS=NONE&quot;)) } # Thumbnails of each scene ------------------------------- foreach(filename=dir(&quot;../output/1_images&quot;, pattern=&quot;p19.*[.]tif$&quot;, recursive=TRUE, full.names=TRUE)) %dopar% { image &lt;- brick(filename) jpeg(sub(&quot;[.]tif$&quot;, &quot;.jpeg&quot;, filename), width=1350, height=3000) par(plt=c(0,1,0,1)) plot(spTransform(TGO, utm.31)) plotRGB(image, r=6, g=5, b=3, stretch=&quot;lin&quot;, add=TRUE) plot(mask(image[[1]], spTransform(TGO, utm.31), inverse=TRUE), col=&quot;#FFFFFF66&quot;, legend=FALSE, add=TRUE) plot(spTransform(TGO, utm.31), add=TRUE, lwd=3) dev.off() } # Prepare SRTM DEM ---------------------------- # Load 90m SRTM DEM (source: CGIAR), dem.90 &lt;- do.call(merge, lapply(as.list(dir(paste0(INPUT.DIR, &quot;/SRTM/3arcsecond&quot;), pattern=&quot;.*[.]tif$&quot;, full.names=TRUE)), raster)) # Merge 30m SRTM DEM and fill voids with 90m SRTM (source: USGS) dem.30 &lt;- foreach(tile=dir(paste0(INPUT.DIR, &quot;/SRTM/1arcsecond&quot;), pattern=&quot;.*[.]tif$&quot;, full.names=TRUE), .combine=merge, .multicombine=TRUE) %dopar% { dem.30.t &lt;- raster(tile) dem.90.t &lt;- round(projectRaster(dem.90, dem.30.t)) merge(dem.30.t, dem.90.t) } # write it to the disk and reproject to reference Landsat image writeRaster(dem.30, paste0(OUTPUT.DIR, &quot;/1_images/SRTM/SRTM-1arcsec_raw.tif&quot;), datatype=&quot;INT2S&quot;, overwrite=TRUE) system(paste(&quot;gdalwarp&quot;, paste0(OUTPUT.DIR, &quot;/1_images/SRTM/SRTM-1arcsec_raw.tif&quot;), &quot;-t_srs &#39;+proj=utm +zone=31 +datum=WGS84&#39;&quot;, &quot;-tr 30 30&quot;, paste(&quot;-te&quot;, TGO.ext@xmin, TGO.ext@ymin, TGO.ext@xmax, TGO.ext@ymax), paste0(OUTPUT.DIR, &quot;/1_images/SRTM/SRTM-1arcsec.tif&quot;), &quot;-ot &#39;Int16&#39;&quot;, &quot;-co COMPRESS=&#39;LZW&#39;&quot;, &quot;-co INTERLEAVE=&#39;BAND&#39;&quot;, &quot;-overwrite&quot;)) file.remove(paste0(OUTPUT.DIR, &quot;/1_images/SRTM/SRTM-1arcsec_raw.tif&quot;)) system(paste(&quot;gdalinfo -stats&quot;, paste0(OUTPUT.DIR, &quot;/1_images/SRTM/SRTM-1arcsec.tif&quot;))) dem &lt;- raster(paste0(OUTPUT.DIR, &quot;/1_images/SRTM/SRTM-1arcsec.tif&quot;)) jpeg(paste0(OUTPUT.DIR, &quot;/1_images/SRTM/SRTM-1arcsec.jpeg&quot;), width=1350, height=3000) par(plt=c(0,1,0,1)) plot(dem) plot(mask(dem, TGO, inverse=TRUE), col=&quot;#FFFFFF66&quot;, legend=FALSE, add=TRUE) plot(TGO, add=TRUE, lwd=3) dev.off() # Prepare Worldclim v2 Data ---------------------------- foreach(file=dir(paste0(INPUT.DIR, &quot;/Worldclim&quot;), pattern=&quot;.*Togo[.]tif$&quot;)) %dopar% { # wc.raster &lt;- raster(paste0(INPUT.DIR, &quot;/Worldclim/&quot;, file)) # raster.downscale(raster(paste0(INPUT.DIR, &quot;/Worldclim/&quot;, file)), dem.30, ...) system(paste(&quot;gdalwarp&quot;, paste0(INPUT.DIR, &quot;/Worldclim/&quot;, file), &quot;-t_srs &#39;+proj=utm +zone=31 +datum=WGS84&#39;&quot;, &quot;-tr 30 30&quot;, paste(&quot;-te&quot;, TGO.ext@xmin, TGO.ext@ymin, TGO.ext@xmax, TGO.ext@ymax), paste0(OUTPUT.DIR, &quot;/1_images/WCv2/&quot;, file), &quot;-dstnodata -3.4e+38&quot;, &quot;-co COMPRESS=&#39;LZW&#39;&quot;, &quot;-co INTERLEAVE=&#39;BAND&#39;&quot;, &quot;-overwrite&quot;)) system(paste(&quot;gdalinfo -stats&quot;, paste0(OUTPUT.DIR, &quot;/1_images/WCv2/&quot;, file))) } foreach(file=dir(paste0(OUTPUT.DIR, &quot;/1_images/WCv2&quot;), pattern=&quot;.*[.]tif$&quot;)) %dopar% { image &lt;- stack(paste0(OUTPUT.DIR, &quot;/1_images/WCv2/&quot;, file)) type &lt;- unlist(strsplit(file, &quot;_&quot;))[3] if (type == &quot;prec&quot;) { zlim &lt;- c(0,320); col &lt;- rev(topo.colors(255)) } else if (type == &quot;tmin&quot;) { zlim &lt;- c(14.0,27.8); col &lt;- rev(heat.colors(255)) } else if (type == &quot;tmax&quot;) { zlim &lt;- c(24.9,37.5); col &lt;- rev(heat.colors(255)) } else if (type == &quot;tavg&quot;) { zlim &lt;- c(19.7,32.7); col &lt;- rev(heat.colors(255)) } else { zlim &lt;- NA; col &lt;- rev(cm.colors(255)) } foreach(i=1:nlayers(image)) %dopar% { jpeg(paste0(OUTPUT.DIR, &quot;/1_images/WCv2/&quot;, sub(&quot;[.]tif$&quot;, &quot;&quot;, file), &quot;-&quot;, str_pad(i, 2, &quot;left&quot;, 0), &quot;.jpeg&quot;), width=1350, height=3000) plot(image[[i]], col=col, zlim=zlim) plot(mask(image[[i]], TGO, inverse=TRUE), col=&quot;#FFFFFF66&quot;, legend=FALSE, add=TRUE) plot(TGO, add=TRUE, lwd=3) dev.off() } } "],
["4-0_FCC.html", "4 Cartographie des surfaces forestiers", " 4 Cartographie des surfaces forestiers Description générale de l’approche / méthodologies "],
["4-2_create-train-points.html", "4.1 Parcelles d’entraînement", " 4.1 Parcelles d’entraînement Description Description de la méthodologie / script FCC/2_create-train-points.R ################################################################################## # NERF_Togo/FCC/2_create-train-points.R: create a set of tree cover training plots # -------------------------------------------------------------------------------- # Bern University of Applied Sciences # Oliver Gardi, &lt;oliver.gardi@bfh.ch&gt; # 20 May 2019 library(&quot;spsurvey&quot;) TRNPTS &lt;- c(paste0(TRNPTS.DIR, &quot;/1_trn10k-s10ndvi_rev/2_assessed&quot;), paste0(TRNPTS.DIR, &quot;/2_trn05k-prob040-060/2_assessed&quot;), paste0(TRNPTS.DIR, &quot;/3_trn1k-prob040-060/2_assessed&quot;)) # Create a grid of observation points over whole Togo, based on Landsat images (30 x 30m) each 480 m ------------------- res &lt;- 480 x.min &lt;- res * extent(TGO)@xmin %/% res x.max &lt;- res * extent(TGO)@xmax %/% res y.min &lt;- res * extent(TGO)@ymin %/% res y.max &lt;- res * extent(TGO)@ymax %/% res frame.points &lt;- SpatialPoints(expand.grid(seq(x.min, x.max, by=res), seq(y.min, y.max, by=res)), proj4string=utm.31)[TGO] # add attributes frame.points$PLOTID &lt;- paste0(str_pad(frame.points@coords[,1], 7, &quot;left&quot;, &quot;0&quot;), &quot;_&quot;, str_pad(frame.points@coords[,2], 7, &quot;left&quot;, &quot;0&quot;)) frame.points$xcoords &lt;- frame.points@coords[,1] frame.points$ycoords &lt;- frame.points@coords[,2] # load 2018 NDVI and mask with water, clouds and shadow ndvi.p192 &lt;- mask(raster(paste0(OUTPUT.DIR, &quot;/1_images/p192/p192_2018.tif&quot;), band=12), raster(paste0(OUTPUT.DIR, &quot;/1_images/p192/p192_2018_qaLC08.tif&quot;)) %in% c(qa.cloud, qa.shadow, qa.water, qa.ice), maskvalue=TRUE) ndvi.p193 &lt;- mask(raster(paste0(OUTPUT.DIR, &quot;/1_images/p193/p193_2018.tif&quot;), band=12), raster(paste0(OUTPUT.DIR, &quot;/1_images/p193/p193_2018_qaLC08.tif&quot;)) %in% c(qa.cloud, qa.shadow, qa.water, qa.ice), maskvalue=TRUE) ndvi.p194 &lt;- mask(raster(paste0(OUTPUT.DIR, &quot;/1_images/p194/p194_2018.tif&quot;), band=12), raster(paste0(OUTPUT.DIR, &quot;/1_images/p194/p194_2018_qaLC08.tif&quot;)) %in% c(qa.cloud, qa.shadow, qa.water, qa.ice), maskvalue=TRUE) ndvi &lt;- merge(ndvi.p192, ndvi.p193, ndvi.p194) rm(ndvi.p192, ndvi.p193, ndvi.p194) # read sampling frame and add NDVI for each plot frame.points$ndvi &lt;- raster::extract(ndvi, frame.points) frame.points$ndvi_c &lt;- cut(frame.points$ndvi, 10, labels=paste0(&quot;s&quot;, 0:9)) writeOGR(frame.points, dsn=paste0(INPUT.DIR, &quot;/Train-Val/201908&quot;), layer=&quot;TGO_frame_480m&quot;, driver=&quot;ESRI Shapefile&quot;, overwrite=TRUE) # Sampling frame for training-points ----------------------- # Initialize random number generator set.seed(1) # design for a spatially balanced sample, drawing 1500 samples of each stratum Dsgn.grt &lt;- list(&quot;s0&quot;=list(panel=c(PanelOne=1500), seltype=&quot;Equal&quot;), &quot;s1&quot;=list(panel=c(PanelOne=1500), seltype=&quot;Equal&quot;), &quot;s2&quot;=list(panel=c(PanelOne=1500), seltype=&quot;Equal&quot;), &quot;s3&quot;=list(panel=c(PanelOne=1500), seltype=&quot;Equal&quot;), &quot;s4&quot;=list(panel=c(PanelOne=1500), seltype=&quot;Equal&quot;), &quot;s5&quot;=list(panel=c(PanelOne=1500), seltype=&quot;Equal&quot;), &quot;s6&quot;=list(panel=c(PanelOne=1500), seltype=&quot;Equal&quot;), &quot;s7&quot;=list(panel=c(PanelOne=1500), seltype=&quot;Equal&quot;), &quot;s8&quot;=list(panel=c(PanelOne=1500), seltype=&quot;Equal&quot;), &quot;s9&quot;=list(panel=c(PanelOne=1500), seltype=&quot;Equal&quot;) ) train.points &lt;- grts(design=Dsgn.grt, # using Dsgn design object DesignID=&#39;train&#39;, # prefix for each point name type.frame=&#39;finite&#39;, # type src.frame=&#39;sp.object&#39;, # sample frame is shapefile sp.object=frame.points, stratum=&quot;ndvi_c&quot; ) summary(train.points) # apply coordinate reference system proj4string(train.points) &lt;- proj4string(frame.points) # shuffle the rows train.points &lt;- train.points[sample(1:nrow(train.points)), ] train.points$SAMPLEID &lt;- paste0(&quot;trn-&quot;, str_pad(string=1:nrow(train.points), width = 4, pad = &quot;0&quot;, side = &quot;left&quot;)) # Convert to plots and add attributes -------------------- landsat.grid &lt;- raster(ndvi) values(landsat.grid) &lt;- 1 # convert to polygons train.plots &lt;- rasterToPolygons(mask(landsat.grid, train.points)) # fetch attributes train.plots@data &lt;- over(train.plots, train.points[, c(&quot;PLOTID&quot;, &quot;SAMPLEID&quot;, &quot;xcoords&quot;, &quot;ycoords&quot;, &quot;ndvi&quot;, &quot;stratum&quot;)]) # add attributes train.plots$ccov &lt;- as.character(NA) train.plots$img_src &lt;- train.plots$img_date &lt;- as.character(NA) train.plots$author &lt;- train.plots$mod_date &lt;- as.character(NA) # # write plots as Shapefile and KML writeOGR(train.plots, dsn=paste0(INPUT.DIR, &quot;/Train-Val/201908/trn10k-s10ndvi/empty&quot;), layer=&quot;COV_parcelles&quot;, driver=&quot;ESRI Shapefile&quot;, overwrite=TRUE) writeKML(train.plots, kmlname=&quot;COV_parcelles&quot;, filename=paste0(INPUT.DIR, &quot;/Train-Val/201908/trn10k-s10ndvi/empty/COV_parcelles.kml&quot;)) # Create 7x7 sample grid ------------------------------------ grid.size &lt;- 7 res &lt;- res(landsat.grid)[1] offset &lt;- c(res/grid.size/2 + (0:(grid.size-1))*res/grid.size) # split the plots for parallel processing subsets &lt;- split(train.plots, f=1:86) train.grids &lt;- foreach(subset=subsets, .combine=rbind, .multicombine=TRUE) %dopar% { grids &lt;- SpatialPointsDataFrame(data.frame(x = 0, y = 0), data=data.frame(PLOTID = 0, SAMPLEID = 0, GRIDPOINT = 0))[-1,] for(p in 1:length(subset)) { plot &lt;- subset[p,] ext &lt;- extent(plot) grids &lt;- bind(grids, SpatialPointsDataFrame(expand.grid(ext@xmin+offset, ext@ymin+offset), data=data.frame(PLOTID = plot$PLOTID, SAMPLEID = plot$SAMPLEID, GRIDPOINT = 1:grid.size^2))) } grids } proj4string(train.grids) &lt;- proj4string(train.plots) train.grids$tree &lt;- as.integer(NA) # 1 or 0 writeOGR(train.grids, dsn=paste0(INPUT.DIR, &quot;/Train-Val/201908/trn10k-s10ndvi/empty&quot;), layer=&quot;COV_parcelles_grid&quot;, driver=&quot;ESRI Shapefile&quot;, overwrite=TRUE) # Divide into 10 subsets and export -------------------- subsets &lt;- split(train.plots, f=1:10) for(i in 1:length(subsets)) { writeOGR(subsets[[i]], dsn=paste0(INPUT.DIR, &quot;/Train-Val/201908/trn10k-s10ndvi/empty&quot;), layer=paste0(&quot;COV_parcelles_&quot;, i), driver=&quot;ESRI Shapefile&quot;, overwrite=TRUE) writeKML(subsets[[i]], kmlname=paste0(&quot;COV_parcelles_&quot;, i) , filename=paste0(INPUT.DIR, &quot;/Train-Val/201908/trn10k-s10ndvi/empty/COV_parcelles_&quot;, i, &quot;.kml&quot;)) subset.grids &lt;- train.grids[train.grids$PLOTID %in% subsets[[i]]$PLOTID,] writeOGR(subset.grids, dsn=paste0(INPUT.DIR, &quot;/Train-Val/201908/trn10k-s10ndvi/empty&quot;), layer=paste0(&quot;COV_parcelles_&quot;, i, &quot;_grid&quot;), driver=&quot;ESRI Shapefile&quot;, overwrite=TRUE) } # read and merge assessed training-plots ------------------------ subsets &lt;- dir(TRNPTS, pattern=&quot;[[:digit:]]\\\\.shp$&quot;, recursive=TRUE, full.names=TRUE) train.plots.in &lt;- lapply(subsets, readOGR) names(train.plots.in) &lt;- list.dirs(TRNPTS, recursive=FALSE) for(i in 1:length(train.plots.in)) { train.plots.in[[i]]$author &lt;- as.character(train.plots.in[[i]]$author) train.plots.in[[i]]$author[!is.na(train.plots.in[[i]]$author)] &lt;- names(train.plots.in[i]) train.plots.in[[i]] &lt;- train.plots.in[[i]][, c(&quot;PLOTID&quot;, &quot;SAMPLEID&quot;, &quot;xcoords&quot;, &quot;ycoords&quot;, &quot;ccov&quot;, &quot;img_date&quot;, &quot;img_src&quot;, &quot;mod_date&quot;, &quot;author&quot;)] proj4string(train.plots.in[[i]]) &lt;- utm.31 } train.plots.in &lt;- do.call(rbind, train.plots.in) subsets.grids &lt;- dir(TRNPTS, pattern=&quot;.*[[:digit:]]+_grid\\\\.shp$&quot;, recursive=TRUE, full.names=TRUE) train.grids.in &lt;- lapply(subsets.grids, readOGR) for(i in 1:length(train.grids.in)) { proj4string(train.grids.in[[i]]) &lt;- utm.31 } train.grids.in &lt;- do.call(rbind, train.grids.in) # recalculate crown-cover and merge train.plots.in &lt;- train.plots.in[, names(train.plots.in) != &quot;ccov&quot;] tmp &lt;- aggregate(list(ccov=train.grids.in$tree), by=list(PLOTID=train.grids.in$PLOTID), FUN=function(x) sum(!is.na(x) &amp; x==1)/sum(!is.na(x))) tmp$ccov[is.nan(tmp$ccov)] &lt;- NA train.plots.in &lt;- merge(train.plots.in, tmp, by=&quot;PLOTID&quot;) # clean training plot data --------------------- # remove plots without ccov train.plots.in &lt;- train.plots.in[!is.na(train.plots.in$ccov),] # remove plots with strange dates year &lt;- as.numeric(sub(&quot;-.*$&quot;, &quot;&quot;, as.character(train.plots.in$img_date))) train.plots.in &lt;- train.plots.in[!is.na(year) &amp; year &gt;= 2000 &amp; year &lt;= 2019, ] # clean grid accordingly train.grids.in &lt;- train.grids.in[train.grids.in$PLOTID %in% unique(train.plots.in$PLOTID), ] # Write clean training plots ----------------- writeOGR(train.plots.in, dsn=TRNPTS.DIR, layer=&quot;COV_parcelles&quot;, driver=&quot;ESRI Shapefile&quot;, overwrite=TRUE) writeOGR(train.grids.in, dsn=TRNPTS.DIR, layer=&quot;COV_parcelles_grid&quot;, driver=&quot;ESRI Shapefile&quot;, overwrite=TRUE) dir.create(paste0(TRNPTS.DIR, &quot;/descr&quot;), showWarnings = FALSE) # Write some descriptive information pdf(paste0(TRNPTS.DIR, &quot;/descr/hist-years.pdf&quot;)) hist(as.numeric(sub(&quot;-.*$&quot;, &quot;&quot;, as.character(train.plots.in$img_date))), breaks=2000:2019) dev.off() pdf(paste0(TRNPTS.DIR, &quot;/descr/hist-ccov.pdf&quot;)) hist(train.plots.in$ccov) dev.off() sink(paste0(INPUT.DIR, &quot;/Train-Val/201908/trn10k-s10ndvi/descr/author-stats.txt&quot;), split=TRUE) table(train.plots.in$author) sink() # # Create additional 500 traininplots for regions of ambiquity (for Ayele) ------------------------ # # # get probability map # prob.map &lt;- raster(paste0(OUTPUT.DIR, &quot;/2_forest-maps/TGO/1_ref-maps/p193/p193_2018_FC30_prob.tif&quot;)) # prob.map[prob.map &lt; 0.4 | prob.map &gt; 0.6] &lt;- NA # prob.map &lt;- mask(prob.map, TGO) # # # sample cells with ambiguity (values 0.4 - 0.6) # amb.plots &lt;- rasterToPolygons(sampleRandom(prob.map, 500, asRaster=TRUE)) # # # fetch attributes # names(amb.plots) &lt;- &quot;NDVI&quot; # amb.plots$PLOTID &lt;- paste0(&quot;add-&quot;, str_pad(string=1:nrow(amb.plots), width = 4, pad = &quot;0&quot;, side = &quot;left&quot;)) # amb.plots$SAMPLEID &lt;- amb.plots$PLOTID # amb.plots$xcoords &lt;- amb.plots$ycoords &lt;- amb.plots$stratum &lt;- as.numeric(NA) # # # add attributes # amb.plots$ccov &lt;- as.character(NA) # amb.plots$img_src &lt;- amb.plots$img_date &lt;- as.character(NA) # amb.plots$author &lt;- amb.plots$mod_date &lt;- as.character(NA) # # # # write plots as Shapefile and KML # writeOGR(amb.plots, dsn=paste0(INPUT.DIR, &quot;/Train-Val/201908/trn10k-s10ndvi/add&quot;), layer=&quot;COV_parcelles_add&quot;, driver=&quot;ESRI Shapefile&quot;, overwrite=TRUE) # writeKML(amb.plots, kmlname=&quot;COV_parcelles&quot;, filename=paste0(INPUT.DIR, &quot;/Train-Val/201908/trn10k-s10ndvi/add/COV_parcelles_add.kml&quot;)) # # # Create sample grid # # grid.size &lt;- 7 # res &lt;- res(amb.map)[1] # offset &lt;- c(res/grid.size/2 + (0:(grid.size-1))*res/grid.size) # # # split the plots for parallel processing # subsets &lt;- split(amb.plots, f=1:10) # # registerDoParallel(.env$numCores-1) # amb.grids &lt;- foreach(subset=subsets, .combine=rbind, .multicombine=TRUE) %dopar% { # grids &lt;- SpatialPointsDataFrame(data.frame(x = 0, y = 0), data=data.frame(PLOTID = 0, SAMPLEID = 0, GRIDPOINT = 0))[-1,] # for(p in 1:length(subset)) { # plot &lt;- subset[p,] # ext &lt;- extent(plot) # grids &lt;- bind(grids, SpatialPointsDataFrame(expand.grid(ext@xmin+offset, ext@ymin+offset), data=data.frame(PLOTID = plot$PLOTID, SAMPLEID = plot$SAMPLEID, GRIDPOINT = 1:grid.size^2))) # } # grids # } # # proj4string(amb.grids) &lt;- proj4string(amb.plots) # amb.grids$tree &lt;- as.integer(NA) # 1 or 0 # # writeOGR(amb.grids, dsn=paste0(INPUT.DIR, &quot;/Train-Val/201908/trn10k-s10ndvi/add&quot;), layer=&quot;COV_parcelles_add_grid&quot;, driver=&quot;ESRI Shapefile&quot;, overwrite=TRUE) # # Create additional 1000 traininplots for regions of ambiquity (for Etse) ------------------------------ # # # get probability map # prob.map &lt;- merge( # raster(paste0(REFMAPS.DIR, &quot;/p193_2018_FC30_R_prob.tif&quot;)), # raster(paste0(REFMAPS.DIR, &quot;/p192_2018_FC30_R_prob.tif&quot;)), # raster(paste0(REFMAPS.DIR, &quot;/p194_2018_FC30_R_prob.tif&quot;)) # ) # # prob.map &lt;- mask(crop(prob.map, TGO), TGO) # prob.map[prob.map &lt; 0.4 | prob.map &gt; 0.6] &lt;- NA # # # # sample cells with ambiguity (values 0.4 - 0.6) # amb.plots &lt;- rasterToPolygons(sampleRandom(prob.map, 1000, asRaster=TRUE)) # # # fetch attributes # names(amb.plots) &lt;- &quot;NDVI&quot; # amb.plots$PLOTID &lt;- paste0(&quot;add2-&quot;, str_pad(string=1:nrow(amb.plots), width = 4, pad = &quot;0&quot;, side = &quot;left&quot;)) # amb.plots$SAMPLEID &lt;- amb.plots$PLOTID # amb.plots$xcoords &lt;- amb.plots$ycoords &lt;- amb.plots$stratum &lt;- as.numeric(NA) # # # add attributes # amb.plots$ccov &lt;- as.character(NA) # amb.plots$img_src &lt;- amb.plots$img_date &lt;- as.character(NA) # amb.plots$author &lt;- amb.plots$mod_date &lt;- as.character(NA) # # # # write plots as Shapefile and KML # writeOGR(amb.plots, dsn=paste0(REFMAPS.DIR, &quot;/3_trn1k-prob040-060/1_empty&quot;), layer=&quot;COV_parcelles_add2&quot;, driver=&quot;ESRI Shapefile&quot;, overwrite=TRUE) # writeKML(amb.plots, kmlname=&quot;COV_parcelles&quot;, filename=paste0(REFMAPS.DIR, &quot;/3_trn1k-prob040-060/1_empty/COV_parcelles_add2.kml&quot;)) # # # Create sample grid # # grid.size &lt;- 7 # res &lt;- res(prob.map)[1] # offset &lt;- c(res/grid.size/2 + (0:(grid.size-1))*res/grid.size) # # # split the plots for parallel processing # subsets &lt;- split(amb.plots, f=1:10) # # registerDoParallel(.env$numCores-1) # amb.grids &lt;- foreach(subset=subsets, .combine=rbind, .multicombine=TRUE) %dopar% { # grids &lt;- SpatialPointsDataFrame(data.frame(x = 0, y = 0), data=data.frame(PLOTID = 0, SAMPLEID = 0, GRIDPOINT = 0))[-1,] # for(p in 1:length(subset)) { # plot &lt;- subset[p,] # ext &lt;- extent(plot) # grids &lt;- bind(grids, SpatialPointsDataFrame(expand.grid(ext@xmin+offset, ext@ymin+offset), data=data.frame(PLOTID = plot$PLOTID, SAMPLEID = plot$SAMPLEID, GRIDPOINT = 1:grid.size^2))) # } # grids # } # # proj4string(amb.grids) &lt;- proj4string(amb.plots) # amb.grids$tree &lt;- as.integer(NA) # 1 or 0 # # writeOGR(amb.grids, dsn=paste0(REFMAPS.DIR, &quot;/3_trn1k-prob040-060/1_empty&quot;), layer=&quot;COV_parcelles_add2_grid&quot;, driver=&quot;ESRI Shapefile&quot;, overwrite=TRUE) "]
]
