[
["index.html", "Système Nationale de Surveillance des Forêts au Togo Préface", " Système Nationale de Surveillance des Forêts au Togo Oliver Gardi, Fifonsi Dangbo, Sophie Dzigbod, Ditorgue Bakabima 2020-03-03 Préface Ce manuel d’opération à comme objectif de décrire le fonctionnement du Système National de Surveillance des Forêts au Togo (SNSF). Les éléments à traiter sont les arrangements instutitionelles, l’implémentation de l’Inventaires Forestier National (IFN) et de Système Surveillance Terrestres par Satellite (SSTS) et l’approche technique pour en sortir les informations nécessaires pour le Niveau des emissions de référence pour les forêts du Togo (NERF) ainsi que pour le Monitoring, Reporting et Verification (MRV) dans le cadre de l’engagement du Togo pour le REDD+. Le manuel NERF/MRV décrit en détail les outils utilisés pour établir et le Niveau des emissions de référence pour les forêts du Togo et pour mettre à jour les analyses dans le cadre d’une surveillance de la biomasse forestier dans la cadre du Monitoring, Reporting et Verification pour la REDD+. Il décrit également l’acquistion des données nécessaires, notamment l’acquisition des images satellitaies, la définition des parcelles d’entraînement de validation dans la cadre du Système de surveillance des terres par satellites (SSTS) et les inventaires forestiers. Les résultats de ces analyses sont publiés ailleurs (liens sur les rapports sur le site CCNUCC et géoportail). Les outils peuvent être utilisé pour faire autres projets de classification des changements d’utilisation des terres et cartographie de la biomasse. En cas de questions, veuillez contacter la Coordination nationale REDD+ du Togo "],
["00_introduction.html", "1 Introduction", " 1 Introduction Blah, blah, blah "],
["01_arrangements.html", "1.1 Arrangements institutionelles", " 1.1 Arrangements institutionelles Blah, blah "],
["00_STSS.html", "2 Système de Surveillance Terrestre", " 2 Système de Surveillance Terrestre Blah, blah, blah "],
["00_IFN.html", "3 Inventaire Forestier National", " 3 Inventaire Forestier National Blah, blah, blah "],
["00_NERF-MRV.html", "4 Analyses NERF/MRV", " 4 Analyses NERF/MRV Blah, blah, blah "],
["01-organisation.html", "4.1 Organisation du travail", " 4.1 Organisation du travail 4.1.1 Logiciel et serveur La cartographie des surfaces forestier et de la biomasse aérienne des arbres se fait avec le logiciel R, dirigé à travers RStudio. Les scripts R sont mis à disposition dans un dépot GitHub. Les scripts dépendent des outils GDAL disponible dans l’environnement, à installer pour les systèmes Linux avec apt-get install python-gdal. Le traitement et l’analyse des images satellitaires demande des ressources intensives en terme de stockage de données (plusieurs TB) et capacité de calcul (plusieurs processeurs en parallèle). Pour le moment les analyses pour le Niveau des emissions de référence pour les forêts du Togo (NERF-Togo) sont éfféctués sur l’infrastructure informatique centrale de la BFH-HAFL. L’interface R-Studio est disponible à http://r.gro1.bfh.science. 4.1.2 Structure des fichiers Le repertoire de base NERF_Togo est structuré comme suivant: NERF_Togo ├── data # Données de base │ ├── GADM # frontières administratives │ ├── IFN # inventaire forestier national │ ├── Landsat # images satellitaires │ ├── SRTM # données topographiques │ └── Worldclim # données climatiques └── NERF_v1 # Répétoire projet ├── input # données spécifiques du projet │ ├── 1_images # images pre-traités │ ├── 2_train-plots # données d&#39;entraînement │ └── 3_val-plots # données de validation ├── src # scripts │ ├── 0_set-up.R # initialisation │ ├── 1_prepare-images.R # préparation des images │ ├── AGB # cartographie biomasse │ │ ├── 2_compile-IFN.R # évaluer inventaire │ │ ├── 3_create-AGB-maps.R # calibration des cartes │ │ ├── 4_clean-AGB-maps.R # nettoyage des cartes │ │ └── 5_analyze-AGB.R # analyse des cartes │ └── FCC # cartographie surfaces forestiers │ ├── 2_create-train-points.R # création des point d&#39;entraînement │ ├── 3_create-fc-maps.R # calibration des cartes │ ├── 4_clean-fc-maps.R # nettoyage des cartes │ ├── 5_create-val-points.R # création des points de validation │ ├── 6_validate-fc-maps.R # validation des cartes │ ├── 7_fc-maps-accuracy.R # analyse de la précision des cartes │ └── 8_analyse-fc-maps.R # analyse des cartes ├── output # résultats │ ├── 1_forest-cover # cartes surfaces forestiers │ │ ├── 1_ref-maps # cartes référence │ │ ├── 2_raw-maps # cartes brutes │ │ ├── 3_clean-maps # cartes nettoyées │ │ ├── 4_validation # résultats validation │ │ └── 5_results # résultats surfaces │ └── 2_biomass # cartes biomasse │ ├── 1_ref-maps # cartes de référence │ └── 5_results # résultats biomasse ├── report # rapport des résultats └── manuel # ce manuel C’est seulement les répétoires src et manual qui sont mis à diposition dans le dépôt GitHub. Les autres répétoires et données doivent être installés manuellement. "],
["00_preparation.html", "4.2 Préparation des données", " 4.2 Préparation des données Blah, blah, blah "],
["01_nouveau-projet.html", "", " 4.2.1 Création d’un nouveau projet Pour la création d’un nouveau projet, le plus simple est de copier un projet existant, normalement le projet sur lequel on aimeriait continuer le travail. On peut faire ça à travers RStudio, SFTP ou directement sur le server par exemple cp -r 2019_NERF_v1 2020_MNV_v0. Après on efface les résultat de l’ancien projet rm -r 2020_MNV_v0/output/*. Les rapports on peut laisser, mais il faut les rédiger à la fin. 4.2.2 Acquisition des images Ouvrir le site USGS Earthexplorer. Dans la fenêtre Search Criteria il faut selectionner la période pour laquelle on cherche des images (Nov - Jan). Dans la fenêtre Data Sets, il faut sélectionner les produits Landsat Level-2 (Surface Reflectance). Dans la fenêtre Additional Criteria il faut choisir les scènes (chemin 192: 054-056 / chemin 193: 052-055 / chemin 194: 052-053). On prend le date sur les images de tout le chemin sont de meilleure qualité. On copie les identifier des images à télécharger dans un fichier text. Ensuite on ouvre le site USGS ESPA pour commander les scènes. On charge le fichier text avec les scènes et on commande les bandes surface reflectances et les indices. Pour commander des images, il faut qu’on a un compte USGS. Une fois on est notifié par eMail que les images sont prêts, on les téléchargent manuellement ou tous ensemble avec le USGS bulkdownloader et la commande download_espa_order.py -u oliver.gardi@gmail.com -o ALL -d NERF_Togo/data/Landsat. On dézip les images et les rangent dans le répétoire data/Landsat sous la scène et l’année correspondante. Pour des images de l’hiver 2019/20, l’année correspondante est 2020. "],
["02_set-up.html", "", " 4.2.3 Définition des variables Description Le traitement des images ce fait par executer les scripts R, un après l’autre. Tout d’abord on ouvre et execute le script 0_set-up.R pour charger les librairies et définir des variables qu’on utilise également dans les autres scripts. 0_set-up.R ################################################################ # NERF_Togo/0_set-up.R: Loading libraries and defining functions # -------------------------------------------------------------- # Bern University of Applied Sciences # Oliver Gardi, &lt;oliver.gardi@bfh.ch&gt; # 13 November 2019 Sys.setlocale(&quot;LC_CTYPE&quot;, &quot;de_CH.UTF-8&quot;) # set working directory setwd(&quot;~/NERF_Togo/NERF_v1/R&quot;) # Hidden environment for variables and functions .env = new.env() # Libraries ----------------- library(&quot;sp&quot;) # Classes and methods for spatial data library(&quot;rgdal&quot;) # Bindings for the Geospatial Data Abstraction Library library(&quot;rgeos&quot;) # Interface to Geometry Engine - Open Source library(&quot;raster&quot;) # Geographic Data Analysis and Modeling library(&quot;randomForest&quot;) # Breiman and Cutler&#39;s random forests for classification and regression library(&quot;RStoolbox&quot;) # for terrain correction (topCor) library(&quot;caret&quot;) # for confusion matrix library(&quot;openxlsx&quot;) # pour directement lire des fichiers excel (xlsx) library(&quot;dplyr&quot;) # pour cr??er des tableaux crois??s library(&quot;tidyr&quot;) library(&quot;ggplot2&quot;) library(&quot;foreach&quot;) library(&quot;doParallel&quot;) library(&quot;gdalUtils&quot;) library(&quot;stringr&quot;) library(&quot;maptools&quot;) # kmlPolygon # Years ---------------------------------- # .env$YEARS &lt;- c(1987, 1991, 2000, 2003, 2005, 2007, 2010, 2013, 2015, 2017, 2018) .env$PERIOD &lt;- 1985:2019 .env$JNT.YEARS &lt;- c(1987, 2003, 2005, 2007, 2015, 2017, 2018) .env$VAL.YEARS &lt;- c(1987, 2003, 2015, 2018) .env$REF.YEARS &lt;- c( 2003, 2015, 2018) # Directories ---------------------------------- .env$DATA.DIR &lt;- &quot;../../data&quot; .env$INPUT.DIR &lt;- &quot;../input&quot; .env$IMAGES.DIR &lt;- paste0(.env$INPUT.DIR, &quot;/1_images&quot;) .env$TRNPTS.DIR &lt;- paste0(.env$INPUT.DIR, &quot;/2_train-plots&quot;) .env$VALPTS.DIR &lt;- paste0(.env$INPUT.DIR, &quot;/3_val-plots&quot;) .env$INVENT.DIR &lt;- paste0(.env$INPUT.DIR, &quot;/4_IFN&quot;) .env$OUTPUT.DIR &lt;- &quot;../output&quot; .env$FCC.DIR &lt;- paste0(.env$OUTPUT.DIR, &quot;/1_forest-cover&quot;) .env$FCC.REF.DIR &lt;- paste0(.env$FCC.DIR, &quot;/1_ref-maps&quot;) .env$FCC.RAW.DIR &lt;- paste0(.env$FCC.DIR, &quot;/2_raw-maps&quot;) .env$FCC.CLN.DIR &lt;- paste0(.env$FCC.DIR, &quot;/3_clean-maps&quot;) .env$FCC.VAL.DIR &lt;- paste0(.env$FCC.DIR, &quot;/4_validation&quot;) .env$FCC.RES.DIR &lt;- paste0(.env$FCC.DIR, &quot;/5_results&quot;) .env$AGB.DIR &lt;- paste0(.env$OUTPUT.DIR, &quot;/2_biomass&quot;) .env$AGB.REF.DIR &lt;- paste0(.env$AGB.DIR, &quot;/1_ref-maps&quot;) .env$AGB.RAW.DIR &lt;- paste0(.env$AGB.DIR, &quot;/2_raw-maps&quot;) .env$AGB.CLN.DIR &lt;- paste0(.env$AGB.DIR, &quot;/3_clean-maps&quot;) .env$AGB.VAL.DIR &lt;- paste0(.env$AGB.DIR, &quot;/4_validation&quot;) .env$AGB.RES.DIR &lt;- paste0(.env$AGB.DIR, &quot;/5_results&quot;) .env$BANDS &lt;- c(&quot;B&quot;, &quot;G&quot;, &quot;R&quot;, &quot;NIR&quot;, &quot;SWIR1&quot;, &quot;SWIR2&quot;, &quot;evi&quot;, &quot;msavi&quot;, &quot;nbr&quot;, &quot;nbr2&quot;, &quot;ndmi&quot;, &quot;ndvi&quot;, &quot;savi&quot;) .env$BIOCLIM &lt;- paste0(&quot;BIO&quot;, 1:19) # CRS, AOI, Extents ------------------- .env$utm.30 &lt;- crs(&quot;+proj=utm +zone=30 +datum=WGS84 +units=m +no_defs +ellps=WGS84 +towgs84=0,0,0&quot;) .env$utm.31 &lt;- crs(&quot;+proj=utm +zone=31 +datum=WGS84 +units=m +no_defs +ellps=WGS84 +towgs84=0,0,0&quot;) # Boundary of Togo .env$TGO &lt;- spTransform(readOGR(paste0(.env$DATA.DIR, &quot;/GADM/gadm36_TGO_0.shp&quot;)), .env$utm.31) .env$TGO.reg &lt;- spTransform(readOGR(paste0(.env$DATA.DIR, &quot;/GADM/gadm36_TGO_1.shp&quot;)), .env$utm.31) .env$TGO.ext &lt;- extent(151155, 373005, 670665, 1238175) # define extent by xmin, xmax, ymin and ymax # Cloud and water masks for Landsat 4-7 and Landsat 8 ------------------------ # see Landsat 4-7 and Landsat 8 Surface Reflectance Product Guides .env$qa.water &lt;- c( 68, 132, # Landsat 4-7 324, 388, 836, 900, 1348 # Landsat 8 ) .env$qa.shadow &lt;- c( 72, 136, 328, 392, 840, 904, 1350 ) .env$qa.ice &lt;- c( 80, 112, 144, 176, 336, 368, 400, 432, 848, 880, 912, 944, 1352 ) .env$qa.cloud &lt;- c( 96, 112, 160, 176, 224, 352, 368, 416, 432, 480, 864, 880, 928, 944, 992 ) # Misc ------------------- # prepare for parallel computing .env$numCores &lt;- detectCores() attach(.env) "],
["03_prepare-images.html", "", " 4.2.4 Prétraitement des images Description Le premier traitement est la préparation des nouveaux images. On ouvre le script 1_prepare-images.R et on ajoute les nouveaux images dans la liste des images: p192.2019 = list( paste0(DATA.DIR, &quot;/Landsat/192_054/2019/LC081920542018122301T1-SC20190405164258/&quot;), paste0(DATA.DIR, &quot;/Landsat/192_055/2019/LC081920552018122301T1-SC20190405163359/&quot;), paste0(DATA.DIR, &quot;/Landsat/192_056/2019/LC081920562018122301T1-SC20190405163342/&quot;)) ... p193.2019 = list( paste0(DATA.DIR, &quot;/Landsat/193_052/2019/LC081930522019021601T1-SC20190405183839/&quot;), paste0(DATA.DIR, &quot;/Landsat/193_053/2019/LC081930532019021601T1-SC20190405181518/&quot;), paste0(DATA.DIR, &quot;/Landsat/193_054/2019/LC081930542019021601T1-SC20190405183609/&quot;), paste0(DATA.DIR, &quot;/Landsat/193_055/2019/LC081930552019021601T1-SC20190405181507/&quot;)) ... p194.2019 = list( paste0(DATA.DIR, &quot;/Landsat/194_052/2019/LC081940522019012201T1-SC20190405172019/&quot;), paste0(DATA.DIR, &quot;/Landsat/194_053/2019/LC081940532019012201T1-SC20190405172055/&quot;)) Le script définit une fonction pour stacker, masquer et couper les images Landsat chemin par chemin. Par défaut, les images qui ont déjà été traité (filename existe déjà) ne sont plus traité (overwrite=FALSE). prepare.image(in.image.dirs, ext=NULL, filename=NULL, overwrite=FALSE, log=TRUE) À la fin du script 1_prepare-images.R, où c’est noté # DO THE WORK ---------, on commence avec le traitement des images. Les images prétraités sont sauveguarder dans le répétoire input/1_images du projet, ensemble avec des Thumbnails des chemins. Dans une prochaîne étape, les images sont néttoyées de l’eau, nuages et ombres en utilisant les bandes Landsat de qualité des pixels. Finalement également les données topographiques (SRTM) et climatiques (Worldclim v2) sont préparés. Example Chemin 193 de l’année 2019 (composé de 4 scènes Landsat du 16 Février 2019) 1_prepare-images.R #################################################################### # NERF_Togo/1_prepare-images.R: cleaning and stacking Landsat images # ------------------------------------------------------------------ # Bern University of Applied Sciences # Oliver Gardi, &lt;oliver.gardi@bfh.ch&gt; # 13 November 2019 ## @knitr load.images # load WRS scenes WRS &lt;- readOGR(paste0(DATA.DIR, &quot;/Landsat/WRS2/WRS2_descending.shp&quot;)) # Selection of images to prepare and to merge ------------------------------------------- in.image.list &lt;- list( # Path 192 p192.1986 = list(paste0(DATA.DIR, &quot;/Landsat/192_054/1986/LT051920541986011301T1-SC20190405164223/&quot;), paste0(DATA.DIR, &quot;/Landsat/192_055/1986/LT051920551986011301T1-SC20190405164227/&quot;), paste0(DATA.DIR, &quot;/Landsat/192_056/1986/LT051920561986011301T1-SC20190405164153/&quot;)), p192.1987 = list(paste0(DATA.DIR, &quot;/Landsat/192_054/1987/LT051920541986123101T1-SC20190405164150/&quot;), paste0(DATA.DIR, &quot;/Landsat/192_055/1987/LT051920551986123101T1-SC20190405163521/&quot;), paste0(DATA.DIR, &quot;/Landsat/192_056/1987/LT051920561986123101T1-SC20190405164444/&quot;)), p192.1991 = list(paste0(DATA.DIR, &quot;/Landsat/192_054/1991/LT041920541991010301T1-SC20190405164201/&quot;), paste0(DATA.DIR, &quot;/Landsat/192_055/1991/LT041920551991010301T1-SC20190405165911/&quot;), paste0(DATA.DIR, &quot;/Landsat/192_056/1991/LT041920561991010301T1-SC20190405163911/&quot;)), p192.2001 = list(paste0(DATA.DIR, &quot;/Landsat/192_054/2001/LE071920542000121301T1-SC20190405165521/&quot;), paste0(DATA.DIR, &quot;/Landsat/192_055/2001/LE071920552000121301T1-SC20190405165645/&quot;), paste0(DATA.DIR, &quot;/Landsat/192_056/2001/LE071920562000121301T1-SC20190405164029/&quot;)), p192.2003 = list(paste0(DATA.DIR, &quot;/Landsat/192_054/2003/LE071920542003010401T1-SC20190520111322/&quot;), paste0(DATA.DIR, &quot;/Landsat/192_055/2003/LE071920552003010401T1-SC20190520100402/&quot;), paste0(DATA.DIR, &quot;/Landsat/192_056/2003/LE071920562003010401T1-SC20190520100206/&quot;)), p192.2005 = list(paste0(DATA.DIR, &quot;/Landsat/192_054/2005/LE071920542004122401T1-SC20190405165520/&quot;), paste0(DATA.DIR, &quot;/Landsat/192_055/2005/LE071920552004122401T1-SC20190405164050/&quot;), paste0(DATA.DIR, &quot;/Landsat/192_056/2005/LE071920562004122401T1-SC20190405164030/&quot;)), p192.2007 = list(paste0(DATA.DIR, &quot;/Landsat/192_054/2007/LE071920542006123001T1-SC20190406034211/&quot;), paste0(DATA.DIR, &quot;/Landsat/192_055/2007/LE071920552006123001T1-SC20190406034231/&quot;), paste0(DATA.DIR, &quot;/Landsat/192_056/2007/LE071920562006123001T1-SC20190406034202/&quot;)), p192.2011 = list(paste0(DATA.DIR, &quot;/Landsat/192_054/2011/LE071920542011011001T1-SC20190406034214/&quot;), paste0(DATA.DIR, &quot;/Landsat/192_055/2011/LE071920552011011001T1-SC20190406034114/&quot;), paste0(DATA.DIR, &quot;/Landsat/192_056/2011/LE071920562011011001T1-SC20190406034155/&quot;)), p192.2013 = list(paste0(DATA.DIR, &quot;/Landsat/192_054/2013/LE071920542013013101T1-SC20190406034224/&quot;), paste0(DATA.DIR, &quot;/Landsat/192_055/2013/LE071920552013013101T1-SC20190406034046/&quot;), paste0(DATA.DIR, &quot;/Landsat/192_056/2013/LE071920562013013101T1-SC20190406034057/&quot;)), p192.2015 = list(paste0(DATA.DIR, &quot;/Landsat/192_054/2015/LC081920542015011301T1-SC20190405163446/&quot;), paste0(DATA.DIR, &quot;/Landsat/192_055/2015/LC081920552015011301T1-SC20190405163723/&quot;), paste0(DATA.DIR, &quot;/Landsat/192_056/2015/LC081920562015011301T1-SC20190405164231/&quot;)), p192.2017 = list(paste0(DATA.DIR, &quot;/Landsat/192_054/2017/LC081920542017021901T1-SC20190405163339/&quot;), paste0(DATA.DIR, &quot;/Landsat/192_055/2017/LC081920552017021901T1-SC20190405163342/&quot;), paste0(DATA.DIR, &quot;/Landsat/192_056/2017/LC081920562017021901T1-SC20190405163222/&quot;)), p192.2018 = list(paste0(DATA.DIR, &quot;/Landsat/192_054/2018/LC081920542018010501T1-SC20190405164304/&quot;), paste0(DATA.DIR, &quot;/Landsat/192_055/2018/LC081920552018010501T1-SC20190405163402/&quot;), paste0(DATA.DIR, &quot;/Landsat/192_056/2018/LC081920562018010501T1-SC20190405163250/&quot;)), p192.2019 = list(paste0(DATA.DIR, &quot;/Landsat/192_054/2019/LC081920542018122301T1-SC20190405164258/&quot;), paste0(DATA.DIR, &quot;/Landsat/192_055/2019/LC081920552018122301T1-SC20190405163359/&quot;), paste0(DATA.DIR, &quot;/Landsat/192_056/2019/LC081920562018122301T1-SC20190405163342/&quot;)), # # Path 193 p193.1985 = list(paste0(DATA.DIR, &quot;/Landsat/193_052/1985/LT051930521985030601T1-SC20190520100259/&quot;), paste0(DATA.DIR, &quot;/Landsat/193_053/1985/LT051930531985030601T1-SC20190520100324/&quot;), paste0(DATA.DIR, &quot;/Landsat/193_054/1985/LT051930541985030601T1-SC20190520100340/&quot;), paste0(DATA.DIR, &quot;/Landsat/193_055/1985/LT051930551985030601T1-SC20190520100140/&quot;)), p193.1987 = list(paste0(DATA.DIR, &quot;/Landsat/193_052/1987/LT051930521987012301T1-SC20190405182322/&quot;), paste0(DATA.DIR, &quot;/Landsat/193_053/1987/LT051930531987012301T1-SC20190405182335/&quot;), paste0(DATA.DIR, &quot;/Landsat/193_054/1987/LT051930541987012301T1-SC20190405182331/&quot;), paste0(DATA.DIR, &quot;/Landsat/193_055/1987/LT051930551987012301T1-SC20190405182328/&quot;)), # images of two different dates! p193.1990.1 = list(paste0(DATA.DIR, &quot;/Landsat/193_052/1990/LT051930521989112801T1-SC20190520100201/&quot;), paste0(DATA.DIR, &quot;/Landsat/193_053/1990/LT051930531989112801T1-SC20190520100233/&quot;)), p193.1990.2 = list(paste0(DATA.DIR, &quot;/Landsat/193_054/1991/LT041930541991011001T1-SC20190402043117/&quot;), paste0(DATA.DIR, &quot;/Landsat/193_055/1991/LT041930551991011001T1-SC20190402042453/&quot;)), p193.2000 = list(paste0(DATA.DIR, &quot;/Landsat/193_052/2000/LE071930522000020401T1-SC20190520100729/&quot;), paste0(DATA.DIR, &quot;/Landsat/193_053/2000/LE071930532000020401T1-SC20190520100345/&quot;), paste0(DATA.DIR, &quot;/Landsat/193_054/2000/LE071930542000020401T1-SC20190402045232/&quot;), paste0(DATA.DIR, &quot;/Landsat/193_055/2000/LE071930552000020401T1-SC20190402043121/&quot;)), p193.2003 = list(paste0(DATA.DIR, &quot;/Landsat/193_052/2003/LE071930522002122601T1-SC20190405182352/&quot;), paste0(DATA.DIR, &quot;/Landsat/193_053/2003/LE071930532002122601T1-SC20190405182309/&quot;), paste0(DATA.DIR, &quot;/Landsat/193_054/2003/LE071930542002122601T1-SC20190405182226/&quot;), paste0(DATA.DIR, &quot;/Landsat/193_055/2003/LE071930552002122601T1-SC20190405190255/&quot;)), p193.2005 = list(paste0(DATA.DIR, &quot;/Landsat/193_052/2005/LE071930522005021701T1-SC20190405190117/&quot;), paste0(DATA.DIR, &quot;/Landsat/193_053/2005/LE071930532005021701T1-SC20190405190003/&quot;), paste0(DATA.DIR, &quot;/Landsat/193_054/2005/LE071930542005021701T1-SC20190405182210/&quot;), paste0(DATA.DIR, &quot;/Landsat/193_055/2005/LE071930552005021701T1-SC20190405190021/&quot;)), p193.2007 = list(paste0(DATA.DIR, &quot;/Landsat/193_052/2007/LE071930522007012201T1-SC20190405182221/&quot;), paste0(DATA.DIR, &quot;/Landsat/193_053/2007/LE071930532007012201T1-SC20190405182607/&quot;), paste0(DATA.DIR, &quot;/Landsat/193_054/2007/LE071930542007012201T1-SC20190405182139/&quot;), paste0(DATA.DIR, &quot;/Landsat/193_055/2007/LE071930552007012201T1-SC20190405182418/&quot;)), p193.2009 = list(paste0(DATA.DIR, &quot;/Landsat/193_052/2009/LE071930522009012701T1-SC20190405182143/&quot;), paste0(DATA.DIR, &quot;/Landsat/193_053/2009/LE071930532009012701T1-SC20190405182301/&quot;), paste0(DATA.DIR, &quot;/Landsat/193_054/2009/LE071930542009012701T1-SC20190405182103/&quot;), paste0(DATA.DIR, &quot;/Landsat/193_055/2009/LE071930552009012701T1-SC20190405182754/&quot;)), p193.2013 = list(paste0(DATA.DIR, &quot;/Landsat/193_052/2013/LE071930522013022301T1-SC20190405182200/&quot;), paste0(DATA.DIR, &quot;/Landsat/193_053/2013/LE071930532013022301T1-SC20190405182213/&quot;), paste0(DATA.DIR, &quot;/Landsat/193_054/2013/LE071930542013022301T1-SC20190405182152/&quot;), paste0(DATA.DIR, &quot;/Landsat/193_055/2013/LE071930552013022301T1-SC20190405182331/&quot;)), p193.2015 = list(paste0(DATA.DIR, &quot;/Landsat/193_052/2015/LC081930522015010401T1-SC20190405181512/&quot;), paste0(DATA.DIR, &quot;/Landsat/193_053/2015/LC081930532015010401T1-SC20190405181751/&quot;), paste0(DATA.DIR, &quot;/Landsat/193_054/2015/LC081930542015010401T1-SC20190402042510/&quot;), paste0(DATA.DIR, &quot;/Landsat/193_055/2015/LC081930552015010401T1-SC20190402042446/&quot;)), p193.2017 = list(paste0(DATA.DIR, &quot;/Landsat/193_052/2017/LC081930522017012501T1-SC20190405181511/&quot;), paste0(DATA.DIR, &quot;/Landsat/193_053/2017/LC081930532017012501T1-SC20190405181440/&quot;), paste0(DATA.DIR, &quot;/Landsat/193_054/2017/LC081930542017012501T1-SC20190405181458/&quot;), paste0(DATA.DIR, &quot;/Landsat/193_055/2017/LC081930552017012501T1-SC20190405181444/&quot;)), p193.2018 = list(paste0(DATA.DIR, &quot;/Landsat/193_052/2018/LC081930522018011201T1-SC20190405181524/&quot;), paste0(DATA.DIR, &quot;/Landsat/193_053/2018/LC081930532018011201T1-SC20190405181459/&quot;), paste0(DATA.DIR, &quot;/Landsat/193_054/2018/LC081930542018011201T1-SC20190405181510/&quot;), paste0(DATA.DIR, &quot;/Landsat/193_055/2018/LC081930552018011201T1-SC20190405181442/&quot;)), p193.2019 = list(paste0(DATA.DIR, &quot;/Landsat/193_052/2019/LC081930522019021601T1-SC20190405183839/&quot;), paste0(DATA.DIR, &quot;/Landsat/193_053/2019/LC081930532019021601T1-SC20190405181518/&quot;), paste0(DATA.DIR, &quot;/Landsat/193_054/2019/LC081930542019021601T1-SC20190405183609/&quot;), paste0(DATA.DIR, &quot;/Landsat/193_055/2019/LC081930552019021601T1-SC20190405181507/&quot;)), # # Path 194 p194.1986 = list(paste0(DATA.DIR, &quot;/Landsat/194_052/1986/LT051940521986011101T1-SC20190405172804/&quot;), paste0(DATA.DIR, &quot;/Landsat/194_053/1986/LT051940531986011101T1-SC20190405172758/&quot;)), p194.1987 = list(paste0(DATA.DIR, &quot;/Landsat/194_052/1987/LT051940521986122901T1-SC20190405172903/&quot;), paste0(DATA.DIR, &quot;/Landsat/194_053/1987/LT051940531986122901T1-SC20190405174433/&quot;)), p194.1997 = list(paste0(DATA.DIR, &quot;/Landsat/194_052/1997/LT051940521997021001T1-SC20190405181746/&quot;), paste0(DATA.DIR, &quot;/Landsat/194_053/1997/LT051940531997021001T1-SC20190405173130/&quot;)), p194.2000 = list(paste0(DATA.DIR, &quot;/Landsat/194_052/2000/LE071940522000012601T1-SC20190405172721/&quot;), paste0(DATA.DIR, &quot;/Landsat/194_053/2000/LE071940532000012601T1-SC20190405172733/&quot;)), p194.2003 = list(paste0(DATA.DIR, &quot;/Landsat/194_052/2003/LE071940522002121701T1-SC20190405172823/&quot;), paste0(DATA.DIR, &quot;/Landsat/194_053/2003/LE071940532002121701T1-SC20190405172739/&quot;)), p194.2005 = list(paste0(DATA.DIR, &quot;/Landsat/194_052/2005/LE071940522004122201T1-SC20190405172700/&quot;), paste0(DATA.DIR, &quot;/Landsat/194_053/2005/LE071940532004122201T1-SC20190405172612/&quot;)), p194.2007 = list(paste0(DATA.DIR, &quot;/Landsat/194_052/2007/LT051940522007010501T1-SC20190405172919/&quot;), paste0(DATA.DIR, &quot;/Landsat/194_053/2007/LT051940532007010501T1-SC20190405172216/&quot;)), p194.2010 = list(paste0(DATA.DIR, &quot;/Landsat/194_052/2010/LE071940522010012101T1-SC20190405172745/&quot;), paste0(DATA.DIR, &quot;/Landsat/194_053/2010/LE071940532010012101T1-SC20190405173304/&quot;)), p194.2012 = list(paste0(DATA.DIR, &quot;/Landsat/194_052/2012/LE071940522012011101T1-SC20190405173146/&quot;), paste0(DATA.DIR, &quot;/Landsat/194_053/2012/LE071940532012011101T1-SC20190405172236/&quot;)), # Indices not available! # p194.2013 = list(paste0(DATA.DIR, &quot;/Landsat/194_052/2013/LE071940522012122801T1-SC20190405172704/&quot;), # paste0(DATA.DIR, &quot;/Landsat/194_053/2013/LE071940532012122801T1-SC20190405172717/&quot;)), p194.2015 = list(paste0(DATA.DIR, &quot;/Landsat/194_052/2015/LC081940522015012701T1-SC20190405172055/&quot;), paste0(DATA.DIR, &quot;/Landsat/194_053/2015/LC081940532015012701T1-SC20190405172042/&quot;)), p194.2017 = list(paste0(DATA.DIR, &quot;/Landsat/194_052/2017/LC081940522016123101T1-SC20190405172058/&quot;), paste0(DATA.DIR, &quot;/Landsat/194_053/2017/LC081940532016123101T1-SC20190405172040/&quot;)), p194.2018 = list(paste0(DATA.DIR, &quot;/Landsat/194_052/2018/LC081940522017121801T1-SC20190405172038/&quot;), paste0(DATA.DIR, &quot;/Landsat/194_053/2018/LC081940532017121801T1-SC20190405174114/&quot;)), p194.2019 = list(paste0(DATA.DIR, &quot;/Landsat/194_052/2019/LC081940522019012201T1-SC20190405172019/&quot;), paste0(DATA.DIR, &quot;/Landsat/194_053/2019/LC081940532019012201T1-SC20190405172055/&quot;)) ) # create scene list ---------- # sink(paste0(INPUT.DIR, &quot;/Landsat/scenes.txt&quot;) # for(image.dirs in in.image.list) { # # for(image.dir in image.dirs) { # # cat(sub(&quot;_ANG[.]txt&quot;, &quot;&quot;, dir(image.dir)[1]),&quot;\\n&quot;) # } # } # sink() # Function for processing and merging a set of Landsat images ----------------------------------------------------------- ## @knitr prepare.image prepare.image &lt;- function(in.image.dirs, ext=NULL, filename=NULL, overwrite=FALSE, log=TRUE) { # load file, if filename already exists and overwrite=FALSE if(!is.null(filename) &amp;&amp; file.exists(filename) &amp;&amp; overwrite==FALSE) { message(&quot;- loading from file &quot;, filename) out.image &lt;- stack(filename) } else { # open logfile if(!is.null(filename) &amp; log==TRUE) { dir.create(dirname(filename), recursive = TRUE, showWarnings = FALSE) logfile &lt;- file(sub(&quot;\\\\.[[:alnum:]]+$&quot;, &quot;.log&quot;, filename), open=&quot;wt&quot;) sink(logfile, type=&quot;output&quot;) sink(logfile, type=&quot;message&quot;) message(date()) } # list for the imported and cleaned images images &lt;- list() qas &lt;- list() # read and process the individual images for(image.dir in in.image.dirs) { image.sensor &lt;- substr(basename(image.dir), 0,4) if(image.sensor==&quot;LC08&quot;) { image &lt;- stack(grep(&quot;^.*_(pixel_qa|band2|band3|band4|band5|band6|band7|evi|msavi|nbr|nbr2|ndmi|ndvi|savi).tif$&quot;, dir(image.dir, full.names=TRUE), value=TRUE)) } else { image &lt;- stack(grep(&quot;^.*_(pixel_qa|band1|band2|band3|band4|band5|band7|evi|msavi|nbr|nbr2|ndmi|ndvi|savi).tif$&quot;, dir(image.dir, full.names=TRUE), value=TRUE)) } image.bands &lt;- sub(&quot;^.*_&quot;, &quot;&quot;, names(image)) # check the number and order of stack-layers if(nlayers(image)!=14 || (image.sensor==&quot;LC08&quot; &amp;&amp; image.bands!=c(&quot;qa&quot;, &quot;band2&quot;, &quot;band3&quot;, &quot;band4&quot;, &quot;band5&quot;, &quot;band6&quot;, &quot;band7&quot;, &quot;evi&quot;, &quot;msavi&quot;, &quot;nbr&quot;, &quot;nbr2&quot;, &quot;ndmi&quot;, &quot;ndvi&quot;, &quot;savi&quot;)) || (image.sensor!=&quot;LC08&quot; &amp;&amp; image.bands!=c(&quot;qa&quot;, &quot;band1&quot;, &quot;band2&quot;, &quot;band3&quot;, &quot;band4&quot;, &quot;band5&quot;, &quot;band7&quot;, &quot;evi&quot;, &quot;msavi&quot;, &quot;nbr&quot;, &quot;nbr2&quot;, &quot;ndmi&quot;, &quot;ndvi&quot;, &quot;savi&quot;))) stop(&quot;image does not have the correct number/order of bands (qa, B, G, R, NIR, SWIR1, SWIR2, evi, msavi, nbr, nbr2, ndmi, ndvi, savi)&quot;) image.name &lt;- substr(names(image)[1], 1, 40) image.scene &lt;- paste0(substr(image.name, 11, 13), &quot;_&quot;, substr(image.name, 14, 16)) image.date &lt;- substr(image.name, 18, 21) image.path &lt;- as.numeric(substr(image.scene, 1, 3)) image.row &lt;- as.numeric(substr(image.scene, 5, 7)) message(&quot;- &quot;, image.name, &quot;: &quot;, appendLF = FALSE) # image &lt;- image[[1]] # crop and mask the image with extent (if any) if(!is.null(ext)) { message(&quot;crop ext ... &quot;, appendLF = FALSE) image &lt;- crop(image, ext) } # crop/mask with WRS message(&quot;crop/mask WRS2 ... &quot;, appendLF = FALSE) wrs &lt;- spTransform(WRS[WRS$PATH==image.path &amp; WRS$ROW==image.row, ], CRS=crs(image)) image &lt;- mask(crop(image, wrs), wrs) # extract and mask with quality band qa &lt;- image[[1]] image &lt;- dropLayer(image, 1) # message(&quot;mask cloud/shadow ... &quot;, appendLF = FALSE) # if(image.sensor==&quot;LC08&quot;) { # # Landsat 8: values 392 - 480 / 840 - 880 / 904 - 992 are medium to high confidence cloud/ice/shadow # # https://prd-wret.s3-us-west-2.amazonaws.com/assets/palladium/production/s3fs-public/atoms/files/LSDS-1368_%20L8_Surface-Reflectance-Code-LASRC-Product-Guide.pdf (p. 22) # qa[qa %in% c(392, 400, 416, 432, 480, # 840, 848, 864, 880, # 904, 912, 928, 944, 992)] &lt;- NA # # } else { # # Landsat 4-7: values ≥ 136 are medium to high confidence cloud/ice/shadow # # https://prd-wret.s3-us-west-2.amazonaws.com/assets/palladium/production/s3fs-public/atoms/files/LSDS-1370_L4-7_Surface%20Reflectance-LEDAPS-Product-Guide.pdf # qa[qa &gt;= 136] &lt;- NA # } # image &lt;- mask(image, qa) # remove non-valid reflectance values message(&quot;clean sr ... &quot;, appendLF = FALSE) for(i in 1:6) { image[[i]] &lt;- reclassify(image[[i]], cbind(-Inf, 0, NA), right=FALSE) image[[i]] &lt;- reclassify(image[[i]], cbind(10000, Inf, NA), right=TRUE) } # remove non-valid index values for(i in 7:13) { image[[i]] &lt;- reclassify(image[[i]], cbind(-Inf, -10000, NA), right=FALSE) image[[i]] &lt;- reclassify(image[[i]], cbind(10000, Inf, NA), right=TRUE) } # set whole stack to NA where one single layer is NA m &lt;- sum(image) image &lt;- mask(image, m) names(image) &lt;- BANDS[-c(length(BANDS)-1:0)] # write out the individual images # message(&quot;writing scene ... &quot;, appendLF = FALSE) # filename.scene &lt;- sub(&quot;complete&quot;, paste0(&quot;r0&quot;, image.row), sub(paste0(&quot;p&quot;, image.path, &quot;_&quot;), paste0(&quot;p&quot;, image.path, &quot;_r0&quot;, image.row, &quot;_&quot;), filename)) # image &lt;- writeRaster(image, filename = filename.scene, overwrite = TRUE, datatype=&quot;INT2S&quot;, options=c(&quot;COMPRESS=NONE&quot;)) # qa &lt;- writeRaster(qa, filename = sub(&quot;[.]tif$&quot;, paste0(&quot;_qa&quot;, image.sensor, &quot;.tif&quot;), filename.scene), overwrite = TRUE, datatype=&quot;INT2S&quot;, options=c(&quot;COMPRESS=NONE&quot;)) # message(&quot;done&quot;) images[[length(images)+1]] &lt;- image qas[[length(qas)+1]] &lt;- qa } # merge the images in the list message(&quot;- merging scenes ... &quot;, appendLF = FALSE) out.image &lt;- do.call(merge, images) out.qa &lt;- do.call(merge, qas) # write it to a file if (!is.null(filename) &amp;&amp; (!file.exists(filename) || overwrite == TRUE)) { message(&quot;writing to file &quot;, filename, &quot; ... &quot;, appendLF = FALSE) out.image &lt;- writeRaster(out.image, filename = filename, overwrite = overwrite, datatype=&quot;INT2S&quot;, options=c(&quot;COMPRESS=NONE&quot;)) names(out.image) &lt;- BANDS[-c(length(BANDS)-1:0)] out.qa &lt;- writeRaster(out.qa, filename = sub(&quot;[.]tif$&quot;, paste0(&quot;_qa&quot;, image.sensor, &quot;.tif&quot;), filename), overwrite = overwrite, datatype=&quot;INT2S&quot;) } } message(&quot;done&quot;) print(out.image) # close the logfile if(!is.null(filename) &amp; log==TRUE) { sink(type=&quot;output&quot;) sink(type=&quot;message&quot;) } # return image invisibly invisible(out.image) } # DO THE WORK --------- ## @knitr execute # be careful, can easily fill the tmp directory! Maybe only for a part of the images and then restart R # extent of Togo + 5km buffer TGO.ext.30 &lt;- extent(spTransform(TGO, utm.30)) + 10000 # go through all scenes (path/row) ... registerDoParallel(.env$numCores-1) #foreach(i=1:length(in.image.list)) %dopar% { # 1) %do% { foreach(i=1:13) %dopar% { # 1) %do% { in.image.dirs &lt;- in.image.list[[i]] name &lt;- unlist(strsplit(names(in.image.list[i]), &quot;[.]&quot;)) path &lt;- name[1] year &lt;- name[2] tile &lt;- name[3] if(path == &quot;p194&quot;) tmp.ext &lt;- TGO.ext.30 else tmp.ext &lt;- TGO.ext out.image.dir &lt;- paste0(OUTPUT.DIR, &quot;/1_images/&quot;, path) if(!dir.exists(out.image.dir)) dir.create(out.image.dir) if(is.na(tile)) { filename &lt;- paste0(out.image.dir, &quot;/&quot;, path, &quot;_&quot;, year, &quot;.tif&quot;) } else { filename &lt;- paste0(out.image.dir, &quot;/&quot;, path, &quot;_&quot;, year, &quot;_&quot;, tile, &quot;.tif&quot;) } message(&quot;Processing &quot;, path, &quot;_&quot;, year) prepare.image(in.image.dirs, ext = tmp.ext, filename = filename, overwrite=FALSE, log=TRUE) } # remove temporary files tmp_dir &lt;- tempdir() files &lt;- list.files(tmp_dir, full.names = T, recursive=T) file.remove(files) # merge logfiles ------------------- for(dir in dir(paste0(OUTPUT.DIR, &quot;/1_images/&quot;), full.names=TRUE)) { path &lt;- basename(dir) system(paste0(&quot;tail -n +1 &quot;, dir, &quot;/*.log &gt; &quot;, dir, &quot;/&quot;, path, &quot;.tmp&quot;)) system(paste0(&quot;rm &quot;, dir, &quot;/*.log&quot;)) system(paste0(&quot;mv &quot;, dir, &quot;/&quot;, path, &quot;.tmp &quot;, dir, &quot;/&quot;, path, &quot;.log&quot;)) } # Reprojection of p194 images from UTM 30 to UTM 31 -------------------------- in.dir &lt;- &quot;../output/1_images/p194&quot; # warp p194 UTM 30 images to UTM 31 foreach(image=dir(in.dir, pattern=&quot;.*[.]tif$&quot;)) %dopar% { image &lt;- paste0(in.dir, &quot;/&quot;, image) image.utm30 &lt;- sub(&quot;[.]tif$&quot;, &quot;utm30.tif&quot;, image) file.rename(image, image.utm30) system(paste(&quot;gdalwarp&quot;, image.utm30, &quot;-t_srs &#39;+proj=utm +zone=31 +datum=WGS84&#39;&quot;, &quot;-tr 30 30&quot;, &quot;-te 147255 1017495 222165 1238265&quot;, image, &quot;-ot &#39;Int16&#39;&quot;, &quot;-overwrite&quot;)) file.remove(image.utm30) } # cloud/shadow mask the images ---------------------------- registerDoParallel(.env$numCores-1) foreach(file= c(dir(&quot;../output/1_images/p192&quot;, pattern=&quot;.*\\\\_[[:digit:]]+\\\\.tif$&quot;, full.names = TRUE), dir(&quot;../output/1_images/p193&quot;, pattern=&quot;.*\\\\_[[:digit:]]+\\\\.tif$&quot;, full.names = TRUE), dir(&quot;../output/1_images/p194&quot;, pattern=&quot;.*\\\\_[[:digit:]]+\\\\.tif$&quot;, full.names = TRUE))) %dopar% { qa &lt;- raster(dir(dirname(file), pattern=gsub(&quot;\\\\_&quot;, &quot;\\\\_&quot;, sub(&quot;\\\\.tif&quot;, &quot;_qa*&quot;, basename(file))), full.names=TRUE)) image &lt;- mask(brick(file), qa %in% c(qa.cloud, qa.shadow, qa.water, qa.ice), maskvalue=TRUE) writeRaster(image, sub(&quot;\\\\.tif&quot;, &quot;_m.tif&quot;, file), overwrite = TRUE, datatype=&quot;INT2S&quot;, options=c(&quot;COMPRESS=NONE&quot;)) } # Thumbnails of each scene ------------------------------- foreach(filename=dir(&quot;../output/1_images&quot;, pattern=&quot;p19.*[.]tif$&quot;, recursive=TRUE, full.names=TRUE)) %dopar% { image &lt;- brick(filename) jpeg(sub(&quot;[.]tif$&quot;, &quot;.jpeg&quot;, filename), width=1350, height=3000) par(plt=c(0,1,0,1)) plot(spTransform(TGO, utm.31)) plotRGB(image, r=6, g=5, b=3, stretch=&quot;lin&quot;, add=TRUE) plot(mask(image[[1]], spTransform(TGO, utm.31), inverse=TRUE), col=&quot;#FFFFFF66&quot;, legend=FALSE, add=TRUE) plot(spTransform(TGO, utm.31), add=TRUE, lwd=3) dev.off() } # Prepare SRTM DEM ---------------------------- # Load 90m SRTM DEM (source: CGIAR), dem.90 &lt;- do.call(merge, lapply(as.list(dir(paste0(INPUT.DIR, &quot;/SRTM/3arcsecond&quot;), pattern=&quot;.*[.]tif$&quot;, full.names=TRUE)), raster)) # Merge 30m SRTM DEM and fill voids with 90m SRTM (source: USGS) dem.30 &lt;- foreach(tile=dir(paste0(INPUT.DIR, &quot;/SRTM/1arcsecond&quot;), pattern=&quot;.*[.]tif$&quot;, full.names=TRUE), .combine=merge, .multicombine=TRUE) %dopar% { dem.30.t &lt;- raster(tile) dem.90.t &lt;- round(projectRaster(dem.90, dem.30.t)) merge(dem.30.t, dem.90.t) } # write it to the disk and reproject to reference Landsat image writeRaster(dem.30, paste0(OUTPUT.DIR, &quot;/1_images/SRTM/SRTM-1arcsec_raw.tif&quot;), datatype=&quot;INT2S&quot;, overwrite=TRUE) system(paste(&quot;gdalwarp&quot;, paste0(OUTPUT.DIR, &quot;/1_images/SRTM/SRTM-1arcsec_raw.tif&quot;), &quot;-t_srs &#39;+proj=utm +zone=31 +datum=WGS84&#39;&quot;, &quot;-tr 30 30&quot;, paste(&quot;-te&quot;, TGO.ext@xmin, TGO.ext@ymin, TGO.ext@xmax, TGO.ext@ymax), paste0(OUTPUT.DIR, &quot;/1_images/SRTM/SRTM-1arcsec.tif&quot;), &quot;-ot &#39;Int16&#39;&quot;, &quot;-co COMPRESS=&#39;LZW&#39;&quot;, &quot;-co INTERLEAVE=&#39;BAND&#39;&quot;, &quot;-overwrite&quot;)) file.remove(paste0(OUTPUT.DIR, &quot;/1_images/SRTM/SRTM-1arcsec_raw.tif&quot;)) system(paste(&quot;gdalinfo -stats&quot;, paste0(OUTPUT.DIR, &quot;/1_images/SRTM/SRTM-1arcsec.tif&quot;))) dem &lt;- raster(paste0(OUTPUT.DIR, &quot;/1_images/SRTM/SRTM-1arcsec.tif&quot;)) jpeg(paste0(OUTPUT.DIR, &quot;/1_images/SRTM/SRTM-1arcsec.jpeg&quot;), width=1350, height=3000) par(plt=c(0,1,0,1)) plot(dem) plot(mask(dem, TGO, inverse=TRUE), col=&quot;#FFFFFF66&quot;, legend=FALSE, add=TRUE) plot(TGO, add=TRUE, lwd=3) dev.off() # Prepare Worldclim v2 Data ---------------------------- foreach(file=dir(paste0(INPUT.DIR, &quot;/Worldclim&quot;), pattern=&quot;.*Togo[.]tif$&quot;)) %dopar% { # wc.raster &lt;- raster(paste0(INPUT.DIR, &quot;/Worldclim/&quot;, file)) # raster.downscale(raster(paste0(INPUT.DIR, &quot;/Worldclim/&quot;, file)), dem.30, ...) system(paste(&quot;gdalwarp&quot;, paste0(INPUT.DIR, &quot;/Worldclim/&quot;, file), &quot;-t_srs &#39;+proj=utm +zone=31 +datum=WGS84&#39;&quot;, &quot;-tr 30 30&quot;, paste(&quot;-te&quot;, TGO.ext@xmin, TGO.ext@ymin, TGO.ext@xmax, TGO.ext@ymax), paste0(OUTPUT.DIR, &quot;/1_images/WCv2/&quot;, file), &quot;-dstnodata -3.4e+38&quot;, &quot;-co COMPRESS=&#39;LZW&#39;&quot;, &quot;-co INTERLEAVE=&#39;BAND&#39;&quot;, &quot;-overwrite&quot;)) system(paste(&quot;gdalinfo -stats&quot;, paste0(OUTPUT.DIR, &quot;/1_images/WCv2/&quot;, file))) } foreach(file=dir(paste0(OUTPUT.DIR, &quot;/1_images/WCv2&quot;), pattern=&quot;.*[.]tif$&quot;)) %dopar% { image &lt;- stack(paste0(OUTPUT.DIR, &quot;/1_images/WCv2/&quot;, file)) type &lt;- unlist(strsplit(file, &quot;_&quot;))[3] if (type == &quot;prec&quot;) { zlim &lt;- c(0,320); col &lt;- rev(topo.colors(255)) } else if (type == &quot;tmin&quot;) { zlim &lt;- c(14.0,27.8); col &lt;- rev(heat.colors(255)) } else if (type == &quot;tmax&quot;) { zlim &lt;- c(24.9,37.5); col &lt;- rev(heat.colors(255)) } else if (type == &quot;tavg&quot;) { zlim &lt;- c(19.7,32.7); col &lt;- rev(heat.colors(255)) } else { zlim &lt;- NA; col &lt;- rev(cm.colors(255)) } foreach(i=1:nlayers(image)) %dopar% { jpeg(paste0(OUTPUT.DIR, &quot;/1_images/WCv2/&quot;, sub(&quot;[.]tif$&quot;, &quot;&quot;, file), &quot;-&quot;, str_pad(i, 2, &quot;left&quot;, 0), &quot;.jpeg&quot;), width=1350, height=3000) plot(image[[i]], col=col, zlim=zlim) plot(mask(image[[i]], TGO, inverse=TRUE), col=&quot;#FFFFFF66&quot;, legend=FALSE, add=TRUE) plot(TGO, add=TRUE, lwd=3) dev.off() } } "],
["00_analyse-FCC.html", "4.3 Analyse surfaces forestiers", " 4.3 Analyse surfaces forestiers Description générale de l’approche / méthodologies "],
["01_create-train-points.html", "", " 4.3.1 Parcelles d’entraînement Description Description de la méthodologie / script FCC/2_create-train-points.R ################################################################################## # NERF_Togo/FCC/2_create-train-points.R: create a set of tree cover training plots # -------------------------------------------------------------------------------- # Bern University of Applied Sciences # Oliver Gardi, &lt;oliver.gardi@bfh.ch&gt; # 20 May 2019 library(&quot;spsurvey&quot;) TRNPTS &lt;- c(paste0(TRNPTS.DIR, &quot;/1_trn10k-s10ndvi_rev/2_assessed&quot;), paste0(TRNPTS.DIR, &quot;/2_trn05k-prob040-060/2_assessed&quot;), paste0(TRNPTS.DIR, &quot;/3_trn1k-prob040-060/2_assessed&quot;)) # Create a grid of observation points over whole Togo, based on Landsat images (30 x 30m) each 480 m ------------------- res &lt;- 480 x.min &lt;- res * extent(TGO)@xmin %/% res x.max &lt;- res * extent(TGO)@xmax %/% res y.min &lt;- res * extent(TGO)@ymin %/% res y.max &lt;- res * extent(TGO)@ymax %/% res frame.points &lt;- SpatialPoints(expand.grid(seq(x.min, x.max, by=res), seq(y.min, y.max, by=res)), proj4string=utm.31)[TGO] # add attributes frame.points$PLOTID &lt;- paste0(str_pad(frame.points@coords[,1], 7, &quot;left&quot;, &quot;0&quot;), &quot;_&quot;, str_pad(frame.points@coords[,2], 7, &quot;left&quot;, &quot;0&quot;)) frame.points$xcoords &lt;- frame.points@coords[,1] frame.points$ycoords &lt;- frame.points@coords[,2] # load 2018 NDVI and mask with water, clouds and shadow ndvi.p192 &lt;- mask(raster(paste0(OUTPUT.DIR, &quot;/1_images/p192/p192_2018.tif&quot;), band=12), raster(paste0(OUTPUT.DIR, &quot;/1_images/p192/p192_2018_qaLC08.tif&quot;)) %in% c(qa.cloud, qa.shadow, qa.water, qa.ice), maskvalue=TRUE) ndvi.p193 &lt;- mask(raster(paste0(OUTPUT.DIR, &quot;/1_images/p193/p193_2018.tif&quot;), band=12), raster(paste0(OUTPUT.DIR, &quot;/1_images/p193/p193_2018_qaLC08.tif&quot;)) %in% c(qa.cloud, qa.shadow, qa.water, qa.ice), maskvalue=TRUE) ndvi.p194 &lt;- mask(raster(paste0(OUTPUT.DIR, &quot;/1_images/p194/p194_2018.tif&quot;), band=12), raster(paste0(OUTPUT.DIR, &quot;/1_images/p194/p194_2018_qaLC08.tif&quot;)) %in% c(qa.cloud, qa.shadow, qa.water, qa.ice), maskvalue=TRUE) ndvi &lt;- merge(ndvi.p192, ndvi.p193, ndvi.p194) rm(ndvi.p192, ndvi.p193, ndvi.p194) # read sampling frame and add NDVI for each plot frame.points$ndvi &lt;- raster::extract(ndvi, frame.points) frame.points$ndvi_c &lt;- cut(frame.points$ndvi, 10, labels=paste0(&quot;s&quot;, 0:9)) writeOGR(frame.points, dsn=paste0(INPUT.DIR, &quot;/Train-Val/201908&quot;), layer=&quot;TGO_frame_480m&quot;, driver=&quot;ESRI Shapefile&quot;, overwrite=TRUE) # Sampling frame for training-points ----------------------- # Initialize random number generator set.seed(1) # design for a spatially balanced sample, drawing 1500 samples of each stratum Dsgn.grt &lt;- list(&quot;s0&quot;=list(panel=c(PanelOne=1500), seltype=&quot;Equal&quot;), &quot;s1&quot;=list(panel=c(PanelOne=1500), seltype=&quot;Equal&quot;), &quot;s2&quot;=list(panel=c(PanelOne=1500), seltype=&quot;Equal&quot;), &quot;s3&quot;=list(panel=c(PanelOne=1500), seltype=&quot;Equal&quot;), &quot;s4&quot;=list(panel=c(PanelOne=1500), seltype=&quot;Equal&quot;), &quot;s5&quot;=list(panel=c(PanelOne=1500), seltype=&quot;Equal&quot;), &quot;s6&quot;=list(panel=c(PanelOne=1500), seltype=&quot;Equal&quot;), &quot;s7&quot;=list(panel=c(PanelOne=1500), seltype=&quot;Equal&quot;), &quot;s8&quot;=list(panel=c(PanelOne=1500), seltype=&quot;Equal&quot;), &quot;s9&quot;=list(panel=c(PanelOne=1500), seltype=&quot;Equal&quot;) ) train.points &lt;- grts(design=Dsgn.grt, # using Dsgn design object DesignID=&#39;train&#39;, # prefix for each point name type.frame=&#39;finite&#39;, # type src.frame=&#39;sp.object&#39;, # sample frame is shapefile sp.object=frame.points, stratum=&quot;ndvi_c&quot; ) summary(train.points) # apply coordinate reference system proj4string(train.points) &lt;- proj4string(frame.points) # shuffle the rows train.points &lt;- train.points[sample(1:nrow(train.points)), ] train.points$SAMPLEID &lt;- paste0(&quot;trn-&quot;, str_pad(string=1:nrow(train.points), width = 4, pad = &quot;0&quot;, side = &quot;left&quot;)) # Convert to plots and add attributes -------------------- landsat.grid &lt;- raster(ndvi) values(landsat.grid) &lt;- 1 # convert to polygons train.plots &lt;- rasterToPolygons(mask(landsat.grid, train.points)) # fetch attributes train.plots@data &lt;- over(train.plots, train.points[, c(&quot;PLOTID&quot;, &quot;SAMPLEID&quot;, &quot;xcoords&quot;, &quot;ycoords&quot;, &quot;ndvi&quot;, &quot;stratum&quot;)]) # add attributes train.plots$ccov &lt;- as.character(NA) train.plots$img_src &lt;- train.plots$img_date &lt;- as.character(NA) train.plots$author &lt;- train.plots$mod_date &lt;- as.character(NA) # # write plots as Shapefile and KML writeOGR(train.plots, dsn=paste0(INPUT.DIR, &quot;/Train-Val/201908/trn10k-s10ndvi/empty&quot;), layer=&quot;COV_parcelles&quot;, driver=&quot;ESRI Shapefile&quot;, overwrite=TRUE) writeKML(train.plots, kmlname=&quot;COV_parcelles&quot;, filename=paste0(INPUT.DIR, &quot;/Train-Val/201908/trn10k-s10ndvi/empty/COV_parcelles.kml&quot;)) # Create 7x7 sample grid ------------------------------------ grid.size &lt;- 7 res &lt;- res(landsat.grid)[1] offset &lt;- c(res/grid.size/2 + (0:(grid.size-1))*res/grid.size) # split the plots for parallel processing subsets &lt;- split(train.plots, f=1:86) train.grids &lt;- foreach(subset=subsets, .combine=rbind, .multicombine=TRUE) %dopar% { grids &lt;- SpatialPointsDataFrame(data.frame(x = 0, y = 0), data=data.frame(PLOTID = 0, SAMPLEID = 0, GRIDPOINT = 0))[-1,] for(p in 1:length(subset)) { plot &lt;- subset[p,] ext &lt;- extent(plot) grids &lt;- bind(grids, SpatialPointsDataFrame(expand.grid(ext@xmin+offset, ext@ymin+offset), data=data.frame(PLOTID = plot$PLOTID, SAMPLEID = plot$SAMPLEID, GRIDPOINT = 1:grid.size^2))) } grids } proj4string(train.grids) &lt;- proj4string(train.plots) train.grids$tree &lt;- as.integer(NA) # 1 or 0 writeOGR(train.grids, dsn=paste0(INPUT.DIR, &quot;/Train-Val/201908/trn10k-s10ndvi/empty&quot;), layer=&quot;COV_parcelles_grid&quot;, driver=&quot;ESRI Shapefile&quot;, overwrite=TRUE) # Divide into 10 subsets and export -------------------- subsets &lt;- split(train.plots, f=1:10) for(i in 1:length(subsets)) { writeOGR(subsets[[i]], dsn=paste0(INPUT.DIR, &quot;/Train-Val/201908/trn10k-s10ndvi/empty&quot;), layer=paste0(&quot;COV_parcelles_&quot;, i), driver=&quot;ESRI Shapefile&quot;, overwrite=TRUE) writeKML(subsets[[i]], kmlname=paste0(&quot;COV_parcelles_&quot;, i) , filename=paste0(INPUT.DIR, &quot;/Train-Val/201908/trn10k-s10ndvi/empty/COV_parcelles_&quot;, i, &quot;.kml&quot;)) subset.grids &lt;- train.grids[train.grids$PLOTID %in% subsets[[i]]$PLOTID,] writeOGR(subset.grids, dsn=paste0(INPUT.DIR, &quot;/Train-Val/201908/trn10k-s10ndvi/empty&quot;), layer=paste0(&quot;COV_parcelles_&quot;, i, &quot;_grid&quot;), driver=&quot;ESRI Shapefile&quot;, overwrite=TRUE) } # read and merge assessed training-plots ------------------------ subsets &lt;- dir(TRNPTS, pattern=&quot;[[:digit:]]\\\\.shp$&quot;, recursive=TRUE, full.names=TRUE) train.plots.in &lt;- lapply(subsets, readOGR) names(train.plots.in) &lt;- list.dirs(TRNPTS, recursive=FALSE) for(i in 1:length(train.plots.in)) { train.plots.in[[i]]$author &lt;- as.character(train.plots.in[[i]]$author) train.plots.in[[i]]$author[!is.na(train.plots.in[[i]]$author)] &lt;- names(train.plots.in[i]) train.plots.in[[i]] &lt;- train.plots.in[[i]][, c(&quot;PLOTID&quot;, &quot;SAMPLEID&quot;, &quot;xcoords&quot;, &quot;ycoords&quot;, &quot;ccov&quot;, &quot;img_date&quot;, &quot;img_src&quot;, &quot;mod_date&quot;, &quot;author&quot;)] proj4string(train.plots.in[[i]]) &lt;- utm.31 } train.plots.in &lt;- do.call(rbind, train.plots.in) subsets.grids &lt;- dir(TRNPTS, pattern=&quot;.*[[:digit:]]+_grid\\\\.shp$&quot;, recursive=TRUE, full.names=TRUE) train.grids.in &lt;- lapply(subsets.grids, readOGR) for(i in 1:length(train.grids.in)) { proj4string(train.grids.in[[i]]) &lt;- utm.31 } train.grids.in &lt;- do.call(rbind, train.grids.in) # recalculate crown-cover and merge train.plots.in &lt;- train.plots.in[, names(train.plots.in) != &quot;ccov&quot;] tmp &lt;- aggregate(list(ccov=train.grids.in$tree), by=list(PLOTID=train.grids.in$PLOTID), FUN=function(x) sum(!is.na(x) &amp; x==1)/sum(!is.na(x))) tmp$ccov[is.nan(tmp$ccov)] &lt;- NA train.plots.in &lt;- merge(train.plots.in, tmp, by=&quot;PLOTID&quot;) # clean training plot data --------------------- # remove plots without ccov train.plots.in &lt;- train.plots.in[!is.na(train.plots.in$ccov),] # remove plots with strange dates year &lt;- as.numeric(sub(&quot;-.*$&quot;, &quot;&quot;, as.character(train.plots.in$img_date))) train.plots.in &lt;- train.plots.in[!is.na(year) &amp; year &gt;= 2000 &amp; year &lt;= 2019, ] # clean grid accordingly train.grids.in &lt;- train.grids.in[train.grids.in$PLOTID %in% unique(train.plots.in$PLOTID), ] # Write clean training plots ----------------- writeOGR(train.plots.in, dsn=TRNPTS.DIR, layer=&quot;COV_parcelles&quot;, driver=&quot;ESRI Shapefile&quot;, overwrite=TRUE) writeOGR(train.grids.in, dsn=TRNPTS.DIR, layer=&quot;COV_parcelles_grid&quot;, driver=&quot;ESRI Shapefile&quot;, overwrite=TRUE) dir.create(paste0(TRNPTS.DIR, &quot;/descr&quot;), showWarnings = FALSE) # Write some descriptive information pdf(paste0(TRNPTS.DIR, &quot;/descr/hist-years.pdf&quot;)) hist(as.numeric(sub(&quot;-.*$&quot;, &quot;&quot;, as.character(train.plots.in$img_date))), breaks=2000:2019) dev.off() pdf(paste0(TRNPTS.DIR, &quot;/descr/hist-ccov.pdf&quot;)) hist(train.plots.in$ccov) dev.off() sink(paste0(INPUT.DIR, &quot;/Train-Val/201908/trn10k-s10ndvi/descr/author-stats.txt&quot;), split=TRUE) table(train.plots.in$author) sink() # # Create additional 500 traininplots for regions of ambiquity (for Ayele) ------------------------ # # # get probability map # prob.map &lt;- raster(paste0(OUTPUT.DIR, &quot;/2_forest-maps/TGO/1_ref-maps/p193/p193_2018_FC30_prob.tif&quot;)) # prob.map[prob.map &lt; 0.4 | prob.map &gt; 0.6] &lt;- NA # prob.map &lt;- mask(prob.map, TGO) # # # sample cells with ambiguity (values 0.4 - 0.6) # amb.plots &lt;- rasterToPolygons(sampleRandom(prob.map, 500, asRaster=TRUE)) # # # fetch attributes # names(amb.plots) &lt;- &quot;NDVI&quot; # amb.plots$PLOTID &lt;- paste0(&quot;add-&quot;, str_pad(string=1:nrow(amb.plots), width = 4, pad = &quot;0&quot;, side = &quot;left&quot;)) # amb.plots$SAMPLEID &lt;- amb.plots$PLOTID # amb.plots$xcoords &lt;- amb.plots$ycoords &lt;- amb.plots$stratum &lt;- as.numeric(NA) # # # add attributes # amb.plots$ccov &lt;- as.character(NA) # amb.plots$img_src &lt;- amb.plots$img_date &lt;- as.character(NA) # amb.plots$author &lt;- amb.plots$mod_date &lt;- as.character(NA) # # # # write plots as Shapefile and KML # writeOGR(amb.plots, dsn=paste0(INPUT.DIR, &quot;/Train-Val/201908/trn10k-s10ndvi/add&quot;), layer=&quot;COV_parcelles_add&quot;, driver=&quot;ESRI Shapefile&quot;, overwrite=TRUE) # writeKML(amb.plots, kmlname=&quot;COV_parcelles&quot;, filename=paste0(INPUT.DIR, &quot;/Train-Val/201908/trn10k-s10ndvi/add/COV_parcelles_add.kml&quot;)) # # # Create sample grid # # grid.size &lt;- 7 # res &lt;- res(amb.map)[1] # offset &lt;- c(res/grid.size/2 + (0:(grid.size-1))*res/grid.size) # # # split the plots for parallel processing # subsets &lt;- split(amb.plots, f=1:10) # # registerDoParallel(.env$numCores-1) # amb.grids &lt;- foreach(subset=subsets, .combine=rbind, .multicombine=TRUE) %dopar% { # grids &lt;- SpatialPointsDataFrame(data.frame(x = 0, y = 0), data=data.frame(PLOTID = 0, SAMPLEID = 0, GRIDPOINT = 0))[-1,] # for(p in 1:length(subset)) { # plot &lt;- subset[p,] # ext &lt;- extent(plot) # grids &lt;- bind(grids, SpatialPointsDataFrame(expand.grid(ext@xmin+offset, ext@ymin+offset), data=data.frame(PLOTID = plot$PLOTID, SAMPLEID = plot$SAMPLEID, GRIDPOINT = 1:grid.size^2))) # } # grids # } # # proj4string(amb.grids) &lt;- proj4string(amb.plots) # amb.grids$tree &lt;- as.integer(NA) # 1 or 0 # # writeOGR(amb.grids, dsn=paste0(INPUT.DIR, &quot;/Train-Val/201908/trn10k-s10ndvi/add&quot;), layer=&quot;COV_parcelles_add_grid&quot;, driver=&quot;ESRI Shapefile&quot;, overwrite=TRUE) # # Create additional 1000 traininplots for regions of ambiquity (for Etse) ------------------------------ # # # get probability map # prob.map &lt;- merge( # raster(paste0(REFMAPS.DIR, &quot;/p193_2018_FC30_R_prob.tif&quot;)), # raster(paste0(REFMAPS.DIR, &quot;/p192_2018_FC30_R_prob.tif&quot;)), # raster(paste0(REFMAPS.DIR, &quot;/p194_2018_FC30_R_prob.tif&quot;)) # ) # # prob.map &lt;- mask(crop(prob.map, TGO), TGO) # prob.map[prob.map &lt; 0.4 | prob.map &gt; 0.6] &lt;- NA # # # # sample cells with ambiguity (values 0.4 - 0.6) # amb.plots &lt;- rasterToPolygons(sampleRandom(prob.map, 1000, asRaster=TRUE)) # # # fetch attributes # names(amb.plots) &lt;- &quot;NDVI&quot; # amb.plots$PLOTID &lt;- paste0(&quot;add2-&quot;, str_pad(string=1:nrow(amb.plots), width = 4, pad = &quot;0&quot;, side = &quot;left&quot;)) # amb.plots$SAMPLEID &lt;- amb.plots$PLOTID # amb.plots$xcoords &lt;- amb.plots$ycoords &lt;- amb.plots$stratum &lt;- as.numeric(NA) # # # add attributes # amb.plots$ccov &lt;- as.character(NA) # amb.plots$img_src &lt;- amb.plots$img_date &lt;- as.character(NA) # amb.plots$author &lt;- amb.plots$mod_date &lt;- as.character(NA) # # # # write plots as Shapefile and KML # writeOGR(amb.plots, dsn=paste0(REFMAPS.DIR, &quot;/3_trn1k-prob040-060/1_empty&quot;), layer=&quot;COV_parcelles_add2&quot;, driver=&quot;ESRI Shapefile&quot;, overwrite=TRUE) # writeKML(amb.plots, kmlname=&quot;COV_parcelles&quot;, filename=paste0(REFMAPS.DIR, &quot;/3_trn1k-prob040-060/1_empty/COV_parcelles_add2.kml&quot;)) # # # Create sample grid # # grid.size &lt;- 7 # res &lt;- res(prob.map)[1] # offset &lt;- c(res/grid.size/2 + (0:(grid.size-1))*res/grid.size) # # # split the plots for parallel processing # subsets &lt;- split(amb.plots, f=1:10) # # registerDoParallel(.env$numCores-1) # amb.grids &lt;- foreach(subset=subsets, .combine=rbind, .multicombine=TRUE) %dopar% { # grids &lt;- SpatialPointsDataFrame(data.frame(x = 0, y = 0), data=data.frame(PLOTID = 0, SAMPLEID = 0, GRIDPOINT = 0))[-1,] # for(p in 1:length(subset)) { # plot &lt;- subset[p,] # ext &lt;- extent(plot) # grids &lt;- bind(grids, SpatialPointsDataFrame(expand.grid(ext@xmin+offset, ext@ymin+offset), data=data.frame(PLOTID = plot$PLOTID, SAMPLEID = plot$SAMPLEID, GRIDPOINT = 1:grid.size^2))) # } # grids # } # # proj4string(amb.grids) &lt;- proj4string(amb.plots) # amb.grids$tree &lt;- as.integer(NA) # 1 or 0 # # writeOGR(amb.grids, dsn=paste0(REFMAPS.DIR, &quot;/3_trn1k-prob040-060/1_empty&quot;), layer=&quot;COV_parcelles_add2_grid&quot;, driver=&quot;ESRI Shapefile&quot;, overwrite=TRUE) "],
["02_create-fc-maps.html", "", " 4.3.2 Calibration et classification Description Description de la méthodologie / script FCC/3_create-fc-maps.R ############################################################################ # NERF_Togo/FCC/3_create-fc-maps.R: create forest cover maps # -------------------------------------------------------------------------- # Bern University of Applied Sciences # Oliver Gardi, &lt;oliver.gardi@bfh.ch&gt; # 10 October 2019 ### DEFINITIONS ############################################################ # Default parameters ------------------------------------------------------- COV.FC &lt;- 10 SAMPLE.DIST &lt;- 1 N.PIXELS &lt;- NA # Number of non-NA cells (will be determined later) SAMPLE.RATIO &lt;- 0.0025 # Share of non-NA cells to sample CAL.RATIO &lt;- 0.75 # Use same amount of ref-points from cal.map as from ref.map / train.points PREDICTORS &lt;- c(&quot;B&quot;, &quot;G&quot;, &quot;R&quot;, &quot;NIR&quot;, &quot;SWIR1&quot;, &quot;SWIR2&quot;, &quot;nbr&quot;, &quot;ndmi&quot;, &quot;ndvi&quot;, &quot;evi&quot;, &quot;BIO1&quot;, &quot;BIO4&quot;, &quot;BIO12&quot;, &quot;BIO15&quot;) # &quot;savi&quot;, &quot;nbr2&quot;, &quot;msavi&quot;, &quot;x&quot;, &quot;y&quot; SEED &lt;- 20191114 # Function for loading an image -------------------------------------------- load.image &lt;- function(filename) { image &lt;- brick(paste0(IMAGES.DIR, filename)) names(image) &lt;- BANDS return(image) } # Function for sampling map ------------------------------------------------ sample.map &lt;- function(map, n) { tmp.src &lt;- tempfile(pattern = &quot;&quot;, fileext = &quot;.tif&quot;) # temporary file for masked reference map tmp.dst1 &lt;- tempfile(pattern = &quot;&quot;, fileext = &quot;.tif&quot;) # temporary file for forest edge tmp.dst3 &lt;- tempfile(pattern = &quot;&quot;, fileext = &quot;.tif&quot;) # temporary file for map &lt;- writeRaster(map, tmp.src) # write it to the disk # draw sample points around forest edge system(paste(&quot;gdal_proximity.py&quot;, tmp.src, tmp.dst1, # forest and 3 pixel non-forest edge (distance to nearest pixel of value 1) &quot;-values 1 -use_input_nodata YES -maxdist &quot;, SAMPLE.DIST, &quot; -fixed-buf-val 3&quot;)) dst1 &lt;- raster(tmp.dst1) NAvalue(dst1) &lt;- 65535 cat(&quot; &quot;) system(paste(&quot;gdal_proximity.py&quot;, tmp.src, tmp.dst3, # non-forest and 3 pixel forest edge (distance to nearest pixel of value 3) &quot;-values 3 -use_input_nodata YES -maxdist &quot;, SAMPLE.DIST, &quot; -fixed-buf-val 1&quot;)) dst3 &lt;- raster(tmp.dst3) NAvalue(dst3) &lt;- 65535 map &lt;- mask(map, dst1) map &lt;- mask(map, dst3) unlink(c(tmp.src, tmp.dst1, tmp.dst3)) # delete temporary files n.classes &lt;- length(unique(map)) # number of classes (should be two) cat(paste0(&quot; -Sampling map (n=&quot;, n.classes, &quot;*&quot;, round(n/n.classes), &quot;) ... &quot;)) sample.pts &lt;- sampleStratified(map, round(n/n.classes), sp=TRUE)[,-1] # stratified sampling (same number of samples for each class) names(sample.pts) &lt;- &quot;CLASS&quot; cat(&quot;done\\n&quot;) return(sample.pts) } # Function for classifying an image ---------------------------------------- classify.image &lt;- function(image, filename, bioclim=NULL, train.pts=NULL, ref.map=NULL, n.ref.map=NULL, cal.map=NULL, n.cal.map=NULL, mask=NULL, preds=NULL, type=&quot;classification&quot;, crossval=FALSE, prob=FALSE, n.cores=8) { txtfile &lt;- paste0(sub(&quot;[.]tif$&quot;, &quot;&quot;, filename), &quot;.txt&quot;) cat(&quot;-- Image classification: &quot;, basename(filename), &quot;/&quot;, date(), &quot; --\\n&quot;, file=txtfile) if(!is.null(train.pts)) { cat(&quot; -Loading training points ... &quot;) train.pts &lt;- train.pts[,1] # only use first column in the attribute table (class) names(train.pts) &lt;- &quot;CLASS&quot; set_ReplCRS_warn(FALSE) proj4string(train.pts) &lt;- proj4string(image) cat(&quot;done\\n&quot;) cat(&quot;Training points:&quot;, nrow(train.pts), &quot;\\n&quot;, file=txtfile, append=TRUE) } # add training data from ref.map if provided if(!is.null(ref.map)) { cat(paste0(&quot; -Masking / buffering reference map ... \\n&quot;)) ref.map &lt;- mask(crop(ref.map, image[[1]]), crop(image[[1]], ref.map)) # crop/mask ref.map with image if(!is.null(mask)) ref.map &lt;- mask(ref.map, mask) # mask with additional mask, if provided if(!is.null(cal.map)) { tmp &lt;- extend(crop(cal.map, ref.map), ref.map) # cut out the piece of the calibration map that overlaps ref map and extend to refmap ref.map &lt;- mask(ref.map, tmp, inverse=TRUE) } cat(&quot; &quot;) ref.pts &lt;- sample.map(ref.map, n.ref.map) cat(&quot;Ref-map points: &quot;, nrow(ref.pts), &quot;/&quot;, ref.map@file@name, &quot;/&quot;, SAMPLE.DIST, &quot;px\\n&quot;, file=txtfile, append=TRUE) if(is.null(train.pts)) { train.pts &lt;- ref.pts # use it as training points or add to existing training points } else { train.pts &lt;- rbind(train.pts, ref.pts) } } if(!is.null(cal.map)) { cat(paste0(&quot; -Masking / buffering calibration map ... \\n&quot;)) cal.map &lt;- mask(crop(cal.map, image[[1]]), crop(image[[1]], cal.map)) # crop/mask ref.map with image if(!is.null(mask)) cal.map &lt;- mask(cal.map, mask) # mask with additional mask, if provided cat(&quot; &quot;) cal.pts &lt;- sample.map(cal.map, n.cal.map) cat(&quot;Cal-map points: &quot;, nrow(cal.pts), &quot;from&quot;, cal.map@file@name, &quot;/&quot;, SAMPLE.DIST, &quot;px\\n&quot;, file=txtfile, append=TRUE) if(is.null(train.pts)) { train.pts &lt;- cal.pts # use it as training points or add to existing training points } else { train.pts &lt;- rbind(train.pts, cal.pts) } } cat(&quot;Total points: &quot;, nrow(train.pts), &quot;\\n&quot;, file=txtfile, append=TRUE) # extract spectral values if(is.null(preds)) { preds &lt;- names(image) if(!is.null(bioclim)) preds &lt;- c(preds, names(bioclim)) } cat(&quot; -Extracting pixel values for bands:&quot;, preds, &quot;... &quot;) train.pts &lt;- raster::extract(image, train.pts, sp=TRUE) if(!is.null(bioclim)) train.pts &lt;- raster::extract(bioclim, train.pts, sp=TRUE) train.dat &lt;- na.omit(train.pts@data)[, c(&quot;CLASS&quot;, preds)] if(type==&quot;classification&quot;) train.dat[,1] &lt;- as.factor(train.dat[,1]) cat(&quot;done\\n&quot;) # calibrate RandomForest classifier cat(&quot; -Calibrating RandomForest ... &quot;) sink(txtfile, append=TRUE) if(crossval) { map.model.cv &lt;- train(y = train.dat[,1], x = train.dat[,-1], method = &quot;rf&quot;, importance = TRUE, trControl = trainControl( method = &quot;repeatedcv&quot;, number = 10, repeats = 3)) print(map.model.cv) map.model &lt;- map.model.cv$finalModel print(map.model) cat(&quot;\\n&quot;) print(varImp(map.model, scale=FALSE)) } else { map.model &lt;- randomForest(y=train.dat[,1], x=train.dat[,-1], importance=TRUE) # , do.trace=100) # Parallelization of RandomForest: confusion, err.rate, mse and rsq will be NULL # https://stackoverflow.com/questions/14106010/parallel-execution-of-random-forest-in-r # map.model &lt;- foreach(ntree=rep(100, 5), .combine=randomForest::combine, .multicombine=TRUE, .packages=&#39;randomForest&#39;) %dopar% { # randomForest(x=ref.pts[,!(names(ref.pts) == &quot;CLASS&quot;)], y=ref.pts$CLASS, importance=TRUE, ntree=ntree) # print(map.model) cat(&quot;\\n&quot;) print(varImp(map.model)) } sink() if(type==&quot;treecover&quot;) { cat(&quot;R2:&quot;, round(map.model$rsq[500], 2), &quot;RMSE:&quot;, round(sqrt(map.model$mse[500]), 2), &quot;\\n&quot;) } else { cat(&quot;OOB error rate:&quot;, round(map.model$err.rate[500,1], 2), &quot;\\n&quot;) } # write model results dir.create(dirname(filename), recursive=TRUE, showWarnings=FALSE) # save RandomForest Model (too large) # save(map.model, file=paste0(sub(&quot;[.]tif$&quot;, &quot;&quot;, filename), &quot;r_rf.RData&quot;)) # classify image cat(&quot; -Creating map ... &quot;) if(!is.null(bioclim)) image &lt;- stack(image, crop(bioclim, image)) beginCluster(n=n.cores) map &lt;- clusterR(image, predict, args=list(model=map.model)) endCluster() # save map of classified image cat(&quot;writing map ... &quot;) if(type==&quot;treecover&quot;) map &lt;- floor(map*100) map &lt;- writeRaster(map, filename=filename, format=&quot;GTiff&quot;, datatype=&quot;INT2U&quot;, overwrite=TRUE) cat(&quot;done\\n&quot;) # create probability map if(prob==TRUE) { cat(&quot; -Creating probability map ... &quot;) beginCluster(n=n.cores) prob.map &lt;- clusterR(image, predict, args=list(model=map.model, type=&quot;prob&quot;)) endCluster() cat(&quot;writing map ... &quot;) writeRaster(prob.map, filename=sub(&quot;\\\\.tif&quot;, &quot;_prob.tif&quot;, filename), format=&quot;GTiff&quot;, overwrite=TRUE) cat(&quot;done\\n&quot;) } else { prob.map &lt;- NULL } cat(&quot;-- Done: &quot;, basename(filename), &quot;/&quot;, date(), &quot; --\\n&quot;, file=txtfile, append=TRUE) invisible(list( &quot;model&quot; = map.model, &quot;map&quot; = map, &quot;prob&quot; = prob.map )) } ### DO THE WORK ########################################################### # create directories if the don&#39;t exist dir.create(FCC.REF.DIR, recursive=TRUE, showWarnings=FALSE) dir.create(FCC.RAW.DIR, recursive=TRUE, showWarnings=FALSE) # load 2018 images -------------------------------------------------------- ref.p192 &lt;- brick(paste0(IMAGES.DIR, &quot;/p192/p192_2018_m.tif&quot;)) ref.p193 &lt;- brick(paste0(IMAGES.DIR, &quot;/p193/p193_2018_m.tif&quot;)) ref.p194 &lt;- brick(paste0(IMAGES.DIR, &quot;/p194/p194_2018_m.tif&quot;)) names(ref.p192) &lt;- names(ref.p193) &lt;- names(ref.p194) &lt;- BANDS ref.images &lt;- list(p192=ref.p192, p193=ref.p193, p194=ref.p194) N.PIXELS &lt;- list(p192 = ncell(ref.p192[[&quot;B&quot;]]) - summary(ref.p192)[&quot;NA&#39;s&quot;,&quot;B&quot;], p193 = ncell(ref.p193[[&quot;B&quot;]]) - summary(ref.p193)[&quot;NA&#39;s&quot;,&quot;B&quot;], p194 = ncell(ref.p194[[&quot;B&quot;]]) - summary(ref.p194)[&quot;NA&#39;s&quot;,&quot;B&quot;]) bioclim.p192 &lt;- brick(paste0(IMAGES.DIR, &quot;/p192/p192_bioclim.tif&quot;)) bioclim.p193 &lt;- brick(paste0(IMAGES.DIR, &quot;/p193/p193_bioclim.tif&quot;)) bioclim.p194 &lt;- brick(paste0(IMAGES.DIR, &quot;/p194/p194_bioclim.tif&quot;)) names(bioclim.p192) &lt;- names(bioclim.p193) &lt;- names(bioclim.p194) &lt;- BIOCLIM bioclim &lt;- list(p192=bioclim.p192, p193=bioclim.p193, p194=bioclim.p194) # load training plots ------------------------------------------------------ train.plots &lt;- readOGR(paste0(TRNPTS.DIR, &quot;/COV_parcelles.shp&quot;)) train.plots &lt;- train.plots[!is.na(train.plots$ccov), c(&quot;PLOTID&quot;, &quot;ccov&quot;, &quot;img_date&quot;, &quot;author&quot;)] train.plots$author &lt;- as.factor(sub(&quot;^.*\\\\/\\\\/&quot;, &quot;&quot;, train.plots$author)) # convert plot polygons to spatial points (centroids) train.points &lt;- SpatialPointsDataFrame(gCentroid(train.plots, byid=TRUE), data.frame(author=train.plots$author, ccov=train.plots$ccov, img_date=as.Date(train.plots$img_date))) # select only those with image date between 1.1.2017 and 31.12.2019 train.points &lt;- train.points[!is.na(train.points$img_date) &amp; train.points$img_date &gt; as.Date(&quot;2017-01-01&quot;) &amp; train.points$img_date &lt;= as.Date(&quot;2019-12-31&quot;), ] pdf(paste0(FCC.REF.DIR, &quot;/training-pts_2018.pdf&quot;)) plot(train.points) dev.off() # extract image values for train.points registerDoParallel(.env$numCores-1) train.points &lt;- foreach(i=1:length(ref.images), .combine=rbind) %dopar% { pts &lt;- raster::extract(ref.images[[i]], train.points, sp=TRUE) pts &lt;- raster::extract(bioclim[[i]], pts, sp=TRUE) pts$image &lt;- names(ref.images[i]) pts[, c(&quot;author&quot;, &quot;image&quot;, &quot;ccov&quot;, BANDS, BIOCLIM)] } # remove rows with NA&#39;s train.points &lt;- train.points[!is.na(rowSums(train.points@data[,-(1:2)])), ] # discard points from authors that add confusion # train.points &lt;- train.points[!train.points$author %in% c(&quot;6_Eric_AGBESSI&quot;, &quot;2_Mamalnassoh_ABIGUIME&quot;, &quot;7_Aklasson_TOLEBA&quot;, &quot;1_Ditorgue_BAKABIMA&quot;, &quot;8_Yawo_KONKO&quot;), ] # for(author in unique(train.points$author[!train.points$author %in% discard])) { # print(author) # map.model &lt;- randomForest(y=train.points@data[!train.points$author %in% c(author, discard),&quot;ccov&quot;], x=train.points@data[!train.points$author %in% c(author, discard),-(1:3)]) # print(map.model) # } train.points@data &lt;- cbind(train.points@data[,c(&quot;image&quot;, &quot;ccov&quot;)], F10=cut(train.points$ccov, breaks=c(0.0,0.1,1.0), labels=c(3, 1), right=FALSE, include.lowest=TRUE), F30=cut(train.points$ccov, breaks=c(0.0,0.3,1.0), labels=c(3, 1), right=FALSE, include.lowest=TRUE), train.points@data[,c(BANDS, BIOCLIM)]) # Parameter selection ----------------------------------------------------- cov.varsel &lt;- rfe(y=train.points@data[train.points$image==&quot;p193&quot;, &quot;ccov&quot;], x=train.points@data[train.points$image==&quot;p193&quot;, PREDICTORS], sizes = c(4, 6, 8, 10), rfeControl=rfeControl( functions=rfFuncs, # use RandomForest method = &quot;repeatedcv&quot;, # repeated cross-validation number = 10, # 10-fold repeats = 3)) # 3 repeats print(cov.varsel) predictors(cov.varsel) plot(cov.varsel, type=c(&quot;g&quot;, &quot;o&quot;)) # 2018 reference tree cover map (TODO) ------------------------------------- # Tree cover map for 2018 for p193 set.seed(SEED) p193.2018.cov &lt;- classify.image(image = load.image(&quot;/p193/p193_2018_m.tif&quot;), bioclim = bioclim[[&quot;p193&quot;]], filename = paste0(FCC.REF.DIR, &quot;/p193_2018_COV_R.tif&quot;), train.pts = train.points[train.points$image == &quot;p193&quot;, &quot;ccov&quot;], preds = PREDICTORS, type = &quot;treecover&quot;, crossval = TRUE, n.cores = 32) # plot observed vs. predicted pdf(paste0(FCC.REF.DIR, &quot;/p193_2018_COV_R.pdf&quot;)) plot(p193.2018.cov[[&quot;model&quot;]]$y, p193.2018.cov[[&quot;model&quot;]]$predicted, xlim=c(0,1), ylim=c(0,1), main=&quot;Couverture houppier p193&quot;, xlab=&quot;Observation&quot;, ylab=&quot;Prédiction&quot;) abline(0,1) dev.off() # 2018 reference forest cover maps ----------------------------------------- # Forest cover map for 2018 for p193 set.seed(SEED) classify.image(image = load.image(&quot;/p193/p193_2018_m.tif&quot;), bioclim = bioclim[[&quot;p193&quot;]], filename = paste0(FCC.REF.DIR, &quot;/FC&quot;, COV.FC, &quot;/p193_2018_FC&quot;, COV.FC, &quot;_R.tif&quot;), train.pts = train.points[train.points$image == &quot;p193&quot;, paste0(&quot;F&quot;, COV.FC)], preds = PREDICTORS, prob = TRUE, crossval = TRUE, n.cores = 32) # Forest cover maps 2018 for p192 and p194, calibrating with p193 set.seed(SEED) registerDoParallel(.env$numCores-1) foreach(path=c(&quot;p192&quot;, &quot;p194&quot;)) %dopar% { train.pts &lt;- train.points[train.points$image == path, paste0(&quot;F&quot;, COV.FC)] classify.image(image = load.image(paste0(&quot;/&quot;, path, &quot;/&quot;, path, &quot;_2018_m.tif&quot;)), bioclim = bioclim[[path]], filename = paste0(FCC.REF.DIR, &quot;/FC&quot;, COV.FC, &quot;/&quot;, path, &quot;_2018_FC&quot;, COV.FC, &quot;_R.tif&quot;), train.pts = train.pts, cal.map = raster(paste0(FCC.REF.DIR, &quot;/FC&quot;, COV.FC, &quot;/p193_2018_FC&quot;, COV.FC, &quot;_R.tif&quot;)), n.cal.map = max(2000, nrow(train.pts)/CAL.RATIO), preds = PREDICTORS, mask = TGO, prob = TRUE, n.cores = 32) } # 2003 reference forest cover maps ----------------------------------------- # Forest cover map for 2003 for p193 set.seed(SEED) classify.image(image = load.image(&quot;/p193/p193_2003_m.tif&quot;), bioclim = bioclim[[&quot;p193&quot;]], filename = paste0(FCC.REF.DIR, &quot;/FC&quot;, COV.FC, &quot;/p193_2003_FC&quot;, COV.FC, &quot;_R.tif&quot;), ref.map = raster(paste0(FCC.REF.DIR, &quot;/FC&quot;, COV.FC, &quot;/p193_2018_FC&quot;, COV.FC, &quot;_R.tif&quot;)), n.ref.map = 2 * SAMPLE.RATIO * N.PIXELS[[&quot;p193&quot;]], preds = PREDICTORS, mask = TGO, n.cores = 32) # # Recalibrate forest cover map for 2018, based on 2003 for p193 # # set.seed(SEED) # classify.image(image = load.image(&quot;/p193/p193_2018.tif&quot;), # bioclim = bioclim[[&quot;p193&quot;]], # filename = paste0(REFMAPS.DIR, &quot;/p193_2018_FC&quot;, COV.FC, &quot;.tif&quot;), # ref.map = raster(paste0(REFMAPS.DIR, &quot;/p193_2003_FC&quot;, COV.FC, &quot;_R.tif&quot;)), # n.ref.map = SAMPLE.RATIO * N.PIXELS[[&quot;p193&quot;]], # preds = PREDICTORS, # mask = TGO, # n.cores = 32) # Forest cover maps 2003 for p192 and p194, calibrating with p193 set.seed(SEED) registerDoParallel(.env$numCores-1) foreach(path=c(&quot;p192&quot;, &quot;p194&quot;)) %dopar% { classify.image(image = load.image(paste0(&quot;/&quot;, path, &quot;/&quot;, path, &quot;_2003_m.tif&quot;)), bioclim = bioclim[[path]], filename = paste0(FCC.REF.DIR, &quot;/FC&quot;, COV.FC, &quot;/&quot;, path, &quot;_2003_FC&quot;, COV.FC, &quot;_R.tif&quot;), ref.map = raster(paste0(FCC.REF.DIR, &quot;/FC&quot;, COV.FC, &quot;/&quot;, path, &quot;_2018_FC&quot;, COV.FC, &quot;_R.tif&quot;)), n.ref.map = 2 * (1 - CAL.RATIO) * SAMPLE.RATIO * N.PIXELS[[path]], cal.map = raster(paste0(FCC.REF.DIR, &quot;/FC&quot;, COV.FC, &quot;/p193_2003_FC&quot;, COV.FC, &quot;_R.tif&quot;)), n.cal.map = 2 * CAL.RATIO * SAMPLE.RATIO * N.PIXELS[[path]], preds = PREDICTORS, mask = TGO, n.cores = 32) } # Forest cover maps for all dates ------------------------------------------ # Forest cover maps for p193 set.seed(SEED) registerDoParallel(.env$numCores-1) foreach(file=dir(paste0(IMAGES.DIR, &quot;/p193&quot;), pattern=&quot;\\\\_[[:digit:]]+\\\\_m\\\\.tif&quot;)) %dopar% { # foreach(file=c(&quot;p193_2017_m.tif&quot;, &quot;p193_2019_m.tif&quot;)) %dopar% { classify.image(image = load.image(paste0(&quot;/p193/&quot;, file)), bioclim = bioclim[[&quot;p193&quot;]], filename = paste0(FCC.RAW.DIR, &quot;/FC&quot;, COV.FC, &quot;/p193/&quot;, sub(&quot;\\\\_m\\\\.tif$&quot;, paste0(&quot;_F&quot;, COV.FC, &quot;r.tif&quot;), file)), ref.map = raster(paste0(FCC.REF.DIR, &quot;/FC&quot;, COV.FC, &quot;/p193_2003_FC&quot;, COV.FC, &quot;_R.tif&quot;)), n.ref.map = SAMPLE.RATIO * N.PIXELS[[&quot;p193&quot;]], preds = PREDICTORS, mask = TGO, n.cores = 6) # n.cores = 32) } # merge the two p193_1990 tiles merge(raster(paste0(FCC.RAW.DIR, &quot;/FC&quot;, COV.FC, &quot;/p193/p193_1990_1_F&quot;, COV.FC, &quot;r.tif&quot;)), raster(paste0(FCC.RAW.DIR, &quot;/FC&quot;, COV.FC, &quot;/p193/p193_1990_2_F&quot;, COV.FC, &quot;r.tif&quot;)), filename=paste0(FCC.RAW.DIR, &quot;/FC&quot;, COV.FC, &quot;/p193/p193_1990_F&quot;, COV.FC, &quot;r.tif&quot;), format=&quot;GTiff&quot;, datatype=&quot;INT2U&quot;, overwrite=TRUE) # Forest cover maps for p192 and p194 using p193 maps for calibration set.seed(SEED) registerDoParallel(.env$numCores-1) # foreach(file=c(dir(paste0(IMAGES.DIR, &quot;/p192&quot;), pattern=&quot;\\\\_[[:digit:]]+\\\\_m\\\\.tif&quot;), # dir(paste0(IMAGES.DIR, &quot;/p194&quot;), pattern=&quot;\\\\_[[:digit:]]+\\\\_m\\\\.tif&quot;))) %dopar% { foreach(file=c(&quot;p194_1997_m.tif&quot;, &quot;p194_2007_m.tif&quot;, &quot;p194_2015_m.tif&quot;, &quot;p194_2018_m.tif&quot;)) %dopar% { path &lt;- sub(&quot;\\\\_.*&quot;, &quot;&quot;, file) if(file.exists(paste0(FCC.RAW.DIR, &quot;/FC&quot;, COV.FC, &quot;/p193/&quot;, sub(&quot;\\\\_m\\\\.tif$&quot;, paste0(&quot;_F&quot;, COV.FC, &quot;r.tif&quot;), sub(path, &quot;p193&quot;, file))))) { cal.map &lt;- raster(paste0(FCC.RAW.DIR, &quot;/FC&quot;, COV.FC, &quot;/p193/&quot;, sub(&quot;\\\\_m\\\\.tif$&quot;, paste0(&quot;_F&quot;, COV.FC, &quot;r.tif&quot;), sub(path, &quot;p193&quot;, file)))) n.cal.map &lt;- CAL.RATIO * SAMPLE.RATIO * N.PIXELS[[path]] } else { cal.map &lt;- NULL n.cal.map &lt;- NULL } classify.image(image = load.image(paste0(&quot;/&quot;, path, &quot;/&quot;, file)), bioclim = bioclim[[path]], filename = paste0(FCC.RAW.DIR, &quot;/FC&quot;, COV.FC, &quot;/&quot;, path, &quot;/&quot;, sub(&quot;\\\\_m\\\\.tif$&quot;, paste0(&quot;_F&quot;, COV.FC, &quot;r.tif&quot;), file)), ref.map = raster(paste0(FCC.REF.DIR, &quot;/FC&quot;, COV.FC, &quot;/&quot;, path, &quot;_2003_FC&quot;, COV.FC, &quot;_R.tif&quot;)), n.ref.map = (1 - CAL.RATIO) * SAMPLE.RATIO * N.PIXELS[[path]], cal.map = cal.map, n.cal.map = n.cal.map, preds = PREDICTORS, mask = TGO, n.cores = 16) # n.cores = 6) } # Merging key date maps --------------------------------------------------- for(year in VAL.YEARS) { merge(mask(crop(brick(paste0(FCC.RAW.DIR, &quot;/FC&quot;, COV.FC, &quot;/p193/p193_&quot;, year, &quot;_F&quot;, COV.FC, &quot;r.tif&quot;)),TGO), TGO), mask(crop(brick(paste0(FCC.RAW.DIR, &quot;/FC&quot;, COV.FC, &quot;/p192/p192_&quot;, year, &quot;_F&quot;, COV.FC, &quot;r.tif&quot;)),TGO), TGO), mask(crop(brick(paste0(FCC.RAW.DIR, &quot;/FC&quot;, COV.FC, &quot;/p194/p194_&quot;, year, &quot;_F&quot;, COV.FC, &quot;r.tif&quot;)),TGO), TGO), filename=paste0(FCC.RAW.DIR, &quot;/FC&quot;, COV.FC, &quot;/TGO/TGO_&quot;, year, &quot;_F&quot;, COV.FC, &quot;r.tif&quot;), overwrite=TRUE) } # and reference maps for(map in c(&quot;2018_FC10_R&quot;, &quot;2018_FC10_R_prob&quot;,&quot;2003_FC10_R&quot;)) { # for(map in c(&quot;2018_FC30_R&quot;, &quot;2018_FC30_R_prob&quot;,&quot;2003_FC30_R&quot;)) { merge(mask(crop(brick(paste0(FCC.REF.DIR, &quot;/FC&quot;, COV.FC, &quot;/p193_&quot;, map, &quot;.tif&quot;)),TGO), TGO), mask(crop(brick(paste0(FCC.REF.DIR, &quot;/FC&quot;, COV.FC, &quot;/p192_&quot;, map, &quot;.tif&quot;)),TGO), TGO), mask(crop(brick(paste0(FCC.REF.DIR, &quot;/FC&quot;, COV.FC, &quot;/p194_&quot;, map, &quot;.tif&quot;)),TGO), TGO), filename=paste0(FCC.REF.DIR, &quot;/FC&quot;, COV.FC, &quot;/TGO_&quot;, map, &quot;.tif&quot;), overwrite=TRUE) } "],
["03_clean-fc-maps.html", "", " 4.3.3 Nettoyage des cartes brutes Description Description de la méthodologie / script FCC/4_clean-fc-maps.R ########################################################################## # NERF_Togo/FCC/4_clean-fc-maps_orig.R: clean time-serie of raw forest cover maps # ------------------------------------------------------------------------ # Bern University of Applied Sciences # Oliver Gardi, &lt;oliver.gardi@bfh.ch&gt; # 20 May 2019 # Default parameters ------------------------------------------------------- COV.FC &lt;- 30 # Function for temporal cleaning of path ------------------------------------ clean.temporal &lt;- function(path) { # Preparation ------------------------------------------------------------- maps &lt;- stack(dir(paste0(FCC.RAW.DIR, &quot;/FC&quot;, COV.FC, &quot;/&quot;, path), pattern=&quot;.*[[:digit:]]{4}\\\\_F.*\\\\.tif$&quot;, full.names=TRUE)) map.names &lt;- sub(&quot;r$&quot;, &quot;&quot;, names(maps)) map.cols &lt;- sub(paste0(path, &quot;\\\\_&quot;), &quot;X&quot;, sub(&quot;\\\\_[[:alnum:]]+$&quot;, &quot;M&quot;, map.names)) no.map.cols &lt;- paste0(&quot;X&quot;, PERIOD[!PERIOD %in% gsub(&quot;[[:alpha:]]&quot;, &quot;&quot;, map.cols)], &quot;_&quot;) col.order &lt;- c(map.cols, no.map.cols)[order(c(map.cols, no.map.cols))] maps.values &lt;- values(maps) # extract vector with cell values (huge matrix, takes some time) colnames(maps.values) &lt;- map.cols nsubsets &lt;- numCores - 1 # define subsets for parallel processing subsets &lt;- c(0, floor((1:nsubsets)*(nrow(maps.values)/nsubsets))) # Parallel cleaning of pixel trajectories ############################### registerDoParallel(.env$numCores-1) maps.values.clean &lt;- foreach(i=1:nsubsets, .combine=rbind) %dopar% { # 0. get subset from maps.values val &lt;- maps.values[(subsets[i]+1):subsets[i+1], ] # get one tile of the matrix val[!(is.na(val) | val %in% c(1,3))] &lt;- NA # set everything to NA that is not NA already or 1,3 # 1. remove isolated NAs str &lt;- apply(val, 1, paste, collapse=&quot;&quot;) # convert to strings str.c &lt;- gsub(&quot;NA&quot;, &quot;9&quot;, str) # replace NA with 9 while(!identical(str, str.c)) { # clean until convergence str &lt;- str.c str.c &lt;- gsub(&quot;^(.*3)9(9*3.*)$&quot;, &quot;\\\\13\\\\2&quot;, str.c) # set NA to the corrsponding class str.c &lt;- gsub(&quot;^(.*1)9(9*1.*)$&quot;, &quot;\\\\11\\\\2&quot;, str.c) } # convert back to numeric vector val &lt;- matrix(as.numeric(unlist(strsplit(str.c, &quot;&quot;))), ncol=ncol(val), byrow=TRUE) val[val==9] &lt;- NA # and set NAs again colnames(val) &lt;- map.cols # 2. clean trajectories with sliding window val.c &lt;- val val.o &lt;- val[] &lt;- 0 iter &lt;- 0 # clean until convergence while(!identical(val, val.c) &amp; !identical(val.o, val.c)) { iter &lt;- iter+1 message(&quot; -Clean modal: iteration &quot;, iter, &quot; ... &quot;, appendLF = FALSE) val.o &lt;- val val &lt;- val.c for(l in 3:(ncol(val)-2)) { # 3rd - 3rd last year: modal window size 5 val.c[,l] &lt;- apply(val[,(l-2):(l+2)], 1, modal, na.rm=TRUE, ties=&#39;NA&#39;) } for(l in c(2, ncol(val)-1)) { # 2nd and 2nd last year: modal window size 3 val.c[,l] &lt;- apply(val.c[,(l-1):(l+1)], 1, modal, na.rm=TRUE, ties=&#39;NA&#39;) } message(&quot;done&quot;) } val &lt;- val.c # 3. final regexp cleaning val &lt;- cbind(val, matrix(nrow=nrow(val), # add years with no observation ncol=length(no.map.cols), dimnames=list(NULL, no.map.cols))) val &lt;- val[, col.order] # order correctly str &lt;- apply(val, 1, paste, collapse=&quot;&quot;) # convert to strings str.c &lt;- gsub(&quot;NA&quot;, &quot;9&quot;, str) # replace NA with 9 # replace remaining NAs with preceeding land-cover while(!identical(str, str.c)) { # clean until convergence str &lt;- str.c str.c &lt;- gsub(&quot;39&quot;, &quot;33&quot;, str.c) # set NA to preceeding class str.c &lt;- gsub(&quot;19&quot;, &quot;11&quot;, str.c) } str &lt;- &quot;&quot; # replace trailing NAs with following land-cover while(!identical(str, str.c)) { # clean until convergence str &lt;- str.c str.c &lt;- gsub(&quot;93&quot;, &quot;33&quot;, str.c) # set NA to following class str.c &lt;- gsub(&quot;91&quot;, &quot;11&quot;, str.c) } str &lt;- &quot;&quot; # removing isolated 1s (regeneration that is lost again, not) while(!identical(str, str.c)) { str &lt;- str.c str.c &lt;-gsub(&quot;^(.*3)1(1{0,9}3.*)$&quot;, &quot;\\\\13\\\\2&quot;, str.c) # removing series of 1s up to length 10 #str.c &lt;-gsub(&quot;^(.*33)1(1*33.*)$&quot;, &quot;\\\\13\\\\2&quot;, str.c) } str &lt;- &quot;&quot; # consider regeneration only as forest after 10 years (before it is considered as potential regeneration 2) while(!identical(str, str.c)) { str &lt;- str.c # irgendetwas -&gt; nichtwald -&gt; (potentielle regen, max 8) -&gt; Wald -&gt; irgendetwas str.c &lt;-gsub(&quot;^(.*32{0,8})1(.*)$&quot;, &quot;\\\\12\\\\2&quot;, str.c) } str &lt;- &quot;&quot; val &lt;- matrix(as.numeric(unlist(strsplit(str.c, &quot;&quot;))), ncol=ncol(val), byrow=TRUE) # convert back to numeric vector val[val==9] &lt;- NA # and set NAs again colnames(val) &lt;- col.order val[,grepl(&quot;M$&quot;, colnames(val))] # and get data for map layers # in order to be considered as forest in 2003, a pixels needs to be forest as well in 2000 and 1991 # in order to be considered as &quot;regeneration&quot; in 2018, a pixel needs to be forest from 2008 onwards } # Write the result to the disk -------------------------------------------- # without the &quot;uncleaned&quot; first and the last layer values(maps) &lt;- maps.values.clean writeRaster(dropLayer(maps, c(1,nlayers(maps))), filename=paste0(FCC.CLN.DIR, &quot;/FC&quot;, COV.FC, &quot;/&quot;, path, &quot;/&quot;, map.names[2:(nlayers(maps)-1)], &quot;c.tif&quot;), bylayer=TRUE, format=&quot;GTiff&quot;, datatype=&quot;INT2U&quot;, overwrite=TRUE) # write trajectories to textfile maps.strings &lt;- apply(maps.values.clean, 1, paste, collapse=&quot;&quot;) sink(paste0(FCC.CLN.DIR, &quot;/FC&quot;, COV.FC, &quot;/&quot;, path, &quot;/Trajectories.txt&quot;)) print(table(maps.strings)) sink() } # Functions for spatial cleaning of a set of images ----------------------- # Remove forest/defor/regen pixels that NEVER has been part of &quot;forest &gt; 0.5ha&quot; since 2003 clean.spatial.forest &lt;- function(maps, exclude = NULL, size=6, connectedness=8){ # default: at least 6 connected pixels (0.54 ha) with 8-connectedness tmp1 &lt;- tempfile(pattern = &quot;&quot;, fileext = &quot;.tif&quot;) tmp2 &lt;- tempfile(pattern = &quot;&quot;, fileext = &quot;.tif&quot;) fcc.map &lt;- apply(maps[[-exclude]][], 1, paste, collapse=&quot;&quot;) onceforest.map &lt;- raster(maps) onceforest.map[] &lt;- NA # onceforest.map &lt;- fcc.map # create &quot;once-forest&quot; map onceforest.map[grepl(&quot;^3*$&quot;, fcc.map)] &lt;- 3 onceforest.map[grepl(&quot;^.*1.*$&quot;, fcc.map)] &lt;- 1 onceforest.map[grepl(&quot;^.*2.*$&quot;, fcc.map)] &lt;- 1 writeRaster(onceforest.map, tmp1) # remove isolated forest patches &lt; xy ha system(paste0(&quot;gdal_sieve.py -st &quot;, size, &quot; -&quot;, connectedness, &quot; -nomask &quot;, tmp1, &quot; &quot;, tmp2)) onceforest.map.clean &lt;- raster(tmp2) onceforest.map.clean[onceforest.map.clean == -2147483648] &lt;- NA # mask the maps with this cleaned forest map maps.clean &lt;- mask(maps, onceforest.map.clean, maskvalue=3, updatevalue=3) writeRaster(maps.clean, filename=paste0(FCC.CLN.DIR, &quot;/FC&quot;, COV.FC, &quot;/TGO/&quot;, names(maps.clean), &quot;f.tif&quot;), bylayer=TRUE, format=&quot;GTiff&quot;, datatype=&quot;INT2U&quot;, overwrite=TRUE) } clean.spatial.fcc &lt;- function(maps, size=3, connectedness=8){ # at least 3 connected pixels (8-connectedness) # combine the maps 2003, 2005 and 2018 # change: 0 &quot;321&quot; &quot;112&quot; &quot;122&quot; &quot;322&quot; &quot;132&quot; &quot;332&quot; &quot;113&quot; &quot;213&quot; &quot;313&quot; # stable non-forest: 3 &quot;333&quot; # stable forest: 1 &quot;111&quot; # filter the map (forest islands and non-forest islands are also filtered out) system(&quot;/Library/Frameworks/GDAL.framework/Programs/gdal_sieve.py -st 3 -8 -nomask ./results/forest.tif ./results/forest_ha.tif&quot;) } ### DO THE WORK ########################################################### # change extent of p192_2019 to the extent of other p192 images (TODO: to be done already in 1_prepare-images.R) extend(brick(paste0(FCC.RAW.DIR, &quot;/FC&quot;, COV.FC, &quot;/p192/p192_2019_F&quot;, COV.FC, &quot;r.tif&quot;)), raster(paste0(FCC.RAW.DIR, &quot;/FC&quot;, COV.FC, &quot;/p192/p192_2018_F&quot;, COV.FC, &quot;r.tif&quot;)), filename=paste0(FCC.RAW.DIR, &quot;/FC&quot;, COV.FC, &quot;/p192/p192_2019_F&quot;, COV.FC, &quot;rt.tif&quot;), format=&quot;GTiff&quot;, datatype=&quot;INT2U&quot;) file.rename(paste0(FCC.RAW.DIR, &quot;/FC&quot;, COV.FC, &quot;/p192/p192_2019_F&quot;, COV.FC, &quot;rt.tif&quot;), paste0(FCC.RAW.DIR, &quot;/FC&quot;, COV.FC, &quot;/p192/p192_2019_F&quot;, COV.FC, &quot;r.tif&quot;)) # Temporal cleaning of paths ---------------------------------------------- for(path in c(&quot;p192&quot;, &quot;p193&quot;, &quot;p194&quot;)) { clean.temporal(path) } # Merging paths ----------------------------------------------------------- for(year in JNT.YEARS) { merge(mask(crop(brick(paste0(FCC.CLN.DIR, &quot;/FC&quot;, COV.FC, &quot;/p193/p193_&quot;, year, &quot;_F&quot;, COV.FC, &quot;c.tif&quot;)), TGO), TGO), mask(crop(brick(paste0(FCC.CLN.DIR, &quot;/FC&quot;, COV.FC, &quot;/p192/p192_&quot;, year, &quot;_F&quot;, COV.FC, &quot;c.tif&quot;)), TGO), TGO), mask(crop(brick(paste0(FCC.CLN.DIR, &quot;/FC&quot;, COV.FC, &quot;/p194/p194_&quot;, year, &quot;_F&quot;, COV.FC, &quot;c.tif&quot;)), TGO), TGO), filename=paste0(FCC.CLN.DIR, &quot;/FC&quot;, COV.FC, &quot;/TGO/TGO_&quot;, year, &quot;_F&quot;, COV.FC, &quot;c.tif&quot;), overwrite=TRUE) } # merge(mask(crop(brick(paste0(FCC.CLN.DIR, &quot;/p193/p193_1990_F30c.tif&quot;)), TGO), TGO), # mask(crop(brick(paste0(FCC.CLN.DIR, &quot;/p192/p192_1991_F30c.tif&quot;)), TGO), TGO), # mask(crop(brick(paste0(FCC.CLN.DIR, &quot;/p194/p194_1997_F30c.tif&quot;)), TGO), TGO), # filename=paste0(FCC.CLN.DIR, &quot;/TGO/1_clean/TGO_1991_F30c.tif&quot;), overwrite=TRUE) # # merge(mask(crop(brick(paste0(FCC.CLN.DIR, &quot;/p193/p193_2000_F30c.tif&quot;)), TGO), TGO), # mask(crop(brick(paste0(FCC.CLN.DIR, &quot;/p192/p192_2001_F30c.tif&quot;)), TGO), TGO), # mask(crop(brick(paste0(FCC.CLN.DIR, &quot;/p194/p194_2000_F30c.tif&quot;)), TGO), TGO), # filename=paste0(FCC.CLN.DIR, &quot;/TGO/1_clean/TGO_2000_F30c.tif&quot;), overwrite=TRUE) # # merge(mask(crop(brick(paste0(FCC.CLN.DIR, &quot;/p193/p193_2009_F30c.tif&quot;)), TGO), TGO), # mask(crop(brick(paste0(FCC.CLN.DIR, &quot;/p192/p192_2011_F30c.tif&quot;)), TGO), TGO), # mask(crop(brick(paste0(FCC.CLN.DIR, &quot;/p194/p194_2010_F30c.tif&quot;)), TGO), TGO), # filename=paste0(FCC.CLN.DIR, &quot;/TGO/1_clean/TGO_2010_F30c.tif&quot;), overwrite=TRUE) # # merge(mask(crop(brick(paste0(FCC.CLN.DIR, &quot;/p193/p193_2013_F30c.tif&quot;)), TGO), TGO), # mask(crop(brick(paste0(FCC.CLN.DIR, &quot;/p192/p192_2013_F30c.tif&quot;)), TGO), TGO), # mask(crop(brick(paste0(FCC.CLN.DIR, &quot;/p194/p194_2012_F30c.tif&quot;)), TGO), TGO), # filename=paste0(FCC.CLN.DIR, &quot;/TGO/1_clean/TGO_2013_F30c.tif&quot;), overwrite=TRUE) # Spatial cleaning of results (only from 2003 onwards) ----------------------------------- clean.spatial.forest(maps = stack(dir(paste0(FCC.CLN.DIR, &quot;/FC&quot;, COV.FC, &quot;/TGO&quot;), pattern = &quot;c\\\\.tif&quot;, full.names = TRUE)), exclude = c(1), size = 6, connectedness = 8) # exclude the first layer (1987) for creating the forest / non-forest mask # # and put together in one stack # tmp &lt;- stack(dir(paste0(FCC.CLN.DIR, &quot;/TGO/2_clean_05ha&quot;), pattern=&quot;.*[[:digit:]]{4}.*&quot;, full.names=TRUE)) # writeRaster(tmp, paste0(FCC.CLN.DIR, &quot;/TGO/2_clean_05ha/TGO_stack_F30c_05ha.tif&quot;), overwrite=TRUE) # # # Loading PHCF-Filter ----------------------------------------------------- # # pixels with value 1 will only remain if they are in squares of 4 or adjacent to that # # otherwise, if they are adjacent to a pixel with value &#39;2&#39; they will be converted to &#39;2&#39; # # otherwise, they will be converted to zero # # # compiling and loading C code that implements the PHCF filter # system(&quot;R CMD SHLIB ./phcf_filter/phcf_filter.c&quot;) # dyn.load(&quot;../phcf_filter/phcf_filter.so&quot;) # # # defining function for facilitating the use of the C function # phcf_pt &lt;- function(defor) { # res &lt;- .C(&quot;phcf_filter&quot;, nrow(defor), ncol(defor), as.integer(defor[])) # return(res[[3]]) # } # # clean.spatial.fcc(maps = stack(raster(paste0(FCC.CLN.DIR, &quot;/TGO/2_clean_05ha/TGO_2003_F30c_05ha.tif&quot;)), # raster(paste0(FCC.CLN.DIR, &quot;/TGO/2_clean_05ha/TGO_2015_F30c_05ha.tif&quot;)), # raster(paste0(FCC.CLN.DIR, &quot;/TGO/2_clean_05ha/TGO_2018_F30c_05ha.tif&quot;))), # size = 3, connectedness = 8) "],
["04_create-val-points.html", "", " 4.3.4 Parcelles de validation Description Description de la méthodologie / script FCC/5_create-val-points.R ########################################################################## # NERF_Togo/FCC/5_create-val-points.R: validate clean forest cover maps # ------------------------------------------------------------------------ # Bern University of Applied Sciences # Oliver Gardi, &lt;oliver.gardi@bfh.ch&gt; # 20 May 2019 # merge the 1987, 2003, 2015 and 2018 maps maps.p192 &lt;- do.call(stack, lapply(dir(paste0(OUTPUT.DIR, &quot;/2_forest-maps/TGO/2_raw-maps/p192&quot;), pattern=&quot;\\\\.tif&quot;, full.names=TRUE)[c(2,5,10,12)], raster)) maps.p193 &lt;- do.call(stack, lapply(dir(paste0(OUTPUT.DIR, &quot;/2_forest-maps/TGO/2_raw-maps/p193&quot;), pattern=&quot;\\\\.tif&quot;, full.names=TRUE)[c(2,6,11,13)], raster)) maps.p194 &lt;- do.call(stack, lapply(dir(paste0(OUTPUT.DIR, &quot;/2_forest-maps/TGO/2_raw-maps/p194&quot;), pattern=&quot;\\\\.tif&quot;, full.names=TRUE)[c(2,5,10,12)], raster)) maps &lt;- merge(maps.p193, maps.p194, maps.p192) writeRaster(maps, filename=paste0(VALIDTN.DIR, &quot;/1_val4k-s16trans/TGO.tif&quot;), bylayer=TRUE, suffix=paste0(c(1987, 2003, 2015, 2018), &quot;_FC30r&quot;)) # maps &lt;- stack(dir(MAPS.DIR, pattern=paste0(&quot;.*c&quot;, MAPS.SUFFIX, &quot;[.]tif$&quot;), full.names=TRUE))[[c(1,4,12,14)]] change.map &lt;- maps[[1]] + maps[[2]]*10 + maps[[3]]*100 + maps[[4]]*1000 # Allocation of validation points # ================================ n &lt;- 4000 alloc &lt;- freq(change.map)[1:16,] alloc &lt;- cbind(alloc, prop=round(n*alloc[,&quot;count&quot;]/sum(alloc[,&quot;count&quot;])), equal=round(n/nrow(alloc))) alloc &lt;- cbind(alloc, balanced=round((alloc[,&quot;prop&quot;] + alloc[,&quot;equal&quot;])/ 2)) sum(alloc[,&quot;balanced&quot;]) # Read the frame # ============== frame.points &lt;- readOGR(paste0(INPUT.DIR, &quot;/SSTS/TGO_frame_480m.shp&quot;)) # extract forest transitions frame.points$trans &lt;- raster::extract(change.map, frame.points) # writeOGR(frame.points, dsn=paste0(INPUT.DIR, &quot;/Train-Val/201908&quot;), layer=&quot;TGO_frame_480m&quot;, driver=&quot;ESRI Shapefile&quot;, overwrite=TRUE) # merge with training-plots data train.plots &lt;- readOGR(paste0(REFMAPS.DIR, &quot;/1_trn10k-s10ndvi/assessed_v1/COV_parcelles.shp&quot;)) frame.points &lt;- merge(frame.points, train.plots@data[,c(&quot;PLOTID&quot;, &quot;img_date&quot;, &quot;img_src&quot;, &quot;mod_date&quot;, &quot;author&quot;, &quot;ccov&quot;)], by=&quot;PLOTID&quot;, all.x=TRUE) # initiate the sample val.points &lt;- frame.points[0,] # Initialize random number generator set.seed(1) # for each stratum ... for(i in 1:nrow(alloc)) { strat.n &lt;- alloc[i, &quot;balanced&quot;] # stratum sample size sample.ids &lt;- NULL # vectors with ids to sample # take as much samples already used as training-points ids.tp &lt;- which(!is.na(frame.points$trans) &amp; frame.points$trans==alloc[i, &quot;value&quot;] &amp; !is.na(frame.points$ccov)) n.tp &lt;- min(length(ids.tp), strat.n) if (n.tp &gt; 0) sample.ids &lt;- c(sample.ids, sample(ids.tp, n.tp)) # and complete with other samples from the grid ids.r &lt;- which(!is.na(frame.points$trans) &amp; frame.points$trans==alloc[i, &quot;value&quot;] &amp; is.na(frame.points$ccov)) n.r &lt;- min(length(ids.r), strat.n - n.tp) if (n.r &gt; 0) sample.ids &lt;- c(sample.ids, sample(ids.r, n.r)) val.points &lt;- rbind(val.points, frame.points[sample.ids, ]) } # shuffle the data val.points &lt;- val.points[sample(1:nrow(val.points)), ] # add ID val.points$SAMPLEID &lt;- paste0(&quot;val-&quot;, str_pad(string=1:nrow(val.points), width = 4, pad = &quot;0&quot;, side = &quot;left&quot;)) # Convert to plots and add attributes # =================================== landsat.grid &lt;- raster(change.map) values(landsat.grid) &lt;- 1 # convert to polygons val.plots &lt;- rasterToPolygons(mask(landsat.grid, val.points)) # fetch attributes val.plots@data &lt;- over(val.plots, val.points[, c(&quot;PLOTID&quot;, &quot;SAMPLEID&quot;, &quot;xcoords&quot;, &quot;ycoords&quot;, &quot;trans&quot;, &quot;ccov&quot;, &quot;img_src&quot;, &quot;img_date&quot;, &quot;author&quot;, &quot;mod_date&quot;)]) # add attributes val.plots$ccov &lt;- format(round(100*val.plots$ccov, 1)) val.plots$ccov[val.plots$ccov == &quot; NA&quot;]&lt;- NA val.plots$lc_18 &lt;- val.plots$lc_15 &lt;- as.character(NA) val.plots$lc_03 &lt;- val.plots$lc_87 &lt;- as.character(NA) # write plots as Shapefile and KML # writeOGR(val.plots, dsn=paste0(INPUT.DIR, &quot;/Train-Val/201908/val9k-s16trans/empty&quot;), layer=&quot;UOT_parcelles&quot;, driver=&quot;ESRI Shapefile&quot;, overwrite=TRUE) # writeKML(val.plots, kmlname=&quot;UOT_parcelles&quot;, filename=paste0(INPUT.DIR, &quot;/Train-Val/201908/val9k-s16trans/empty/UOT_parcelles.kml&quot;)) writeOGR(val.plots, dsn=&quot;.&quot;, layer=&quot;UOT_parcelles&quot;, driver=&quot;ESRI Shapefile&quot;, overwrite=TRUE) writeKML(val.plots, kmlname=&quot;UOT_parcelles&quot;, filename=&quot;UOT_parcelles.kml&quot;) # Create sample grid # ================== grid.size &lt;- 7 res &lt;- res(landsat.grid)[1] offset &lt;- c(res/grid.size/2 + (0:(grid.size-1))*res/grid.size) # split the plots for parallel processing subsets &lt;- split(val.plots, f=1:86) val.grids &lt;- foreach(subset=subsets, .combine=bind, .multicombine=TRUE) %dopar% { grids &lt;- SpatialPointsDataFrame(data.frame(x = 0, y = 0), data=data.frame(PLOTID = 0, SAMPLEID = 0, GRIDPOINT = 0))[-1,] for(p in 1:length(subset)) { plot &lt;- subset[p,] ext &lt;- extent(plot) grids &lt;- bind(grids, SpatialPointsDataFrame(expand.grid(ext@xmin+offset, ext@ymin+offset), data=data.frame(PLOTID = plot$PLOTID, SAMPLEID = plot$SAMPLEID,GRIDPOINT = 1:grid.size^2))) } grids } proj4string(val.grids) &lt;- proj4string(val.plots) # merge with already collected tree attributes from train.grid train.grids &lt;- readOGR(paste0(INPUT.DIR, &quot;/Train-Val/201908/trn10k-s10ndvi/assessed_v1/COV_parcelles_grid.shp&quot;)) val.grids &lt;- merge(val.grids, train.grids@data[, c(&quot;PLOTID&quot;, &quot;GRIDPOINT&quot;, &quot;tree&quot;)], all.x=TRUE) writeOGR(val.grids, dsn=&quot;.&quot;, layer=&quot;UOT_parcelles_grid&quot;, driver=&quot;ESRI Shapefile&quot;, overwrite=TRUE) # Divide into 10 subsets and export # ================================= subsets &lt;- split(val.plots, f=1:10) for(i in 1:length(subsets)) { writeOGR(subsets[[i]], dsn=&quot;.&quot;, layer=paste0(&quot;UOT_parcelles_&quot;, i), driver=&quot;ESRI Shapefile&quot;, overwrite=TRUE) writeKML(subsets[[i]], kmlname=paste0(&quot;UOT_parcelles_&quot;, i) , filename=paste0(&quot;UOT_parcelles_&quot;, i, &quot;.kml&quot;)) subset.grids &lt;- val.grids[val.grids$PLOTID %in% subsets[[i]]$PLOTID,] writeOGR(subset.grids, dsn=&quot;.&quot;, layer=paste0(&quot;UOT_parcelles_&quot;, i, &quot;_grid&quot;), driver=&quot;ESRI Shapefile&quot;, overwrite=TRUE) } # do the assessment # ================= # read and merge assessed validation-plots # ======================================== subsets &lt;- dir(paste0(VALIDTN.DIR, &quot;/1_val4k-s16trans/3_assessed&quot;), pattern=&quot;.*_[[:digit:]]+\\\\.shp$&quot;, recursive=TRUE, full.names=TRUE) val.plots.in &lt;- lapply(subsets, readOGR) names(val.plots.in) &lt;- list.dirs(paste0(VALIDTN.DIR, &quot;/1_val4k-s16trans/3_assessed&quot;), recursive=FALSE) for(i in 1:length(val.plots.in)) { val.plots.in[[i]]$author &lt;- as.character(val.plots.in[[i]]$author) val.plots.in[[i]]$author[!is.na(val.plots.in[[i]]$author)] &lt;- names(val.plots.in[i]) } val.plots.in &lt;- do.call(rbind, val.plots.in) subsets.grids &lt;- dir(paste0(VALIDTN.DIR, &quot;/1_val4k-s16trans/3_assessed&quot;), pattern=&quot;.*_[[:digit:]]+_grid\\\\.shp$&quot;, recursive=TRUE, full.names=TRUE) val.grids.in &lt;- do.call(bind, lapply(subsets.grids, readOGR)) # recalculate crown-cover and merge val.plots.in &lt;- val.plots.in[, names(val.plots.in) != &quot;ccov&quot;] tmp &lt;- aggregate(list(ccov=val.grids.in$tree), by=list(PLOTID=val.grids.in$PLOTID), FUN=function(x) sum(!is.na(x) &amp; x==1)/sum(!is.na(x))) tmp$ccov[is.nan(tmp$ccov)] &lt;- NA val.plots.in &lt;- merge(val.plots.in, tmp, by=&quot;PLOTID&quot;) # clean validation plots #----------------------- # remove plots without ccov val.plots.in &lt;- val.plots.in[!is.na(val.plots.in$ccov),] # remove plots with strange dates year &lt;- as.numeric(sub(&quot;-.*$&quot;, &quot;&quot;, as.character(val.plots.in$img_date))) val.plots.in &lt;- val.plots.in[!is.na(year) &amp; year &gt;= 2000 &amp; year &lt;= 2019, ] # clean grid accordingly val.grids.in &lt;- val.grids.in[val.grids.in$PLOTID %in% unique(val.plots.in$PLOTID), ] # Write clean files # ----------------- writeOGR(val.plots.in, dsn=paste0(VALIDTN.DIR, &quot;/1_val4k-s16trans/3_assessed&quot;), layer=&quot;UOT_parcelles&quot;, driver=&quot;ESRI Shapefile&quot;, overwrite=TRUE) writeOGR(val.grids.in, dsn=paste0(VALIDTN.DIR, &quot;/1_val4k-s16trans/3_assessed&quot;), layer=&quot;UOT_parcelles_grid&quot;, driver=&quot;ESRI Shapefile&quot;, overwrite=TRUE) # Write some descriptive information # ---------------------------------- sink(paste0(VALIDTN.DIR, &quot;/1_val4k-s16trans/4_descr/author-stats.txt&quot;), split=TRUE) table(val.plots.in$author) sink() "],
["05_validate-fc-maps.html", "", " 4.3.5 Validation des cartes Description Description de la méthodologie / script FCC/6_validate-fc-maps.R ########################################################################## # NERF_Togo/FCC/6_validate-fc-maps.R: validate clean forest cover maps # ------------------------------------------------------------------------ # Bern University of Applied Sciences # Oliver Gardi, &lt;oliver.gardi@bfh.ch&gt; # 20 May 2019 # Variables ------------------------------------------------------- COV.FC &lt;- 30 ct &lt;- list() # List of confusion tables to be written to xls file # Load maps, training points -------------------------------- # Load raw and clean forest / non-forest maps maps &lt;- stack(c(paste0(FCC.RAW.DIR, &quot;/FC&quot;, COV.FC, &quot;/TGO/TGO_&quot;, VAL.YEARS, &quot;_F&quot;, COV.FC, &quot;r.tif&quot;), paste0(FCC.CLN.DIR, &quot;/FC&quot;, COV.FC, &quot;/TGO/TGO_&quot;, VAL.YEARS, &quot;_F&quot;, COV.FC, &quot;c.tif&quot;))) names(maps) &lt;- sub(&quot;TGO\\\\_&quot;, &quot;X&quot;, sub(&quot;\\\\_F[[:digit:]]{2}&quot;, &quot;&quot;, names(maps))) # Load training points used for calibration and extract 2018 values from the maps train.plots &lt;- readOGR(paste0(TRNPTS.DIR, &quot;/COV_parcelles.shp&quot;)) train.points &lt;- SpatialPointsDataFrame(gCentroid(train.plots, byid=TRUE), # convert polygons to spatial points (centroids) data.frame(COV=train.plots$ccov)) # with only COV in the attribute table train.points &lt;- raster::extract(maps[[c(&quot;X2018r&quot;,&quot;X2018c&quot;)]], train.points, sp=TRUE) # delete rows with NAs train.points &lt;- train.points[rowSums(is.na(train.points@data)) == 0, ] # reading validation plots ------------------------ # Load validation points, clean and extract map values val.plots &lt;- readOGR(paste0(VALPTS.DIR, &quot;/UOT_parcelles.shp&quot;)) val.points &lt;- SpatialPointsDataFrame(gCentroid(val.plots, byid=TRUE), data.frame(author=sub(&quot;^.*\\\\/&quot;, &quot;&quot;, val.plots$author), V1987=as.numeric(substr(val.plots$lc_87, 1, 1)), V2003=as.numeric(substr(val.plots$lc_03, 1, 1)), V2015=as.numeric(substr(val.plots$lc_15, 1, 1)), V2018=as.numeric(substr(val.plots$lc_18, 1, 1)))) val.points &lt;- raster::extract(maps, val.points, sp=TRUE) # delete rows with NAs val.points &lt;- val.points[rowSums(is.na(val.points@data)) == 0, ] # adjusting classes of reference points ---------------------- # if(COV.FC == 30) { # # change the woodland (2) to non-forest # val.points$V1987[val.points$V1987 == 2] &lt;- 3 # val.points$V2003[val.points$V2003 == 2] &lt;- 3 # val.points$V2015[val.points$V2015 == 2] &lt;- 3 # val.points$V2018[val.points$V2018 == 2] &lt;- 3 # } if(COV.FC == 10) { # change the woodland (2) to forest val.points$V1987[val.points$V1987 == 2] &lt;- 1 val.points$V2003[val.points$V2003 == 2] &lt;- 1 val.points$V2015[val.points$V2015 == 2] &lt;- 1 val.points$V2018[val.points$V2018 == 2] &lt;- 1 } # set cloud/shadow class (4) to forest or non-forest, according to the map val.points$V1987r &lt;- val.points$V1987; val.points$V1987r[val.points$V1987 %in% c(2,4)] &lt;- val.points$X1987r[val.points$V1987 %in% c(2,4)] val.points$V2003r &lt;- val.points$V2003; val.points$V2003r[val.points$V2003 %in% c(2,4)] &lt;- val.points$X2003r[val.points$V2003 %in% c(2,4)] val.points$V2015r &lt;- val.points$V2015; val.points$V2015r[val.points$V2015 %in% c(2,4)] &lt;- val.points$X2015r[val.points$V2015 %in% c(2,4)] val.points$V2018r &lt;- val.points$V2018; val.points$V2018r[val.points$V2018 %in% c(2,4)] &lt;- val.points$X2018r[val.points$V2018 %in% c(2,4)] # same thing for cleaned map val.points$V1987c &lt;- val.points$V1987; val.points$V1987c[val.points$V1987 %in% c(2,4)] &lt;- val.points$X1987c[val.points$V1987 %in% c(2,4)] val.points$V2003c &lt;- val.points$V2003; val.points$V2003c[val.points$V2003 %in% c(2,4)] &lt;- val.points$X2003c[val.points$V2003 %in% c(2,4)] val.points$V2015c &lt;- val.points$V2015; val.points$V2015c[val.points$V2015 %in% c(2,4)] &lt;- val.points$X2015c[val.points$V2015 %in% c(2,4)] val.points$V2018c &lt;- val.points$V2018; val.points$V2018c[val.points$V2018 %in% c(2,4)] &lt;- val.points$X2018c[val.points$V2018 %in% c(2,4)] # adjusting classes of maps ---------------------- # for the clean maps, set regeneration (2) of the clean maps to forest/non-forest according the validation points val.points$X1987c[val.points$X1987c == 2] &lt;- val.points$V1987c[val.points$X1987c == 2] val.points$X2003c[val.points$X2003c == 2] &lt;- val.points$V2003c[val.points$X2003c == 2] val.points$X2015c[val.points$X2015c == 2] &lt;- val.points$V2015c[val.points$X2015c == 2] val.points$X2018c[val.points$X2018c == 2] &lt;- val.points$V2018c[val.points$X2018c == 2] # set to non-forest where a 2 remains (regeneration on the maps and woodland in validation points) val.points$X1987c[val.points$X1987c == 2] &lt;- val.points$V1987c[val.points$X1987c == 2] &lt;- 3 val.points$X2003c[val.points$X2003c == 2] &lt;- val.points$V2003c[val.points$X2003c == 2] &lt;- 3 val.points$X2015c[val.points$X2015c == 2] &lt;- val.points$V2015c[val.points$X2015c == 2] &lt;- 3 val.points$X2018c[val.points$X2018c == 2] &lt;- val.points$V2018c[val.points$X2018c == 2] &lt;- 3 val.points@data &lt;- mutate_all(val.points@data, as.factor) ## DO THE WORK ------------------------------------------------------- # Compare forest cover map 2018 with reference 2018 --------- ref.map &lt;- raster(paste0(FCC.REF.DIR, &quot;/FC&quot;, COV.FC, &quot;/TGO_2018_FC&quot;, COV.FC, &quot;_R.tif&quot;)) ct[[&quot;MAP_REF.18r&quot;]] &lt;- confusionMatrix(as.factor(maps$X2018r[]), as.factor(ref.map[])) pdf(paste0(FCC.VAL.DIR, &quot;/FC2018_vs_COV2018_FC&quot;, COV.FC, &quot;.pdf&quot;)) plot(factor(train.points$X2018r, labels=c(&quot;Forest&quot;, &quot;Non-Forest&quot;)) ~ train.points$COV, xlab=&quot;Tree Cover 2018&quot;, ylab=&quot;Forest Cover Map 2018&quot;) dev.off() pdf(paste0(FCC.VAL.DIR, &quot;/COV2018_vs_FC2018_FC&quot;, COV.FC, &quot;.pdf&quot;)) plot(train.points$COV ~ factor(train.points$X2018r, labels=c(&quot;Forest&quot;, &quot;Non-Forest&quot;)), xlab=&quot;Forest Cover Map 2018&quot;, ylab=&quot;Tree Cover 2018&quot;) dev.off() # Map validation / confusion matrices ---------------------------------- confMat &lt;- function(xtrans, vtrans) { levels &lt;- unique(c(xtrans, vtrans)) return(confusionMatrix(factor(xtrans, levels=levels[order(levels)]), factor(vtrans, levels=levels[order(levels)]))) } # val.points.bu &lt;- val.points # backup val.points &lt;- val.points[!val.points$author %in% c(&quot;7_Aklasson_TOLEBA&quot;, &quot;8_Yawo_KONKO&quot;, &quot;2_Mamalnassoh_ABIGUIME&quot;), ] # check for authors whose validation points reduce precision # print(confMat(paste0(&quot;x&quot;, tmp$X2003c, tmp$X2015c, tmp$X2018c), # paste0(&quot;x&quot;, tmp$VX2003, tmp$VX2015, tmp$VX2018))$overall) # # for(author in unique(tmp$author)) { # print(author) # print(confMat(paste0(&quot;x&quot;, tmp$X2003c[tmp$author != author], tmp$X2015c[tmp$author != author], tmp$X2018c[tmp$author != author]), # paste0(&quot;x&quot;, tmp$VX2003[tmp$author != author], tmp$VX2015[tmp$author != author], tmp$VX2018[tmp$author != author]))$overall) # } for(t in c(&quot;r&quot;, &quot;c&quot;)) { # confusion matrices for individual dates ct[[paste0(&quot;MAP_VAL.87&quot;, t)]] &lt;- confMat(paste0(val.points[[paste0(&quot;X1987&quot;, t)]], &quot;x&quot;, &quot;x&quot;, &quot;x&quot; ), paste0(val.points[[paste0(&quot;V1987&quot;, t)]], &quot;x&quot;, &quot;x&quot;, &quot;x&quot; )) ct[[paste0(&quot;MAP_VAL.03&quot;, t)]] &lt;- confMat(paste0(&quot;x&quot;, val.points[[paste0(&quot;X2003&quot;, t)]], &quot;x&quot;, &quot;x&quot; ), paste0(&quot;x&quot;, val.points[[paste0(&quot;V2003&quot;, t)]], &quot;x&quot;, &quot;x&quot; )) ct[[paste0(&quot;MAP_VAL.15&quot;, t)]] &lt;- confMat(paste0(&quot;x&quot;, &quot;x&quot;, val.points[[paste0(&quot;X2015&quot;, t)]], &quot;x&quot; ), paste0(&quot;x&quot;, &quot;x&quot;, val.points[[paste0(&quot;V2015&quot;, t)]], &quot;x&quot; )) ct[[paste0(&quot;MAP_VAL.18&quot;, t)]] &lt;- confMat(paste0(&quot;x&quot;, &quot;x&quot;, &quot;x&quot;, val.points[[paste0(&quot;X2018&quot;, t)]]), paste0(&quot;x&quot;, &quot;x&quot;, &quot;x&quot;, val.points[[paste0(&quot;V2018&quot;, t)]])) # confusion matrices for 2-date transitions ct[[paste0(&quot;MAP_VAL.87.03&quot;, t)]] &lt;- confMat(paste0(val.points[[paste0(&quot;X1987&quot;, t)]], val.points[[paste0(&quot;X2003&quot;, t)]], &quot;x&quot;, &quot;x&quot; ), paste0(val.points[[paste0(&quot;V1987&quot;, t)]], val.points[[paste0(&quot;V2003&quot;, t)]], &quot;x&quot;, &quot;x&quot; )) ct[[paste0(&quot;MAP_VAL.03.15&quot;, t)]] &lt;- confMat(paste0(&quot;x&quot;, val.points[[paste0(&quot;X2003&quot;, t)]], val.points[[paste0(&quot;X2015&quot;, t)]], &quot;x&quot; ), paste0(&quot;x&quot;, val.points[[paste0(&quot;V2003&quot;, t)]], val.points[[paste0(&quot;V2015&quot;, t)]], &quot;x&quot; )) ct[[paste0(&quot;MAP_VAL.15.18&quot;, t)]] &lt;- confMat(paste0(&quot;x&quot;, &quot;x&quot;, val.points[[paste0(&quot;X2015&quot;, t)]], val.points[[paste0(&quot;X2018&quot;, t)]]), paste0(&quot;x&quot;, &quot;x&quot;, val.points[[paste0(&quot;V2015&quot;, t)]], val.points[[paste0(&quot;V2018&quot;, t)]])) ct[[paste0(&quot;MAP_VAL.87.18&quot;, t)]] &lt;- confMat(paste0(val.points[[paste0(&quot;X1987&quot;, t)]], &quot;x&quot;, &quot;x&quot;, val.points[[paste0(&quot;X2018&quot;, t)]]), paste0(val.points[[paste0(&quot;V1987&quot;, t)]], &quot;x&quot;, &quot;x&quot;, val.points[[paste0(&quot;V2018&quot;, t)]])) ct[[paste0(&quot;MAP_VAL.03.18&quot;, t)]] &lt;- confMat(paste0(&quot;x&quot;, val.points[[paste0(&quot;X2003&quot;, t)]], &quot;x&quot;, val.points[[paste0(&quot;X2018&quot;, t)]]), paste0(&quot;x&quot;, val.points[[paste0(&quot;V2003&quot;, t)]], &quot;x&quot;, val.points[[paste0(&quot;V2018&quot;, t)]])) # confusion matrices for 3-date transition ct[[paste0(&quot;MAP_VAL.03.15.18&quot;, t)]] &lt;- confMat(paste0(&quot;x&quot;,val.points[[paste0(&quot;X2003&quot;, t)]], val.points[[paste0(&quot;X2015&quot;, t)]], val.points[[paste0(&quot;X2018&quot;, t)]]), paste0(&quot;x&quot;, val.points[[paste0(&quot;V2003&quot;, t)]], val.points[[paste0(&quot;V2015&quot;, t)]], val.points[[paste0(&quot;V2018&quot;, t)]])) # confusion matrices for 4-date transition ct[[paste0(&quot;MAP_VAL.87.03.15.18&quot;, t)]] &lt;- confMat(paste0(val.points[[paste0(&quot;X1987&quot;, t)]], val.points[[paste0(&quot;X2003&quot;, t)]], val.points[[paste0(&quot;X2015&quot;, t)]], val.points[[paste0(&quot;X2018&quot;, t)]]), paste0(val.points[[paste0(&quot;V1987&quot;, t)]], val.points[[paste0(&quot;V2003&quot;, t)]], val.points[[paste0(&quot;V2015&quot;, t)]], val.points[[paste0(&quot;V2018&quot;, t)]])) } # Validation RapidEye map 2015 -------------------------------- rey &lt;- raster(paste0(DATA.DIR, &quot;/RapidEye/TGO_30m.tif&quot;)) names(rey) &lt;- &quot;R2015&quot; val.points &lt;- raster::extract(rey, val.points, sp=TRUE) val.points$Rr2015 &lt;- 3 val.points$Rr2015[val.points$R2015 %in% c(11, 12, 16, 18, 19)] &lt;- 1 val.points$VR2015 &lt;- val.points$V2015 val.points$VR2015[val.points$V2015 == 2] &lt;- val.points$Rr2015[val.points$V2015 == 2] ct[[&quot;REY_VAL.15&quot;]] &lt;- confMat(paste0(&quot;x&quot;, &quot;x&quot;, val.points$Rr2015, &quot;x&quot; ), paste0(&quot;x&quot;, &quot;x&quot;, val.points$VR2015, &quot;x&quot; )) # Write confusion tables ------------------------------------- save(ct, file=paste0(FCC.VAL.DIR, &quot;/FC&quot;, COV.FC, &quot;_flex_ConfTab.RData&quot;)) # write error matrices to Excel File write.xlsx(lapply(ct, &quot;[[&quot;, &quot;table&quot;), file=paste0(FCC.VAL.DIR, &quot;/FC&quot;, COV.FC, &quot;_flex_ConfTab.xlsx&quot;), colnames=TRUE, overwrite=TRUE) "],
["06_fc-maps-accuracy.html", "", " 4.3.6 Analyse de la précision Description Description de la méthodologie / script FCC/7_fc-maps-accuracy.R ########################################################################## # NERF_Togo/FCC/7_fc-maps-accuracy.R: validate clean forest cover maps # ------------------------------------------------------------------------ # Bern University of Applied Sciences # Oliver Gardi, &lt;oliver.gardi@bfh.ch&gt; # 20 May 2019 # based on OpenForis implementation of Olofsson et al. (2014), written by # Antonia Ortmann, 20 October, 2014 # Source: https://github.com/openforis/accuracy-assessment/blob/master/Rscripts/error_matrix_analysis.R VALSET &lt;- &quot;FC30_flex&quot; # Function for estimating accuracies ---------------------------------- accuracy.estimate &lt;- function(areas.map, error.matrix, filename=NULL, pixelsize=30^2/10000) { # remove &quot;x&quot; from the category colnames(error.matrix) &lt;- rownames(error.matrix) &lt;- gsub(&quot;x&quot;, &quot;&quot;, colnames(error.matrix)) # match category order maparea &lt;- areas.map[match(rownames(error.matrix), names(areas.map))] ma &lt;- error.matrix dyn &lt;- names(maparea) # calculate the area proportions for each map class aoi &lt;- sum(maparea) propmaparea &lt;- maparea/aoi # convert the absolute cross tab into a probability cross tab ni. &lt;- rowSums(ma) # number of reference points per map class propma &lt;- as.matrix(ma/ni. * as.vector(propmaparea)) propma[is.nan(propma)] &lt;- 0 # for classes with ni. = 0 # estimate the accuracies now OA &lt;- sum(diag(propma)) # overall accuracy (Eq. 1 in Olofsson et al. 2014) pe &lt;- 0 # Agreement by chance ... for (i in 1:length(dyn)) { pe &lt;- pe + sum(propma[i,]) * sum(propma[,i]) } K &lt;- (OA - pe) / (1 - pe) # ... for Cohen&#39;s Kappa UA &lt;- diag(propma) / rowSums(propma) # user&#39;s accuracy (Eq. 2 in Olofsson et al. 2014) PA &lt;- diag(propma) / colSums(propma) # producer&#39;s accuracy (Eq. 3 in Olofsson et al. 2014) # estimate confidence intervals for the accuracies V_OA &lt;- sum(as.vector(propmaparea)^2 * UA * (1 - UA) / (ni. - 1), na.rm=T) # variance of overall accuracy (Eq. 5 in Olofsson et al. 2014) V_UA &lt;- UA * (1 - UA) / (rowSums(ma) - 1) # variance of user&#39;s accuracy (Eq. 6 in Olofsson et al. 2014) # variance of producer&#39;s accuracy (Eq. 7 in Olofsson et al. 2014) N.j &lt;- array(0, dim=length(dyn)) aftersumsign &lt;- array(0, dim=length(dyn)) for(cj in 1:length(dyn)) { N.j[cj] &lt;- sum(maparea / ni. * ma[, cj], na.rm=T) aftersumsign[cj] &lt;- sum(maparea[-cj]^2 * ma[-cj, cj] / ni.[-cj] * ( 1 - ma[-cj, cj] / ni.[-cj]) / (ni.[-cj] - 1), na.rm = T) } V_PA &lt;- 1/N.j^2 * ( maparea^2 * (1-PA)^2 * UA * (1-UA) / (ni.-1) + PA^2 * aftersumsign ) V_PA[is.nan(V_PA)] &lt;- 0 # proportional area estimation propAreaEst &lt;- colSums(propma) # proportion of area (Eq. 8 in Olofsson et al. 2014) AreaEst &lt;- propAreaEst * sum(maparea) # estimated area # standard errors of the area estimation (Eq. 10 in Olofsson et al. 2014) V_propAreaEst &lt;- array(0, dim=length(dyn)) for (cj in 1:length(dyn)) { V_propAreaEst[cj] &lt;- sum((as.vector(propmaparea) * propma[, cj] - propma[, cj] ^ 2) / ( rowSums(ma) - 1)) } V_propAreaEst[is.na(V_propAreaEst)] &lt;- 0 # produce result tables res &lt;- list() res$PREDICTED_PX &lt;- as.table(maparea) res$ERROR_MATRIX &lt;- ma res$ERROR_MATRIX_PROP &lt;- round(propma, 3) res$OVERALL_ACC &lt;- data.frame(accuracy=round(c(OA, K), 3), CI=round(c(1.96 * sqrt(V_OA), NA), 3), row.names=c(&quot;OA&quot;, &quot;Kappa&quot;)) res$CLASSES_ACC &lt;- data.frame(maparea=round(maparea * pixelsize, 3)) # in ha res$CLASSES_ACC$prop_maparea &lt;- round(propmaparea, 3) res$CLASSES_ACC$adj_proparea &lt;- round(propAreaEst, 3) res$CLASSES_ACC$CI_adj_proparea &lt;- round(1.96 * sqrt(V_propAreaEst), 3) res$CLASSES_ACC$adj_area &lt;- round(propAreaEst * aoi * pixelsize, 3) # in ha res$CLASSES_ACC$CI_adj_area &lt;- round(1.96 * sqrt(V_propAreaEst) * aoi * pixelsize, 3) # in ha res$CLASSES_ACC$UA &lt;- round(UA, 3) res$CLASSES_ACC$CI_UA &lt;- round(1.96 * sqrt(V_UA), 3) res$CLASSES_ACC$PA &lt;- round(PA, 3) res$CLASSES_ACC$CI_PA &lt;- round(1.96 * sqrt(V_PA), 3) # write results to Excel File if(!is.null(filename)) { write.xlsx(res, file=filename, col.names=TRUE, row.names=TRUE, overwrite=TRUE) } return(res) } # DO THE WORK ---------------------------------- # load the error matrices (R object ct) load(paste0(FCC.VAL.DIR, &quot;/ConfTab_&quot;, VALSET, &quot;.RData&quot;)) # load the predictions and convert &quot;potential regeneration (2)&quot; to &quot;non-forest (3)&quot; fc.2003 &lt;- brick(paste0(FCC.CLN.DIR, &quot;/FC30/TGO/TGO_2003_F30cf.tif&quot;)); fc.2003[fc.2003==2] &lt;- 3 fc.2015 &lt;- brick(paste0(FCC.CLN.DIR, &quot;/FC30/TGO/TGO_2015_F30cf.tif&quot;)); fc.2015[fc.2015==2] &lt;- 3 fc.2018 &lt;- brick(paste0(FCC.CLN.DIR, &quot;/FC30/TGO/TGO_2018_F30cf.tif&quot;)); fc.2018[fc.2018==2] &lt;- 3 # create 3-date transition map fcc &lt;- 100 * fc.2003 + 10 * fc.2015 + 1 * fc.2018 # get pixel counts (takes time) and separate for different dates / transitions freq &lt;- table(fcc[]) tmp &lt;- as.numeric(freq); names(tmp) &lt;- names(freq); freq &lt;- tmp freq.03.15.18 &lt;- c(freq[&quot;111&quot;], freq[&quot;113&quot;], freq[&quot;131&quot;], freq[&quot;133&quot;], freq[&quot;311&quot;], freq[&quot;313&quot;], freq[&quot;331&quot;], freq[&quot;333&quot;]) names(freq.03.15.18) &lt;- c(&quot;111&quot;, &quot;113&quot;, &quot;131&quot;, &quot;133&quot;, &quot;311&quot;, &quot;313&quot;, &quot;331&quot;, &quot;333&quot;); freq.03.15.18[is.na(freq.03.15.18)] &lt;- 0 freq.03.18 &lt;- c(sum(freq[&quot;111&quot;],freq[&quot;131&quot;], na.rm=T), sum(freq[&quot;113&quot;],freq[&quot;133&quot;], na.rm=T), sum(freq[&quot;311&quot;],freq[&quot;331&quot;], na.rm=T), sum(freq[&quot;313&quot;],freq[&quot;333&quot;], na.rm=T)) names(freq.03.18) &lt;- c(&quot;11&quot;, &quot;13&quot;, &quot;31&quot;, &quot;33&quot;); freq.03.18[is.na(freq.03.18)] &lt;- 0 freq.03.15 &lt;- c(sum(freq[&quot;111&quot;],freq[&quot;113&quot;], na.rm=T), sum(freq[&quot;131&quot;],freq[&quot;133&quot;], na.rm=T), sum(freq[&quot;311&quot;],freq[&quot;313&quot;], na.rm=T), sum(freq[&quot;331&quot;],freq[&quot;333&quot;], na.rm=T)) names(freq.03.15) &lt;- c(&quot;11&quot;, &quot;13&quot;, &quot;31&quot;, &quot;33&quot;); freq.03.15[is.na(freq.03.15)] &lt;- 0 freq.15.18 &lt;- c(sum(freq[&quot;111&quot;],freq[&quot;311&quot;], na.rm=T), sum(freq[&quot;113&quot;],freq[&quot;313&quot;], na.rm=T), sum(freq[&quot;131&quot;],freq[&quot;331&quot;], na.rm=T), sum(freq[&quot;133&quot;],freq[&quot;333&quot;], na.rm=T)) names(freq.15.18) &lt;- c(&quot;11&quot;, &quot;13&quot;, &quot;31&quot;, &quot;33&quot;); freq.15.18[is.na(freq.15.18)] &lt;- 0 freq.03 &lt;- c(sum(freq[&quot;111&quot;],freq[&quot;131&quot;], freq[&quot;113&quot;],freq[&quot;133&quot;], na.rm=T), sum(freq[&quot;311&quot;],freq[&quot;331&quot;], freq[&quot;313&quot;],freq[&quot;333&quot;], na.rm=T)) names(freq.03) &lt;- c(&quot;1&quot;, &quot;3&quot;); freq.03[is.na(freq.03)] &lt;- 0 freq.15 &lt;- c(sum(freq[&quot;111&quot;],freq[&quot;311&quot;], freq[&quot;113&quot;],freq[&quot;313&quot;], na.rm=T), sum(freq[&quot;131&quot;],freq[&quot;331&quot;], freq[&quot;133&quot;],freq[&quot;333&quot;], na.rm=T)) names(freq.15) &lt;- c(&quot;1&quot;, &quot;3&quot;); freq.15[is.na(freq.15)] &lt;- 0 freq.18 &lt;- c(sum(freq[&quot;111&quot;],freq[&quot;311&quot;], freq[&quot;131&quot;],freq[&quot;331&quot;], na.rm=T), sum(freq[&quot;113&quot;],freq[&quot;313&quot;], freq[&quot;133&quot;],freq[&quot;333&quot;], na.rm=T)) names(freq.18) &lt;- c(&quot;1&quot;, &quot;3&quot;); freq.18[is.na(freq.18)] &lt;- 0 # create accuracy maps ------------------------------------ accuracy.estimate(freq.18, ct$MAP_VAL.18c$table , filename=paste0(FCC.VAL.DIR, &quot;/&quot;, VALSET, &quot;_Acc_18.xlsx&quot;)) accuracy.estimate(freq.15, ct$MAP_VAL.15c$table , filename=paste0(FCC.VAL.DIR, &quot;/&quot;, VALSET, &quot;_Acc_15.xlsx&quot;)) accuracy.estimate(freq.03, ct$MAP_VAL.03c$table , filename=paste0(FCC.VAL.DIR, &quot;/&quot;, VALSET, &quot;_Acc_03.xlsx&quot;)) accuracy.estimate(freq.03.15, ct$MAP_VAL.03.15c$table , filename=paste0(FCC.VAL.DIR, &quot;/&quot;, VALSET, &quot;_Acc_03-15.xlsx&quot;)) accuracy.estimate(freq.15.18, ct$MAP_VAL.15.18c$table , filename=paste0(FCC.VAL.DIR, &quot;/&quot;, VALSET, &quot;_Acc_15-18.xlsx&quot;)) accuracy.estimate(freq.03.18, ct$MAP_VAL.03.18c$table , filename=paste0(FCC.VAL.DIR, &quot;/&quot;, VALSET, &quot;_Acc_03-18.xlsx&quot;)) accuracy.estimate(freq.03.15.18, ct$MAP_VAL.03.15.18c$table , filename=paste0(FCC.VAL.DIR, &quot;/&quot;, VALSET, &quot;_Acc_03-15-18.xlsx&quot;)) # calculate forest cover, loss and gains --------------------- res &lt;- data.frame(year = c(2003, 2015, 2018), total.ha = c(freq.03[&quot;1&quot;], freq.15[&quot;1&quot;], freq.18[&quot;1&quot;]) * 30^2/10000, defor.ha = c(NA, freq.03.15[&quot;13&quot;], freq.15.18[&quot;13&quot;]) * 30^2/10000, regen.ha = c(NA, freq.03.15[&quot;31&quot;], freq.15.18[&quot;31&quot;]) * 30^2/10000) # Example from Olofsson et al. (2014) # ----------------------------------- # ct &lt;- matrix(data=c(66, 0, 5, 4, # 0, 55, 8, 12, # 1, 0, 153, 11, # 2, 1, 9, 313), # nrow=4, # byrow=TRUE, # dimnames = list(Prediction=c(&quot;FN&quot;, &quot;NF&quot;, &quot;FF&quot;, &quot;NN&quot;), Reference=c(&quot;FN&quot;, &quot;NF&quot;, &quot;FF&quot;, &quot;NN&quot;))) # # px &lt;- c(FN=200000, NF=150000, FF=3200000, NN=6450000) # # accuracy.estimate(px, ct) "],
["07_analyse-fc-maps.html", "", " 4.3.7 Production des résultats Description Description de la méthodologie / script FCC/8_analyse-fc-maps.R ########################################################################## # NERF_Togo/FCC/8_analyse-fc-maps.R: analyze clean forest cover maps # ------------------------------------------------------------------------ # Bern University of Applied Sciences # Oliver Gardi, &lt;oliver.gardi@bfh.ch&gt; # 20 May 2019 COV.FC &lt;- 30 # Function for building forest cover table --------------------------------- fc &lt;- function(map, aoi=NULL) { if(!is.null(aoi)) map &lt;- mask(crop(map, aoi), aoi) # convert non-forest to 0 # potential regeneration to 1 # regeneration to 2 # forest to 3 tab.fc &lt;- map[] tab.fc[tab.fc == 3] &lt;- 0 tab.fc[tab.fc == 1] &lt;- 3 tab.fc[tab.fc == 2] &lt;- 1 for(i in 2:ncol(tab.fc)) { tab.fc[!is.na(tab.fc[,i]) &amp; tab.fc[,i] == 3 &amp; !is.na(tab.fc[,i-1]) &amp; tab.fc[,i-1] &lt; 3, i] &lt;- 2 # mark as regeneration when it was non-forest or regeneration before } tab.fcc &lt;- tab.fc[,1:ncol(tab.fc)-1] tab.fcc[] &lt;- 0 for(i in 1:ncol(tab.fcc)) { tab.fcc[tab.fc[,i] &lt;= 1 &amp; tab.fc[,i+1] &gt;= 2, i] &lt;- 1 # mark forest gain tab.fcc[tab.fc[,i] &gt;= 2 &amp; tab.fc[,i+1] &lt;= 1, i] &lt;- 2 # mark forest loss } dates &lt;- as.numeric(substr(names(map), 2, 5)) fc &lt;- data.frame(year = dates, initi.ha = colSums(tab.fc==3, na.rm=TRUE) * 30^2/10000, secon.ha = colSums(tab.fc==2, na.rm=TRUE) * 30^2/10000, poten.ha = colSums(tab.fc==1, na.rm=TRUE) * 30^2/10000) fc$total.ha &lt;- fc$initi.ha + fc$secon.ha fc$defor.ha &lt;- -c(colSums(tab.fcc==2, na.rm=TRUE) * 30^2/10000, NA) fc$regen.ha &lt;- c(colSums(tab.fcc==1, na.rm=TRUE) * 30^2/10000, NA) return(fc) } fcc &lt;- function(fc.tab, years=fc.tab$year) { for(i in 1:(length(years)-1)) { fc.tab$defor.ha.yr[fc.tab$year==years[i]] &lt;- sum(fc.tab$defor.ha[fc.tab$year &gt;= years[i] &amp; fc.tab$year &lt; years[i+1]])/(years[i+1]-years[i]) fc.tab$regen.ha.yr[fc.tab$year==years[i]] &lt;- sum(fc.tab$regen.ha[fc.tab$year &gt;= years[i] &amp; fc.tab$year &lt; years[i+1]])/(years[i+1]-years[i]) fc.tab$netch.ha.yr[fc.tab$year==years[i]] &lt;- fc.tab$defor.ha.yr[fc.tab$year==years[i]] + fc.tab$regen.ha.yr[fc.tab$year==years[i]] fc.tab$defor.pc.yr[fc.tab$year==years[i]] &lt;- round(100 * 1/(years[i+1]-years[i]) * log((fc.tab$total.ha[fc.tab$year==years[i+1]] - sum(fc.tab$regen.ha[fc.tab$year &gt;= years[i] &amp; fc.tab$year &lt; years[i+1]]))/fc.tab$total.ha[fc.tab$year==years[i]]), 3) fc.tab$regen.pc.yr[fc.tab$year==years[i]] &lt;- round(100 * 1/(years[i+1]-years[i]) * log((fc.tab$total.ha[fc.tab$year==years[i+1]] - sum(fc.tab$defor.ha[fc.tab$year &gt;= years[i] &amp; fc.tab$year &lt; years[i+1]]))/fc.tab$total.ha[fc.tab$year==years[i]]), 3) fc.tab$netch.pc.yr[fc.tab$year==years[i]] &lt;- round(100 * 1/(years[i+1]-years[i]) * log(fc.tab$total.ha[fc.tab$year==years[i+1]]/fc.tab$total.ha[fc.tab$year==years[i]]), 3) } return(fc.tab) } # Function for plotting evolution of forest cover ----------------------------------------- plot.fc &lt;- function(fc, zone, filename=NULL) { if(zone==&quot;TGO&quot;) { title &lt;- &quot;Togo: Évolution de la couverure forestière 2003 - 2018&quot; ylim &lt;- c(0, 1400000) ybreaks &lt;- seq(0, 1400000, 200000) ymbreaks &lt;- seq(0, 1400000, 100000) } else { title &lt;- paste0(zone, &quot;: Évolution de la couverure forestière 2003 - 2018&quot;) ylim &lt;- c(0, 500000) ybreaks &lt;- seq(0, 500000, 100000) ymbreaks &lt;- seq(0, 500000, 50000) } if(!is.null(filename)) pdf(filename) print( fc %&gt;% gather(variable, value, secon.ha, initi.ha) %&gt;% ggplot(aes(x = year, y=value, fill=variable)) + geom_area(position=position_stack(reverse=TRUE)) + scale_fill_manual(name = NULL, breaks=c(&quot;secon.ha&quot;, &quot;initi.ha&quot;), values=c(alpha(&quot;#009E73&quot;, 0.8), alpha(&quot;#00BFC4&quot;, 0.5)), labels=c(&quot;Régéneration&quot;, &quot;Forêt depuis 1987&quot;)) + xlab(&quot;Année&quot;) + ylab(&quot;Hectares&quot;) + ggtitle(title) + scale_x_continuous(minor_breaks = NULL, breaks=c(1991, 2000, 2005, 2010, 2015, 2018)) + scale_y_continuous(breaks=ybreaks, minor_breaks = ymbreaks) + coord_cartesian(ylim=ylim) + theme_light() + theme(legend.position=c(0.6,0.2), legend.box = &quot;horizontal&quot;, legend.justification=c(-0.2,1.2)) ) if(!is.null(filename)) dev.off() } # Function for plotting forest cover change ----------------------------------- plot.fcc &lt;- function(fc, zone=NULL, breaks=NULL, filename=NULL) { my.fcc &lt;- fcc(fc) fcc.breaks &lt;- my.fcc if(!is.null(breaks)) fcc.breaks &lt;- fcc(fc, breaks)[fc$year %in% breaks,] my.fcc$period &lt;- c(my.fcc$year[2:nrow(my.fcc)] - my.fcc$year[1:(nrow(my.fcc)-1)], NA) my.fcc$center &lt;- my.fcc$year + my.fcc$period/2 fcc.breaks$period &lt;- c(fcc.breaks$year[2:nrow(fcc.breaks)] - fcc.breaks$year[1:(nrow(fcc.breaks)-1)], NA) fcc.breaks$center &lt;- fcc.breaks$year + fcc.breaks$period/2 if(zone==&quot;TGO&quot;) { title &lt;- &quot;Togo: Changements bruts et nets des surfaces forestières 2003 - 2018&quot; ylim &lt;- c(-35000, 20000) ybreaks &lt;- seq(-35000, 20000, 5000) ymbreaks &lt;- seq(-35000, 20000, 2500) } else { title &lt;- paste0(zone, &quot;: Changements bruts et nets des surfaces forestières 2003 - 2018&quot;) ylim &lt;- c(-15000, 10000) ybreaks &lt;- seq(-15000, 10000, 5000) ymbreaks &lt;- seq(-15000, 10000, 2500) } if(!is.null(filename)) pdf(filename) fcc.breaks$netdefor.ha.yr &lt;- fcc.breaks$netch.ha.yr fcc.breaks$netdefor.ha.yr[fcc.breaks$netdefor.ha.yr &gt; 0] &lt;- 0 fcc.breaks$netregen.ha.yr &lt;- fcc.breaks$netch.ha.yr fcc.breaks$netregen.ha.yr[fcc.breaks$netregen.ha.yr &lt; 0] &lt;- 0 print( fcc.breaks[-nrow(fcc.breaks),] %&gt;% gather(variable, value, defor.ha.yr, regen.ha.yr, netdefor.ha.yr, netregen.ha.yr) %&gt;% ggplot(aes(y=value, x = center, width=period-0.7)) + geom_bar(data=. %&gt;% filter(variable %in% c(&quot;defor.ha.yr&quot;, &quot;regen.ha.yr&quot;)), aes(fill=variable), stat = &quot;identity&quot;) + geom_bar(data=. %&gt;% filter(variable %in% c(&quot;netdefor.ha.yr&quot;, &quot;netregen.ha.yr&quot;)), aes(fill=variable), stat = &quot;identity&quot;) + geom_errorbar(data=my.fcc[1:5,], aes(x=center, y=NULL, ymax=defor.ha.yr, ymin=defor.ha.yr, width=period-0.7), colour=alpha(&quot;#F8766D&quot;, 1)) + geom_hline(aes(yintercept=0)) + scale_fill_manual(name = NULL, breaks = c(&quot;defor.ha.yr&quot;, &quot;netdefor.ha.yr&quot;, &quot;netregen.ha.yr&quot;, &quot;regen.ha.yr&quot;), values=c(&quot;regen.ha.yr&quot; = alpha(&quot;#00BFC4&quot;, 0.4), &quot;netregen.ha.yr&quot; = &quot;#00BFC4&quot;, &quot;netdefor.ha.yr&quot; = &quot;#F8766D&quot;, &quot;defor.ha.yr&quot; = alpha(&quot;#F8766D&quot;, 0.4)), labels=c(&quot;Perte brutte&quot;, &quot;Perte nette&quot;, &quot;Gain net&quot;, &quot;Gain brut&quot;)) + xlab(&quot;Année&quot;) + ylab(&quot;Hectares par année&quot;) + ggtitle(title) + scale_x_continuous(minor_breaks = NULL, breaks=c(1991, 2000, 2005, 2010, 2015, 2018)) + scale_y_reverse(breaks=ybreaks, minor_breaks = ymbreaks) + coord_cartesian(ylim=ylim) + theme_light() + theme(legend.position=c(0,1), legend.box = &quot;horizontal&quot;, legend.justification=c(-0.2,1.2)) ) if(!is.null(filename)) dev.off() } # DO THE WORK ############################## # Load and rename forest-cover maps ---------------------------- maps &lt;- stack(dir(paste0(FCC.CLN.DIR, &quot;/FC&quot;, COV.FC, &quot;/TGO&quot;), pattern = &quot;cf.tif&quot;, full.names = TRUE)) names(maps) &lt;- paste0(&quot;X&quot;, substr(names(maps), 5, 8)) # Create forest cover tables for different periods ------------------------------------ fc.all &lt;- fc(maps) write.csv(fc.all, paste0(FCC.RES.DIR, &quot;/TGO/TGO_fc.csv&quot;), row.names=FALSE) fc.all &lt;- read.csv(paste0(FCC.RES.DIR, &quot;/TGO/TGO_fc.csv&quot;)) fc.cln &lt;- fc.all fc.cln &lt;- fc.cln[!fc.cln$year %in% c(1987, 1991, 2000), ] fcc(fc.cln) fcc(fc.cln, c(2003, 2018)) fcc(fc.cln, c(2003, 2015, 2017, 2018)) write.xlsx(list(&quot;Total&quot; = fcc(fc.cln)), file = paste0(FCC.RES.DIR, &quot;/TGO/TGO_fcc-all-dates.xlsx&quot;)) write.xlsx(list(&quot;Total&quot; = fcc(fc.cln, c(2003, 2018))), file = paste0(FCC.RES.DIR, &quot;/TGO/TGO_fcc-03-18.xlsx&quot;)) write.xlsx(list(&quot;Total&quot; = fcc(fc.cln, c(2003, 2015, 2018))), file = paste0(FCC.RES.DIR, &quot;/TGO/TGO_fcc-03-15-18.xlsx&quot;)) plot.fc(fc.cln, &quot;TGO&quot;, paste0(FCC.RES.DIR, &quot;/TGO/TGO_fc.pdf&quot;)) plot.fcc(fc = fc.cln, zone = &quot;TGO&quot;, breaks=c(2003, 2015, 2018), filename=paste0(FCC.RES.DIR, &quot;/TGO/TGO_fcc.pdf&quot;)) registerDoParallel(.env$numCores-1) foreach(i=1:length(TGO.reg)) %do% { region &lt;- TGO.reg[i,] # fc.all &lt;- fc(maps, aoi=region) # write.csv(fc.all, paste0(RESULTS.DIR, &quot;/tables/&quot;, region$NAME_1, &quot;_fc.csv&quot;), row.names=FALSE) fc.all &lt;- read.csv(paste0(RESULTS.DIR, &quot;/tables/&quot;, region$NAME_1, &quot;_fc.csv&quot;)) fc.cln &lt;- fc.all[!fc.all$year %in% c(1987, 1991, 2000), ] plot.fc(fc.cln, region$NAME_1, paste0(RESULTS.DIR, &quot;/figures/&quot;, region$NAME_1, &quot;_fc.pdf&quot;)) plot.fcc(fc.cln, region$NAME_1, breaks=c(2003, 2015, 2018), paste0(RESULTS.DIR, &quot;/figures/&quot;, region$NAME_1, &quot;_fcc.pdf&quot;)) write.xlsx(list(&quot;Total&quot; = fcc(fc.cln)), file = paste0(RESULTS.DIR, &quot;/tables/&quot;, region$NAME_1, &quot;_fcc-all-dates.xlsx&quot;)) write.xlsx(list(&quot;Total&quot; = fcc(fc.cln, c(2003, 2018))), file = paste0(RESULTS.DIR, &quot;/tables/&quot;, region$NAME_1, &quot;_fcc-03-18.xlsx&quot;)) write.xlsx(list(&quot;Total&quot; = fcc(fc.cln, c(2003, 2015, 2018))), file = paste0(RESULTS.DIR, &quot;/tables/&quot;, region$NAME_1, &quot;_fcc-03-15-18.xlsx&quot;)) } # creating GIS layers for forest loss and forest gain per date --------------------------------------------- forest.loss &lt;- raster(maps) forest.gain &lt;- raster(maps) # create empty rasters from stack forest.pote &lt;- raster(maps) for(i in 1:(nlayers(maps)-1)) { print(paste0(&quot;Checking deforestation / reforestation in year &quot;, maps.dates[i])) forest.loss[is.na(forest.loss) &amp; is.na(forest.gain) &amp; maps[[i]] == 1 &amp; maps[[i+1]] == 3] &lt;- maps.dates[i] # take the first date of forest loss observed and only if no regeneration before that forest.gain[maps[[i]] %in% c(2,3) &amp; maps[[i+1]] == 1] &lt;- maps.dates[i] forest.pote[maps[[i]] ==3 &amp; maps[[i+1]] == 2] &lt;- maps.dates[i] } # special layer for forest gain in areas where loss has been observed before loss.gain &lt;- raster(maps) loss.gain[!is.na(forest.loss) &amp; !is.na(forest.gain) &amp; forest.gain &gt; forest.loss] &lt;- forest.gain[!is.na(forest.loss) &amp; !is.na(forest.gain) &amp; forest.gain &gt; forest.loss] forest.2018 &lt;- maps$X2018 forest.2018[forest.2018==2] &lt;- 3 writeRaster(forest.2018, paste0(RESULTS.DIR, &quot;/maps/forest-2018.tif&quot;), datatype=&quot;INT2U&quot;, overwrite=TRUE) writeRaster(forest.loss, paste0(RESULTS.DIR, &quot;/maps/forest-loss.tif&quot;), datatype=&quot;INT2U&quot;, overwrite=TRUE) writeRaster(forest.gain, paste0(RESULTS.DIR, &quot;/maps/forest-gain.tif&quot;), datatype=&quot;INT2U&quot;, overwrite=TRUE) writeRaster(forest.pote, paste0(RESULTS.DIR, &quot;/maps/forest-potential.tif&quot;), datatype=&quot;INT2U&quot;, overwrite=TRUE) writeRaster(loss.gain, paste0(RESULTS.DIR, &quot;/maps/loss-gain.tif&quot;), datatype=&quot;INT2U&quot;, overwrite=TRUE) "],
["00_analyse-AGB.html", "4.4 Analyse biomasse aérienne", " 4.4 Analyse biomasse aérienne Description générale de l’approche / méthodologies "],
["01_compile-IFN.html", "", " 4.4.1 Analyse des données IFN Description Description de la méthodologie / script AGB/2_compile-IFN.R #################################################################### # NERF_Togo/AGB/2_compile-IFN.R: evaluating AGB of IFN plots # ------------------------------------------------------------------ # Bern University of Applied Sciences # Oliver Gardi, &lt;oliver.gardi@bfh.ch&gt; # 3 December 2019 PLOT.SIZE &lt;- 20^2*pi # Plot size in square meters RSR.Y &lt;- 0.563 # Root-shoot ratios according Mokany RSR.Y.SE &lt;- 0.086 RSR.O &lt;- 0.275 RSR.O.SE &lt;- 0.003 RSR.AGB &lt;- 20 # read inventory data # ------------------- plots &lt;- read.xlsx(paste0(DATA.DIR, &quot;/IFN/IFN-Togo-2015.xlsx&quot;), &quot;placettes&quot;)[,1:4] names(plots) &lt;- c(&quot;PlotID&quot;, &quot;X&quot;, &quot;Y&quot;, &quot;LULC&quot;) trees &lt;- read.xlsx(paste0(DATA.DIR, &quot;/IFN/IFN-Togo-2015.xlsx&quot;), &quot;arbres&quot;)[,c(1,4:6,8)] names(trees) &lt;- c(&quot;PlotID&quot;, &quot;Species&quot;, &quot;Status&quot;, &quot;DBH&quot;, &quot;H&quot;) # merge with specific wood densities used by from Fonton et al. 2018 species &lt;- as.data.frame(table(trees$Species)) names(species) &lt;- c(&quot;Species&quot;, &quot;Count&quot;) fonton &lt;- read.csv2(paste0(DATA.DIR, &quot;/IFN/donnees_Tg_Fonton.csv&quot;), encoding=&quot;latin1&quot;)[,c(4,7)] fonton &lt;- aggregate(list(D=fonton$wsg), by=list(Species=fonton$NOM), FUN=modal) species &lt;- merge(species[,c(&quot;Species&quot;, &quot;Count&quot;)], fonton[,c(&quot;Species&quot;, &quot;D&quot;)], by=&quot;Species&quot;) species$Source &lt;- &quot;Fonton&quot; # merge tree table with species densities # ------------------------------------------------------------------------- trees &lt;- merge(trees, species, by=&quot;Species&quot;, all.x=TRUE) # estimer la biomasse des arbres avec la fonction de Chave et al. (2014) # ---------------------------------------------------------------------- trees$AGB &lt;- 0.0673 * (trees$D * trees$DBH^2 * trees$H)^0.976 trees$AGBm &lt;- trees$AGBv &lt;- trees$AGB # copier les valeurs dans une colonne bois mort trees$AGBv[trees$Status != &quot;V&quot;] &lt;- 0 # mettre ?? zero la biomass l?? o?? l&#39;arbre n&#39;est pas vivant trees$AGBm[trees$Status == &quot;V&quot;] &lt;- 0 # mettre ?? zero le bois mort l?? o?? l&#39;arbre est vivant # aggregation de la biomasse est le bois mort par plots (somme) # ---------------------------------------------------------------- plots &lt;- merge(plots, aggregate(trees[,c(&quot;AGBv&quot;, &quot;AGBm&quot;)], by=list(PlotID=trees$PlotID), FUN=function(x) sum(x) * 10000 / PLOT.SIZE / 1000), by=&quot;PlotID&quot;, all.x=TRUE) # joindre tmp avec notre tableau plotss plots$AGBv[is.na(plots$AGBv)] &lt;- 0 # mettre ?? zero la biomasse et le bois mort pour les plotss sans valeurs (NA) plots$AGBm[is.na(plots$AGBm)] &lt;- 0 plots$AGB &lt;- plots$AGBv + plots$AGBm # estimer la biomasse racinaire par plot avec les facteurs root-shoot de Mokany et al. (2006) pour les for??ts tropicales s??ches # ----------------------------------------------------------------------------------------------------------------------------- plots$BGB[plots$AGBv &lt;= RSR.AGB] &lt;- plots$AGBv[plots$AGBv &lt;= RSR.AGB] * RSR.Y plots$BGB[plots$AGBv &gt; RSR.AGB] &lt;- plots$AGBv[plots$AGBv &gt; RSR.AGB] * RSR.O plots$BM &lt;- plots$AGB + plots$BGB # write plot coordinates and biomass to file # ------------------------------------------ write.csv(plots, paste0(AGB.REF.DIR, &quot;/IFN-plots.csv&quot;), row.names = FALSE) #, fileEncoding = &quot;macintosh&quot;) # produire le tableau crois?? avec les biomasses par strate (fonctionalit?? de l&#39;extension dplyr) # --------------------------------------------------------------------------------------------- pdf(paste0(AGB.REF.DIR, &quot;/AGB-vs-LULC.pdf&quot;)) par(mar=c(11,5,1,1), cex.axis=0.7) boxplot(plots$AGBv~plots$LULC, las=2, ylab=&quot;AGB (t/ha)&quot;, xlab=NULL) dev.off() pdf(paste0(AGB.REF.DIR, &quot;/AGBm-vs-LULC.pdf&quot;)) par(mar=c(11,5,1,1), cex.axis=0.7) boxplot(plots$AGBm~plots$LULC, las=2, ylab=&quot;Bmort (t/ha)&quot;, xlab=NULL) dev.off() # biomass per LU/LC category bm.lulc.tab &lt;- plots %&gt;% group_by(LULC) %&gt;% # grouper par strate summarise(n=length(AGB), AGBv.mean=mean(AGBv), AGBv.sd=sd(AGBv), # definir les colonnes et le calcul des valeurs BGB.mean=mean(BGB), BGB.sd=sd(BGB), AGBm.mean=mean(AGBm), AGBm.sd=sd(AGBm), # definir les colonnes et le calcul des valeurs BM.mean =mean(BM), BM.sd =sd(BM)) write.csv(bm.lulc.tab, paste0(AGB.REF.DIR, &quot;/AGB_LULC.csv&quot;), row.names = FALSE) #, fileEncoding = &quot;macintosh&quot;) # differences biomass per LU/LC conversion dbm.lulc.tab &lt;- bm.lulc.tab %&gt;% expand(FROM=nesting(LULC, AGB.mean, AGB.sd, BGB.mean, BGB.sd, BM.mean, BM.sd), TO=nesting(LULC, AGB.mean, AGB.sd, BGB.mean, BGB.sd, BM.mean, BM.sd)) %&gt;% mutate(from = FROM$LULC, to = TO$LULC, dAGB.mean=TO$AGB.mean - FROM$AGB.mean, dAGB.sd=sqrt(FROM$AGB.sd^2 + TO$AGB.sd^2), dBGB.mean=TO$BGB.mean - FROM$BGB.mean, dBGB.sd=sqrt(FROM$BGB.sd^2 + TO$BGB.sd^2), dBM.mean =TO$BM.mean - FROM$BM.mean, dBM.sd =sqrt(FROM$BM.sd^2 + TO$BM.sd^2)) %&gt;% select(c(from, to, dAGB.mean, dAGB.sd, dBGB.mean, dBGB.sd, dBM.mean, dBM.sd)) write.csv(dbm.lulc.tab, paste0(AGB.REF.DIR, &quot;/AGB_LULC-diff.csv&quot;), row.names = FALSE) #, fileEncoding = &quot;macintosh&quot;) "],
["02_create-AGB-maps.html", "", " 4.4.2 Calibration et prédiction Description Description de la méthodologie / script AGB/3_create-AGB-maps.R #################################################################### # NERF_Togo/AGB/3_create-agb-maps.R: create AGB maps for different dates # ------------------------------------------------------------------ # Bern University of Applied Sciences # Oliver Gardi, &lt;oliver.gardi@bfh.ch&gt; # 10 October 2019 ### DEFINITIONS ############################################################ # Default parameters ------------------------------------------------------- AGB.STRATA &lt;- 10 # Number of strata for sampling AGB from reference maps / calibration maps N.PIXELS &lt;- NA # Number of non-NA cells (will be determined later) SAMPLE.RATIO &lt;- 0.001 # Share of non-NA cells to sample CAL.RATIO &lt;- 1 # Use same amount of ref-points from cal.map as from ref.map / train.points PREDICTORS &lt;- c(&quot;B&quot;, &quot;G&quot;, &quot;R&quot;, &quot;NIR&quot;, &quot;SWIR1&quot;, &quot;SWIR2&quot;, &quot;nbr&quot;, &quot;ndmi&quot;, &quot;ndvi&quot;, &quot;evi&quot;, &quot;BIO1&quot;, &quot;BIO4&quot;, &quot;BIO12&quot;, &quot;BIO15&quot;) # &quot;savi&quot;, &quot;nbr2&quot;, &quot;msavi&quot;, &quot;x&quot;, &quot;y&quot; SEED &lt;- 20191114 # Function for loading an image -------------------------------------------- load.image &lt;- function(filename) { image &lt;- brick(paste0(IMAGES.DIR, filename)) names(image) &lt;- BANDS return(image) } # function for creating biomass map based on image and training data ------------ agb.map &lt;- function(image, filename, bioclim=NULL, train.dat=NULL, ref.map=NULL, n.ref.map=NULL, cal.map=NULL, n.cal.map=NULL, mask=NULL, preds=NULL, crossval=FALSE, bias.corr=TRUE, n.cores=8) { name &lt;- sub(&quot;[.]tif$&quot;, &quot;&quot;, filename) txtfile &lt;- paste0(sub(&quot;[.]tif$&quot;, &quot;&quot;, filename), &quot;.txt&quot;) cat(&quot;-- Biomass map: &quot;, basename(filename), &quot;/&quot;, date(), &quot; --\\n&quot;, file=txtfile) # add training data from ref.map if provided if(!is.null(ref.map)) { cat(paste0(&quot; -Masking / buffering reference map ... \\n&quot;)) ref.map &lt;- mask(crop(ref.map, image[[1]]), crop(image[[1]], ref.map)) # crop/mask ref.map with image if(!is.null(mask)) ref.map &lt;- mask(ref.map, mask) # mask with additional mask, if provided if(!is.null(cal.map)) { tmp &lt;- extend(crop(cal.map, ref.map), ref.map) # cut out the piece of the calibration map that overlaps ref map and extend to refmap ref.map &lt;- mask(ref.map, tmp, inverse=TRUE) } cat(&quot; &quot;) cat(paste0(&quot; -Sampling map (n=&quot;, AGB.STRATA, &quot;*&quot;, round(n.ref.map/AGB.STRATA), &quot;) ... &quot;)) ref.pts &lt;- sampleStratified(cut(ref.map, AGB.STRATA), round(n.ref.map/AGB.STRATA), sp=TRUE)[,-1] # stratified sampling (same number of samples for each class) names(ref.pts) &lt;- &quot;AGB&quot; cat(&quot;extracting values ... \\n&quot;) ref.dat &lt;- cbind(AGB=raster::extract(ref.map, ref.pts, df=TRUE)[,-1], raster::extract(image, ref.pts, df=TRUE)[,-1], raster::extract(bioclim, ref.pts, df=TRUE)[,-1]) cat(&quot;Ref-map points: &quot;, nrow(ref.dat), &quot;/&quot;, ref.map@file@name, file=txtfile, append=TRUE) if(is.null(train.dat)) { train.dat &lt;- ref.dat # use it as training points or add to existing training points } else { train.dat &lt;- rbind(train.dat, ref.dat) } } if(!is.null(cal.map)) { cat(paste0(&quot; -Masking calibration map ... \\n&quot;)) cal.map &lt;- mask(crop(cal.map, image[[1]]), crop(image[[1]], cal.map)) # crop/mask ref.map with image if(!is.null(mask)) cal.map &lt;- mask(cal.map, mask) # mask with additional mask, if provided cat(paste0(&quot; -Sampling map (n=&quot;, AGB.STRATA, &quot;*&quot;, round(n.cal.map/AGB.STRATA), &quot;) ... &quot;)) cal.pts &lt;- sampleStratified(cut(cal.map, AGB.STRATA), round(n.cal.map/AGB.STRATA), sp=TRUE)[,-1] # stratified sampling (same number of samples for each class) names(cal.pts) &lt;- &quot;AGB&quot; cat(&quot;extracting values ... \\n&quot;) cal.dat &lt;- cbind(AGB=raster::extract(cal.map, cal.pts, df=TRUE)[,-1], raster::extract(image, cal.pts, df=TRUE)[,-1], raster::extract(bioclim, cal.pts, df=TRUE)[,-1]) if(is.null(train.dat)) { train.dat &lt;- cal.dat # use it as training points or add to existing training points } else { train.dat &lt;- rbind(train.dat, cal.dat) } cat(&quot;Cal-map points: &quot;, nrow(cal.dat), &quot;from&quot;, cal.map@file@name, file=txtfile, append=TRUE) } cat(&quot;Total points: &quot;, nrow(train.dat), &quot;\\n&quot;, file=txtfile, append=TRUE) # extract spectral values if(is.null(preds)) { preds &lt;- names(image) if(!is.null(bioclim)) preds &lt;- c(preds, names(bioclim)) } # cat(&quot; -Extracting pixel values for bands:&quot;, preds, &quot;... &quot;) # train.pts &lt;- raster::extract(image, train.pts, sp=TRUE) # if(!is.null(bioclim)) train.pts &lt;- raster::extract(bioclim, train.pts, sp=TRUE) # train.dat &lt;- na.omit(train.pts@data)[, c(&quot;CLASS&quot;, preds)] # if(type==&quot;classification&quot;) train.dat[,1] &lt;- as.factor(train.dat[,1]) # cat(&quot;done\\n&quot;) # calibrate RandomForest classifier cat(&quot; -Calibrating RandomForest ... &quot;) sink(txtfile, append=TRUE) if(crossval) { map.model.cv &lt;- train(y = train.dat[,1], x = train.dat[,preds], method = &quot;rf&quot;, importance = TRUE, trControl = trainControl( method = &quot;repeatedcv&quot;, number = 10, repeats = 3)) print(map.model.cv) map.model &lt;- map.model.cv$finalModel print(map.model) cat(&quot;\\n&quot;) print(varImp(map.model, scale=FALSE)) } else { map.model &lt;- randomForest(y=train.dat[,1], x=train.dat[,preds], importance=TRUE) # , do.trace=100) # Parallelization of RandomForest: confusion, err.rate, mse and rsq will be NULL # https://stackoverflow.com/questions/14106010/parallel-execution-of-random-forest-in-r # map.model &lt;- foreach(ntree=rep(100, 5), .combine=randomForest::combine, .multicombine=TRUE, .packages=&#39;randomForest&#39;) %dopar% { # randomForest(x=ref.pts[,!(names(ref.pts) == &quot;CLASS&quot;)], y=ref.pts$CLASS, importance=TRUE, ntree=ntree) # print(map.model) cat(&quot;\\n&quot;) print(varImp(map.model)) } sink() cat(&quot;R2:&quot;, round(map.model$rsq[500], 2), &quot;RMSE:&quot;, round(sqrt(map.model$mse[500]), 2), &quot;\\n&quot;) # write model results dir.create(dirname(filename), recursive=TRUE, showWarnings=FALSE) # save RandomForest Model (too large) # save(map.model, file=paste0(sub(&quot;[.]tif$&quot;, &quot;&quot;, filename), &quot;r_rf.RData&quot;)) # classify image cat(&quot; -Creating map ... &quot;) if(!is.null(bioclim)) image &lt;- stack(image, crop(bioclim, image)) beginCluster(n=n.cores) map &lt;- clusterR(image, predict, args=list(model=map.model)) endCluster() pdf(paste0(name, &quot;.pdf&quot;)) plot(train.dat$AGB ~ map.model$predicted, ylab=&quot;AGB (tDM/ha)&quot;, xlab=&quot;Predicted AGB (tDM/ha)&quot;) abline(0,1, lty=2) if(bias.corr) { bc &lt;- lm(train.dat$AGB ~ map.model$predicted) abline(bc, lty=3, col=&quot;red&quot;) } else { bc &lt;- NULL } dev.off() if(bias.corr) { sink(paste0(name, &quot;_bc.txt&quot;), split=TRUE) print(summary(bc)) sink() save(bc, file=paste0(name, &quot;_bc.RData&quot;)) cat(&quot;Applying linear bias correction ...&quot;) map &lt;- calc(map, fun=function(x){bc$coefficients[1] + bc$coefficients[2]*x}) } # save map of classified image cat(&quot;writing map ... &quot;) map &lt;- writeRaster(map, filename=filename, format=&quot;GTiff&quot;, datatype=&quot;INT2U&quot;, overwrite=TRUE) cat(&quot;done\\n&quot;) cat(&quot;-- Done: &quot;, basename(filename), &quot;/&quot;, date(), &quot; --\\n&quot;, file=txtfile, append=TRUE) invisible(list( &quot;rf.model&quot; = map.model, &quot;bc.model&quot; = bc, &quot;map&quot; = map )) } ### DO THE WORK ############################################################ # read images 2015 and bioclim variables --------------------------- ref.p192 &lt;- brick(paste0(IMAGES.DIR, &quot;/p192/p192_2015_m.tif&quot;)) ref.p193 &lt;- brick(paste0(IMAGES.DIR, &quot;/p193/p193_2015_m.tif&quot;)) ref.p194 &lt;- brick(paste0(IMAGES.DIR, &quot;/p194/p194_2015_m.tif&quot;)) names(ref.p192) &lt;- names(ref.p193) &lt;- names(ref.p194) &lt;- BANDS ref.images &lt;- list(p192=ref.p192, p193=ref.p193, p194=ref.p194) N.PIXELS &lt;- list(p192 = ncell(ref.p192[[&quot;B&quot;]]) - summary(ref.p192)[&quot;NA&#39;s&quot;,&quot;B&quot;], p193 = ncell(ref.p193[[&quot;B&quot;]]) - summary(ref.p193)[&quot;NA&#39;s&quot;,&quot;B&quot;], p194 = ncell(ref.p194[[&quot;B&quot;]]) - summary(ref.p194)[&quot;NA&#39;s&quot;,&quot;B&quot;]) bioclim.p192 &lt;- brick(paste0(IMAGES.DIR, &quot;/p192/p192_bioclim.tif&quot;)) bioclim.p193 &lt;- brick(paste0(IMAGES.DIR, &quot;/p193/p193_bioclim.tif&quot;)) bioclim.p194 &lt;- brick(paste0(IMAGES.DIR, &quot;/p194/p194_bioclim.tif&quot;)) names(bioclim.p192) &lt;- names(bioclim.p193) &lt;- names(bioclim.p194) &lt;- BIOCLIM bioclim &lt;- list(p192=bioclim.p192, p193=bioclim.p193, p194=bioclim.p194) # # Load 30m DEM and calculate slope and aspect # dem &lt;- stack(raster(paste0(OUTPUT.DIR, &quot;/1_images/SRTM/SRTM-1arcsec.tif&quot;))) # names(dem) &lt;- &quot;ALT&quot; # dem$SLP &lt;- terrain(dem$ALT, opt=&quot;slope&quot;) # dem$ASP &lt;- terrain(dem$ALT, opt=&quot;aspect&quot;) # # # wc2 &lt;- stack(paste0(OUTPUT.DIR, &quot;/1_images/WCv2/wc2.0_30s_tmin_Togo.tif&quot;), # paste0(OUTPUT.DIR, &quot;/1_images/WCv2/wc2.0_30s_tmax_Togo.tif&quot;), # paste0(OUTPUT.DIR, &quot;/1_images/WCv2/wc2.0_30s_prec_Togo.tif&quot;), # paste0(OUTPUT.DIR, &quot;/1_images/WCv2/wc2.0_30s_bio_Togo.tif&quot;)) # # names(wc2) &lt;- c(paste0(&quot;tmin.&quot;, 1:12), paste0(&quot;tmax.&quot;, 1:12), paste0(&quot;prec.&quot;, 1:12), paste0(&quot;bio.&quot;, 1:19)) # load inventory data ------------------------------------------------------ plots &lt;- read.csv(paste0(AGB.REF.DIR, &quot;/IFN-plots.csv&quot;)) # , fileEncoding=&quot;macintosh&quot;) coordinates(plots) &lt;- ~X+Y proj4string(plots) &lt;- utm.31 pdf(paste0(AGB.REF.DIR, &quot;/IFN-plots_location.pdf&quot;), width=3.5, height=7) par(mar=c(1,1,1,1)) plot(spTransform(TGO, utm.31), col=&quot;lightyellow&quot;) plot(plots, add=TRUE, col=&quot;black&quot;, pch=16, cex=0.3) plot(plots, add=TRUE, col=&quot;darkgreen&quot;, pch=1, cex=plots$AGB/100) dev.off() # convert points to polygons, for extraction of raster values plots.poly &lt;- SpatialPolygonsDataFrame(gBuffer(plots, byid=TRUE, width=20), plots@data) # extract raster values registerDoParallel(.env$numCores-1) x &lt;- foreach(i=1:length(ref.images), .combine=cbind) %:% foreach(j=1:nlayers(ref.images[[i]]), .combine=cbind) %dopar% { raster::extract(ref.images[[i]][[j]], plots.poly, weights=TRUE, fun=mean, df=TRUE)[,2] # weighted means for spectral values } x2 &lt;- foreach(i=1:length(bioclim), .combine=cbind) %:% foreach(j=1:nlayers(bioclim[[i]]), .combine=cbind) %dopar% { raster::extract(bioclim[[i]][[j]], plots, df=TRUE)[,2] # bioclimatic values } dat.p192 &lt;- na.omit(as.data.frame(cbind(plots$AGB, x[,1:13], x2[,1:19]))) dat.p193 &lt;- na.omit(as.data.frame(cbind(plots$AGB, x[,14:26], x2[,20:38]))) dat.p194 &lt;- na.omit(as.data.frame(cbind(plots$AGB, x[,27:39], x2[,39:57]))) names(dat.p192) &lt;- names(dat.p193) &lt;- names(dat.p194) &lt;- c(&quot;AGB&quot;, BANDS, BIOCLIM) train.data &lt;- list(p192=dat.p192, p193=dat.p193, p194=dat.p194) # variable selection ----------------------------- # train.data &lt;- na.omit(x[,c(1, (2*13-13)+2:14, 41:98)]) # use &#39;2&#39; for selecting p193 # # # determine explicative variables # # # TODO: include regeneration map as predictor!! # # var.sel &lt;- rfe(train.data[,2:ncol(train.data)], train.data[,1], # sizes=c(1:15, 20, 50), #sizes=seq(8,26,2), # # rfeControl=rfeControl( # functions=rfFuncs, # method = &quot;repeatedcv&quot;, # number = 10, # repeats = 10 # )) # # sink(paste0(OUTPUT.DIR, &quot;/3_forest-biomass/2_ref-maps/p193_2015_AGB_varsel-fin.txt&quot;), split=TRUE) # predictors(var.sel) # print(var.sel) # sink() # # pdf(paste0(OUTPUT.DIR, &quot;/3_forest-biomass/2_ref-maps/p193_2015_AGB_varsel-fin.pdf&quot;)) # plot(var.sel, type=c(&quot;g&quot;, &quot;o&quot;)) # dev.off() # # # train.data &lt;- train.data[, c(&quot;AGB&quot;, predictors(var.sel))] # # # use Landsat bands + ndvi + annual mean temp, temp seasonality, annual prec and prec seasonality # train.data &lt;- train.data[, c(&quot;AGB&quot;, # &quot;B&quot;, &quot;G&quot;, &quot;R&quot;, &quot;NIR&quot;, &quot;SWIR1&quot;, &quot;SWIR2&quot;, &quot;ndvi&quot;, # paste0(&quot;bio.&quot;, c(1, 4, 12, 15)) # )] # create reference biomass maps 2015 ------------------------------ set.seed(SEED) p193.2015.agb &lt;- agb.map(image = load.image(&quot;/p193/p193_2015_m.tif&quot;), bioclim = bioclim[[&quot;p193&quot;]], filename = paste0(AGB.REF.DIR, &quot;/p193_2015_AGB_R.tif&quot;), train.dat = train.data[[&quot;p193&quot;]], preds = PREDICTORS, crossval = TRUE, bias.corr = FALSE, n.cores = 32) set.seed(SEED) p192.2015.agb &lt;- agb.map(image = load.image(&quot;/p192/p192_2015_m.tif&quot;), bioclim = bioclim[[&quot;p192&quot;]], filename = paste0(AGB.REF.DIR, &quot;/p192_2015_AGB_R.tif&quot;), train.dat = train.data[[&quot;p192&quot;]], cal.map = raster(paste0(AGB.REF.DIR, &quot;/p193_2015_AGB_R.tif&quot;)), n.cal.map = max(200, nrow(train.data[[&quot;p192&quot;]])*CAL.RATIO), preds = PREDICTORS, crossval = FALSE, bias.corr = FALSE, n.cores = 32, mask = TGO) set.seed(SEED) p194.2015.agb &lt;- agb.map(image = load.image(&quot;/p194/p194_2015_m.tif&quot;), bioclim = bioclim[[&quot;p194&quot;]], filename = paste0(AGB.REF.DIR, &quot;/p194_2015_AGB_R.tif&quot;), train.dat = train.data[[&quot;p194&quot;]], cal.map = raster(paste0(AGB.REF.DIR, &quot;/p193_2015_AGB_R.tif&quot;)), n.cal.map = max(200, nrow(train.data[[&quot;p194&quot;]])*CAL.RATIO), preds = PREDICTORS, crossval = FALSE, bias.corr = FALSE, n.cores = 32, mask = TGO) # merge the three maps agb.2015 &lt;- mask(crop(mosaic(raster(paste0(AGB.REF.DIR, &quot;/p192_2015_AGB_R.tif&quot;)), raster(paste0(AGB.REF.DIR, &quot;/p193_2015_AGB_R.tif&quot;)), raster(paste0(AGB.REF.DIR, &quot;/p194_2015_AGB_R.tif&quot;)), fun=mean), TGO), TGO, filename=paste0(AGB.REF.DIR, &quot;/TGO_2015_AGB_R.tif&quot;), overwrite=TRUE) # plot biomass map library(RColorBrewer) pdf(paste0(AGB.REF.DIR, &quot;/TGO_2015_AGB_R.pdf&quot;), width=3.5, height=7) par(mar=c(1,1,1,1)) plot(agb.2015, axes=FALSE, col=brewer.pal(9, &quot;YlGn&quot;), zlim=c(0,220)) plot(spTransform(TGO, utm.31), add=TRUE) dev.off() # compare resulting biomass map with the IFN AGB values agb.pred &lt;- raster::extract(agb.2015, plots.poly, weights=TRUE, fun=mean, df=TRUE)[,2] # weighted means for spectral values pdf(paste0(AGB.REF.DIR, &quot;/AGB-model_2015.pdf&quot;)) plot(agb.pred ~ plots.poly$AGB, xlab=&quot;Biomasse aérienne IFN (t/ha)&quot;, ylab=&quot;Carte AGB 2015 (t/ha)&quot;, xlim=c(0,350), ylim=c(0,350)) abline(0, 1, lty=&quot;dashed&quot;) dev.off() plots.poly$AGB.pred &lt;- agb.pred # bias correction train.control &lt;- trainControl(method = &quot;cv&quot;, number = 10) cv &lt;- train(AGB ~ AGB.pred, data=plots.poly@data[!is.na(plots.poly$AGB.pred),], method = &quot;lm&quot;, trControl = train.control) model &lt;- lm(AGB ~ AGB.pred, data=plots.poly@data[!is.na(plots.poly$AGB.pred),]) sink(paste0(AGB.REF.DIR, &quot;/TGO_2015_AGB_Rc.txt&quot;), split=TRUE) print(cv) summary(model) sink() agb.2015 &lt;- writeRaster(model$coefficients[&quot;(Intercept)&quot;] + model$coefficients[&quot;AGB.pred&quot;] * agb.2015, paste0(AGB.REF.DIR, &quot;/TGO_2015_AGB_R.tif&quot;), overwrite=TRUE) # compare bias corrected biomass map with the IFN AGB values agb.pred.c &lt;- raster::extract(agb.2015, plots.poly, weights=TRUE, fun=mean, df=TRUE)[,2] # weighted means for spectral values pdf(paste0(AGB.REF.DIR, &quot;/AGB-model_2015_c.pdf&quot;)) plot(agb.pred ~ plots.poly$AGB, xlab=&quot;Biomasse aérienne IFN (t/ha)&quot;, ylab=&quot;Carte AGB 2015 (t/ha)&quot;, xlim=c(0,350), ylim=c(0,350)) abline(0, 1, lty=&quot;dashed&quot;) dev.off() # create reference biomass maps 2003 ------------------------------ set.seed(SEED) p193.2003.agb &lt;- agb.map(image = load.image(&quot;/p193/p193_2003_m.tif&quot;), bioclim = bioclim[[&quot;p193&quot;]], filename = paste0(AGB.REF.DIR, &quot;/p193_2003_AGB_R.tif&quot;), train.dat = NULL, ref.map = raster(paste0(AGB.REF.DIR, &quot;/TGO_2015_AGB_R.tif&quot;)), n.ref.map = SAMPLE.RATIO * N.PIXELS[[&quot;p193&quot;]], preds = PREDICTORS, crossval = FALSE, bias.corr = TRUE, n.cores = 32) set.seed(SEED) p192.2003.agb &lt;- agb.map(image = load.image(&quot;/p192/p192_2003_m.tif&quot;), bioclim = bioclim[[&quot;p192&quot;]], filename = paste0(AGB.REF.DIR, &quot;/p192_2003_AGB_R.tif&quot;), train.dat = NULL, ref.map = raster(paste0(AGB.REF.DIR, &quot;/TGO_2015_AGB_R.tif&quot;)), n.ref.map = SAMPLE.RATIO * N.PIXELS[[&quot;p192&quot;]] / (1 + CAL.RATIO), cal.map = raster(paste0(AGB.REF.DIR, &quot;/p193_2003_AGB_R.tif&quot;)), n.cal.map = SAMPLE.RATIO * N.PIXELS[[&quot;p192&quot;]] / (1 + 1/CAL.RATIO), preds = PREDICTORS, crossval = FALSE, bias.corr = FALSE, n.cores = 32, mask = TGO) set.seed(SEED) p194.2003.agb &lt;- agb.map(image = load.image(&quot;/p194/p194_2003_m.tif&quot;), bioclim = bioclim[[&quot;p194&quot;]], filename = paste0(AGB.REF.DIR, &quot;/p194_2003_AGB_R.tif&quot;), train.dat = NULL, ref.map = raster(paste0(AGB.REF.DIR, &quot;/TGO_2015_AGB_R.tif&quot;)), n.ref.map = SAMPLE.RATIO * N.PIXELS[[&quot;p194&quot;]] / (1 + CAL.RATIO), cal.map = raster(paste0(AGB.REF.DIR, &quot;/p193_2003_AGB_R.tif&quot;)), n.cal.map = SAMPLE.RATIO * N.PIXELS[[&quot;p194&quot;]] / (1 + 1/CAL.RATIO), preds = PREDICTORS, crossval = FALSE, bias.corr = FALSE, n.cores = 32, mask = TGO) # merge the three maps agb.2003 &lt;- mask(crop(mosaic(raster(paste0(AGB.REF.DIR, &quot;/p192_2003_AGB_R.tif&quot;)), raster(paste0(AGB.REF.DIR, &quot;/p193_2003_AGB_R.tif&quot;)), raster(paste0(AGB.REF.DIR, &quot;/p194_2003_AGB_R.tif&quot;)), fun=mean), TGO), TGO, filename=paste0(AGB.REF.DIR, &quot;/TGO_2003_AGB_R.tif&quot;), overwrite=TRUE) # create reference biomass maps 2018 ------------------------------ set.seed(SEED) p193.2018.agb &lt;- agb.map(image = load.image(&quot;/p193/p193_2018_m.tif&quot;), bioclim = bioclim[[&quot;p193&quot;]], filename = paste0(AGB.REF.DIR, &quot;/p193_2018_AGB_R.tif&quot;), train.dat = NULL, ref.map = raster(paste0(AGB.REF.DIR, &quot;/TGO_2015_AGB_R.tif&quot;)), n.ref.map = SAMPLE.RATIO * N.PIXELS[[&quot;p193&quot;]], preds = PREDICTORS, crossval = FALSE, bias.corr = TRUE, n.cores = 32) set.seed(SEED) p192.2018.agb &lt;- agb.map(image = load.image(&quot;/p192/p192_2018_m.tif&quot;), bioclim = bioclim[[&quot;p192&quot;]], filename = paste0(AGB.REF.DIR, &quot;/p192_2018_AGB_R.tif&quot;), train.dat = NULL, ref.map = raster(paste0(AGB.REF.DIR, &quot;/TGO_2015_AGB_R.tif&quot;)), n.ref.map = SAMPLE.RATIO * N.PIXELS[[&quot;p192&quot;]] / (1 + CAL.RATIO), cal.map = raster(paste0(AGB.REF.DIR, &quot;/p193_2018_AGB_R.tif&quot;)), n.cal.map = SAMPLE.RATIO * N.PIXELS[[&quot;p192&quot;]] / (1 + 1/CAL.RATIO), preds = PREDICTORS, crossval = FALSE, bias.corr = FALSE, n.cores = 32, mask = TGO) set.seed(SEED) p194.2018.agb &lt;- agb.map(image = load.image(&quot;/p194/p194_2018_m.tif&quot;), bioclim = bioclim[[&quot;p194&quot;]], filename = paste0(AGB.REF.DIR, &quot;/p194_2018_AGB_R.tif&quot;), train.dat = NULL, ref.map = raster(paste0(AGB.REF.DIR, &quot;/TGO_2015_AGB_R.tif&quot;)), n.ref.map = SAMPLE.RATIO * N.PIXELS[[&quot;p194&quot;]] / (1 + CAL.RATIO), cal.map = raster(paste0(AGB.REF.DIR, &quot;/p193_2018_AGB_R.tif&quot;)), n.cal.map = SAMPLE.RATIO * N.PIXELS[[&quot;p194&quot;]] / (1 + 1/CAL.RATIO), preds = PREDICTORS, crossval = FALSE, bias.corr = FALSE, n.cores = 32, mask = TGO) # merge the three maps agb.2018 &lt;- mask(crop(mosaic(raster(paste0(AGB.REF.DIR, &quot;/p192_2018_AGB_R.tif&quot;)), raster(paste0(AGB.REF.DIR, &quot;/p193_2018_AGB_R.tif&quot;)), raster(paste0(AGB.REF.DIR, &quot;/p194_2018_AGB_R.tif&quot;)), fun=mean), TGO), TGO, filename=paste0(AGB.REF.DIR, &quot;/TGO_2018_AGB_R.tif&quot;), overwrite=TRUE) # # biomass maps for p193 -------------------------------------- # # registerDoParallel(.env$numCores-1) # foreach(file=dir(paste0(IMAGES.DIR, &quot;/p193&quot;), pattern=&quot;\\\\_[[:digit:]]+\\\\_m\\\\.tif&quot;)) %dopar% { # #foreach(file=c(&quot;p193_1987.tif&quot;, &quot;p193_2003.tif&quot;, &quot;p193_2015.tif&quot;, &quot;p193_2018.tif&quot;)) %dopar% { # # foreach(file=c(&quot;p193_1985.tif&quot;, &quot;p193_1990_1.tif&quot;, &quot;p193_1990_2.tif&quot;, &quot;p193_2000.tif&quot;, &quot;p193_2005.tif&quot;, &quot;p193_2007.tif&quot;, &quot;p193_2009.tif&quot;, &quot;p193_2013.tif&quot;, &quot;p193_2017.tif&quot;, &quot;p193_2019.tif&quot;)) %dopar% { # # agb.map(image = load.image(paste0(&quot;/p193/&quot;, file)), # bioclim = bioclim[[&quot;p193&quot;]], # filename = paste0(BIOMASS.DIR, &quot;/3_raw-maps/p193/&quot;, sub(&quot;\\\\.tif$&quot;, &quot;r.tif&quot;, file)), # ref.map = raster(paste0(BIOMASS.DIR, &quot;/2_ref-maps/p193_2003_AGB_R.tif&quot;)), # n.ref.map = SAMPLE.RATIO * N.PIXELS[[&quot;p193&quot;]], # preds = PREDICTORS, # mask = TGO, # n.cores = 6) # # n.cores = 32) # } # # # merge the two p193_1990 tiles # merge(raster(paste0(BIOMASS.DIR, &quot;/3_raw-maps/p193/p193_1990_1_mr.tif&quot;)), # raster(paste0(BIOMASS.DIR, &quot;/3_raw-maps/p193/p193_1990_2_mr.tif&quot;)), # filename=paste0(BIOMASS.DIR, &quot;/3_raw-maps/p193/p193_1990_mr.tif&quot;), format=&quot;GTiff&quot;, datatype=&quot;INT2U&quot;, overwrite=TRUE) # # # # biomass maps for p192 and p194 ---------------------- # # registerDoParallel(.env$numCores-1) # foreach(file=c(dir(paste0(IMAGES.DIR, &quot;/p192&quot;), pattern=&quot;\\\\_[[:digit:]]+\\\\_m\\\\.tif&quot;), # dir(paste0(IMAGES.DIR, &quot;/p194&quot;), pattern=&quot;\\\\_[[:digit:]]+\\\\_m\\\\.tif&quot;))) %dopar% { # # path &lt;- sub(&quot;\\\\_.*&quot;, &quot;&quot;, file) # # if(file.exists(paste0(BIOMASS.DIR, &quot;/3_raw-maps/p193/&quot;, sub(&quot;\\\\.tif$&quot;, &quot;r.tif&quot;, sub(path, &quot;p193&quot;, file))))) { # cal.map &lt;- raster(paste0(BIOMASS.DIR, &quot;/3_raw-maps/p193/&quot;, sub(&quot;\\\\.tif$&quot;, &quot;r.tif&quot;, sub(path, &quot;p193&quot;, file)))) # n.cal.map &lt;- SAMPLE.RATIO * N.PIXELS[[path]] / (1 + 1/CAL.RATIO) # n.ref.map &lt;- SAMPLE.RATIO * N.PIXELS[[path]] / (1 + CAL.RATIO) # } else { # cal.map &lt;- NULL # n.cal.map &lt;- NULL # n.ref.map &lt;- SAMPLE.RATIO * N.PIXELS[[path]] # } # # agb.map(image = load.image(paste0(&quot;/&quot;, path, &quot;/&quot;, file)), # bioclim = bioclim[[path]], # filename = paste0(BIOMASS.DIR, &quot;/3_raw-maps/&quot;, path, &quot;/&quot;, sub(&quot;\\\\.tif$&quot;, &quot;r.tif&quot;, file)), # ref.map = raster(paste0(BIOMASS.DIR, &quot;/2_ref-maps/&quot;, path, &quot;_2003_AGB_R.tif&quot;)), # n.ref.map = n.ref.map, # cal.map = cal.map, # n.cal.map = n.cal.map, # preds = PREDICTORS, # mask = TGO, # n.cores = 32) # } "],
["03_clean-AGB-maps.html", "", " 4.4.3 Nettoyage des cartes brutes Description Description de la méthodologie / script AGB/4_clean-AGB-maps.R #################################################################### # NERF_Togo/AGB/4_clean-agb-maps.R: clean AGB raw maps # ------------------------------------------------------------------ # Bern University of Applied Sciences # Oliver Gardi, &lt;oliver.gardi@bfh.ch&gt; # 10 October 2019 ### DEFINITIONS ############################################################ PERIOD &lt;- 1985:2019 path &lt;- &quot;p193&quot; # LOESS smoothing of time series loess.filter &lt;- function (agb, years, pred, span) { ifelse( all(is.na(agb)), return(rep(NA, length(pred))), return(predict(loess(agb ~ years, degree = 1, span = span), pred) )) } clean.agb &lt;- function(path) { # Preparation ------------------------------------------------------------- maps &lt;- stack(dir(paste0(BIOMASS.DIR, &quot;/3_raw-maps/&quot;, path), pattern=&quot;.*[[:digit:]]{4}\\\\_mr\\\\.tif$&quot;, full.names=TRUE)) map.names &lt;- sub(&quot;r$&quot;, &quot;&quot;, names(maps)) map.cols &lt;- sub(paste0(path, &quot;\\\\_&quot;), &quot;X&quot;, sub(&quot;\\\\_[[:alnum:]]+$&quot;, &quot;M&quot;, map.names)) map.years &lt;- as.numeric(substr(map.cols, 2, 5)) maps.values &lt;- values(maps) # extract vector with cell values (huge matrix, takes some time) colnames(maps.values) &lt;- map.cols nsubsets &lt;- numCores - 1 # define subsets for parallel processing subsets &lt;- c(0, floor((1:nsubsets)*(nrow(maps.values)/nsubsets))) # Parallel cleaning of pixel trajectories ############################### registerDoParallel(.env$numCores-1) loess.dat &lt;- foreach(i=1:nsubsets, .combine=rbind) %dopar% { val &lt;- maps.values[(subsets[i]+1):subsets[i+1], ] # get one tile of the matrix t(apply(val, 1, loess.filter, years=map.years, pred=map.years, span=2)) } maps.loess &lt;- maps maps.loess[] &lt;- loess.dat writeRaster(maps.loess, paste0(BIOMASS.DIR, &quot;/4_clean-maps/&quot;, path, &quot;_loess.tif&quot;), overwrite=TRUE) } ################################ dates &lt;- c(1987, 1991, 2000, 2003, 2005, 2007, 2008, 2009, 2010, 2012, 2013, 2015, 2017, 2018, 2019) for(year in dates) { print(paste(&quot;Creating AGB map&quot;, year, &quot;...&quot;)) image &lt;- dir(path=&quot;../results/images/&quot;, pattern=paste0(&quot;^&quot;, year, &quot;.*_L2.tif$&quot;), full.names=TRUE) base &lt;- paste0(&quot;../results/carbone/AGB_&quot;, year, &quot;_2015&quot;) biomass.map &lt;- create.biomass.map(ref = raster(&quot;../results/carbone/AGB_2015_REF_corr.tif&quot;), x = brick(image)) writeRaster(biomass.map$map, filename = paste0(base, &quot;.tif&quot;), overwrite=TRUE) writeRaster(biomass.map$map.bias.corr, filename = paste0(base, &quot;_corr.tif&quot;), overwrite=TRUE) sink( file = paste0(base, &quot;_models.txt&quot;)) print(biomass.map$rf); cat(&quot;\\n&quot;) print(importance(biomass.map$rf)); cat(&quot;\\n&quot;) print(summary(biomass.map$lm.bias.corr)) sink() } # Show the effect of bias correction ggplot(biomass.map$sample.pts@data, aes(y=ref.agb, x=agb)) + geom_point(alpha=0.01) + geom_abline(slope=1, linetype=3, alpha=0.9) + geom_abline(intercept = biomass.map$lm.bias.corr$coefficients[1], slope = biomass.map$lm.bias.corr$coefficients[2], linetype=2, alpha=0.9) + theme_light() # load the raw carbon maps and do some analysis # ≠≠≠≠≠≠≠≠≠≠≠≠≠≠≠≠≠≠≠≠≠≠≠≠≠≠≠≠≠≠≠≠≠≠≠≠≠≠≠≠≠≠≠≠≠ files &lt;- dir(path=&quot;../results/carbone&quot;, pattern=&quot;^.*2015.tif$&quot;, full.names=TRUE) maps &lt;- stack(files) names(maps) &lt;- dates maps.dat &lt;- as.data.frame(na.omit(maps[])) dens.plot &lt;- maps.dat %&gt;% gather(variable, value, X1987, X1991, X2000, X2003, X2008, X2013, X2015, X2018) %&gt;% ggplot(aes(x=value, color=variable, linetype=variable)) + geom_density() + theme_light() print(dens.plot) pdf(&quot;../results/carbone/densities_all.pdf&quot;) print(dens.plot) dev.off() # Validation of loess-smoothed 2015 map.2015 &lt;- stack(&quot;../results/carbone/level2/AGBmaps_loess.tif&quot;)[[12]] map.2003 &lt;- stack(&quot;../results/carbone/level2/AGBmaps_loess.tif&quot;)[[4]] plots &lt;- raster::extract(map.2015, plots, sp=TRUE, fun=mean, buffer=20, normalizeWeights=TRUE) plots &lt;- raster::extract(map.2003, plots, sp=TRUE, fun=mean, buffer=20, normalizeWeights=TRUE) names(plots) tmp &lt;- plots@data[,c(&quot;BaHa&quot;, &quot;AGBmaps_loess.12&quot;, &quot;AGBmaps_loess.4&quot;)] tmp$cex &lt;- 1+abs(tmp$AGBmaps_loess.12-tmp$AGBmaps_loess.4)/30 tmp$col[tmp$AGBmaps_loess.12-tmp$AGBmaps_loess.4 &lt;= 0] &lt;- &quot;red&quot; tmp$col[tmp$AGBmaps_loess.12-tmp$AGBmaps_loess.4 &gt; 0] &lt;- &quot;blue&quot; pdf(&quot;../results/carbone/figures/Validation_LoessMap2015.pdf&quot;) plot(tmp$BaHa, tmp$AGBmaps_loess.12, main=&quot;AGB Carbon Stocks on Permanent Inventory Plots&quot;, xlab=&quot;National Forest Inventory 2015 (tC/ha)&quot;, ylab=&quot;LOESS-smoothed Carbon Map 2015&quot;) points(tmp$BaHa[tmp$cex &gt; 1.7], tmp$AGBmaps_loess.12[tmp$cex &gt; 1.7], cex=tmp$cex[tmp$cex &gt; 1.7], col=tmp$col[tmp$cex &gt; 1.7]) abline(0,1, lty=2) abline(0,0.5, lty=3) abline(0,1/0.5, lty=3) legend(&quot;bottomright&quot;, col=c(&quot;red&quot;, &quot;blue&quot;), pch=1, legend=c(&quot;Loss of &gt;= 20 tC since 2003&quot;, &quot;Gain of &gt;= 20 tC since 2003&quot;)) dev.off() pdf(&quot;../results/carbone/figures/Validation_LoessMap2015_Residuals.pdf&quot;) plot(tmp$AGBmaps_loess.12-tmp$BaHa~tmp$BaHa, main=&quot;AGB Carbon Stocks on Permanent Inventory Plots&quot;, xlab=&quot;National Forest Inventory 2015 (tC/ha)&quot;, ylab=&quot;Residuals of LOESS-smoothed Carbon Map 2015&quot;) points(tmp$BaHa[tmp$cex &gt; 1.7], tmp$AGBmaps_loess.12[tmp$cex &gt; 1.7]-tmp$BaHa[tmp$cex &gt; 1.7], cex=tmp$cex[tmp$cex &gt; 1.7], col=tmp$col[tmp$cex &gt; 1.7]) bias.corr &lt;- lm(tmp$AGBmaps_loess.12-tmp$BaHa~tmp$BaHa) abline(0,0) abline(bias.corr, lty=2) legend(&quot;topright&quot;, col=c(&quot;red&quot;, &quot;blue&quot;), pch=1, legend=c(&quot;Loss of &gt;= 20 tC since 2003&quot;, &quot;Gain of &gt;= 20 tC since 2003&quot;)) dev.off() # Load the cleaned maps and create tables # ======================================= biomass.tabs &lt;- function(map, aoi=NULL) { if(!is.null(aoi)) map &lt;- crop(map, aoi) agb.dat &lt;- map[] agb.change.dat &lt;- agb.dat[,2:ncol(agb.dat)] - agb.dat[,1:ncol(agb.dat)-1] agb.loss.dat &lt;- agb.change.dat; agb.loss.dat[agb.loss.dat &gt; 0] &lt;- 0 agb.gain.dat &lt;- agb.change.dat; agb.gain.dat[agb.gain.dat &lt; 0] &lt;- 0 dates &lt;- as.numeric(substr(names(map), 2, 5)) agb.tab &lt;- data.frame(year = dates, total.agb = colSums(agb.dat, na.rm=TRUE) * 30^2/10000, period = c(NA, dates[-1] - dates[-length(dates)]), center = c(NA, (dates[-1] + dates[-length(dates)])/2)) agb.tab$loss.t.yr &lt;- c(NA, colSums(agb.loss.dat, na.rm=TRUE) * 30^2/10000)/agb.tab$period agb.tab$gain.t.yr &lt;- c(NA, colSums(agb.gain.dat, na.rm=TRUE) * 30^2/10000)/agb.tab$period agb.tab$netchange.t.yr &lt;- agb.tab$loss.t.yr + agb.tab$gain.t.yr agb.tab$loss.pc.yr &lt;- round(100 * c(NA, 1/agb.tab$period[2:(nrow(agb.tab))] * log((agb.tab$total.agb[1:nrow(agb.tab)-1] + agb.tab$period[2:(nrow(agb.tab))] * agb.tab$loss.t.yr[2:(nrow(agb.tab))]) / agb.tab$total.agb[1:nrow(agb.tab)-1])), 3) agb.tab$gain.pc.yr &lt;- round(100 * c(NA, 1/agb.tab$period[2:(nrow(agb.tab))] * log((agb.tab$total.agb[1:nrow(agb.tab)-1] + agb.tab$period[2:(nrow(agb.tab))] * agb.tab$gain.t.yr[2:(nrow(agb.tab))]) / agb.tab$total.agb[1:nrow(agb.tab)-1])), 3) agb.tab$netchange.pc.yr &lt;- round(100 * c(NA, 1/agb.tab$period[2:(nrow(agb.tab))] * log( agb.tab$total.agb[2:nrow(agb.tab)] / agb.tab$total.agb[1:nrow(agb.tab)-1])), 3) return(agb.tab) } plot.biomass &lt;- function(fc, zone, filename=NULL) { if(zone==&quot;Zone IV&quot;) { title &lt;- &quot;Net and Gross Changes in Forest Biomass 1987 - 2019 in Ecological Zone IV&quot; ylim &lt;- c(-750000, 750000) ybreaks &lt;- seq(-750000, 750000, 100000) ymbreaks &lt;- seq(-750000, 750000, 50000) } if (zone==&quot;AOI.1&quot;) { title &lt;- &quot;Net and Gross Changes in Forest Biomass 1987 - 2019 in Plaine de Litimé (AOI.1)&quot; ylim &lt;- c(-18000, 15000) ybreaks &lt;- seq(-18000, 15000, 2000) ymbreaks &lt;- seq(-18000, 15000, 1000) } if (zone==&quot;AOI.2&quot;) { title &lt;- &quot;Net and Gross Changes in Forest Biomass 1987 - 2019 in Kpélé (AOI.2)&quot; ylim &lt;- c(-18000, 15000) ybreaks &lt;- seq(-18000, 15000, 2000) ymbreaks &lt;- seq(-18000, 15000, 1000) } if(!is.null(filename)) pdf(filename) fc$netloss.t.yr &lt;- fc$netchange.t.yr fc$netloss.t.yr[fc$netloss.t.yr &gt; 0] &lt;- 0 fc$netgain.t.yr &lt;- fc$netchange.t.yr fc$netgain.t.yr[fc$netgain.t.yr &lt; 0] &lt;- 0 print( fc[-1,] %&gt;% gather(variable, value, loss.t.yr, gain.t.yr, netloss.t.yr, netgain.t.yr) %&gt;% ggplot(aes(y=value, x = center, width=period-0.7)) + geom_bar(data=. %&gt;% filter(variable %in% c(&quot;loss.t.yr&quot;, &quot;gain.t.yr&quot;)), aes(fill=variable, alpha=0.9), stat = &quot;identity&quot;, show.legend=FALSE) + guides(alpha = FALSE) + geom_bar(data=. %&gt;% filter(variable %in% c(&quot;netloss.t.yr&quot;, &quot;netgain.t.yr&quot;)), aes(fill=variable), stat = &quot;identity&quot;) + scale_fill_manual(name = NULL, breaks = c(&quot;gain.t.yr&quot;, &quot;loss.t.yr&quot;), values=c(&quot;#00BFC4&quot;, &quot;#F8766D&quot;, &quot;#00BFC4&quot;, &quot;#F8766D&quot;), labels=c(&quot;Biomass gain &quot;, &quot;Biomass loss &quot;)) + xlab(&quot;Year&quot;) + ylab(&quot;Tonnes AGB per Year&quot;) + ggtitle(title) + scale_x_continuous(minor_breaks = NULL, breaks=c(1987, 2000, 2005, 2010, 2015, 2019)) + scale_y_continuous(breaks=ybreaks, minor_breaks = ymbreaks) + coord_cartesian(ylim=ylim) + theme_light() + theme(legend.position=c(0,1), legend.box = &quot;horizontal&quot;, legend.justification=c(-0.2,1.2)) ) if(!is.null(filename)) dev.off() } maps &lt;- brick(&quot;../results/carbone/level2/AGBmaps_loess.tif&quot;) names(maps) &lt;- dates maps.dat &lt;- as.data.frame(na.omit(maps[])) dens.plot &lt;- maps.dat %&gt;% gather(variable, value, X1991, X2000, X2005, X2010, X2015, X2018) %&gt;% ggplot(aes(x=value, color=variable, linetype=variable)) + geom_density() + theme_light() print(dens.plot) pdf(&quot;../results/carbone/figures/densities.pdf&quot;) print(dens.plot) dev.off() res.all &lt;- biomass.tabs(map=maps) res &lt;- biomass.tabs(map=maps[[c(&quot;X1991&quot;, &quot;X2000&quot;, &quot;X2005&quot;, &quot;X2010&quot;, &quot;X2015&quot;, &quot;X2018&quot;)]]) res.aoi1.all &lt;- biomass.tabs(map=maps, AOI.1) res.aoi1 &lt;- biomass.tabs(map=maps[[c(&quot;X1991&quot;, &quot;X2000&quot;, &quot;X2005&quot;, &quot;X2010&quot;, &quot;X2015&quot;, &quot;X2018&quot;)]], AOI.1) res.aoi2.all &lt;- biomass.tabs(map=maps, AOI.2) res.aoi2 &lt;- biomass.tabs(map=maps[[c(&quot;X1991&quot;, &quot;X2000&quot;, &quot;X2005&quot;, &quot;X2010&quot;, &quot;X2015&quot;, &quot;X2018&quot;)]], AOI.2) res.tmp &lt;- biomass.tabs(map=maps[[c(&quot;X1991&quot;, &quot;X2003&quot;, &quot;X2018&quot;)]]) plot.biomass(res, &quot;Zone IV&quot;, &quot;../results/carbone/figures/biomass.pdf&quot;) plot.biomass(res.aoi1, &quot;AOI.1&quot;, &quot;../results/carbone/figures/biomass_aoi1.pdf&quot;) plot.biomass(res.aoi2, &quot;AOI.2&quot;, &quot;../results/carbone/figures/biomass_aoi2.pdf&quot;) write.xlsx(list(&quot;Total&quot; = res, &quot;AOI.1&quot; = res.aoi1, &quot;AOI.2&quot; = res.aoi2), file = &quot;../results/carbone/figures/biomass-change.xlsx&quot;) Y.diff &lt;- raster(&quot;../results/carbone/AGB_2018_2015_corr.tif&quot;) - Y.corr Y.gain &lt;- Y.diff; Y.gain[Y.gain&lt;=0] &lt;- 0 Y.loss &lt;- Y.diff; Y.loss[Y.loss&gt;=0] &lt;- 0 Y.diff &lt;- writeRaster(round(Y.diff), &quot;../results/carbone/AGB_2018_1991_diff.tif&quot;, datatype=&quot;INT2S&quot;, overwrite=TRUE) -sum(Y.loss[], na.rm=TRUE)/sum(raster(&quot;../results/carbone/AGB_2003_2015_corr.tif&quot;)[], na.rm=TRUE)/15 sum(Y.gain[], na.rm=TRUE)/sum(raster(&quot;../results/carbone/AGB_2003_2015_corr.tif&quot;)[], na.rm=TRUE)/15 dat.c &lt;- data.frame(X1991=raster(&quot;../results/carbone/AGB_1991_2015_corr.tif&quot;)[], X2003=raster(&quot;../results/carbone/AGB_2003_2015_corr.tif&quot;)[], X2018=raster(&quot;../results/carbone/AGB_2018_2015_corr.tif&quot;)[]) dat.c &lt;- na.omit(dat.c) dat.c %&gt;% gather(variable, value, X1991:X2018) %&gt;% ggplot(aes(y=value, x=variable, color=variable)) + geom_boxplot() + coord_flip() + theme_light() pdf(&quot;../results/carbone/densities_91-03-18.pdf&quot;) dat.c %&gt;% gather(variable, value, X1991:X2018) %&gt;% ggplot(aes(x=value, color=variable)) + geom_density() + theme_light() dev.off() # verify the map val.points &lt;- raster::extract(Y.corr, dat.placette, sp=TRUE, fun=mean, buffer=20, normalizeWeights=TRUE) val.points &lt;- raster::extract(Y.2003.corr, val.points, sp=TRUE, fun=mean, buffer=20, normalizeWeights=TRUE) val.points &lt;- raster::extract(Y.2015, val.points, sp=TRUE, fun=mean, buffer=20, normalizeWeights=TRUE) plot(val.points$BaHa, val.points$AGB_2015_2003_corr) abline(0,1) mod &lt;- lm(AGB_2015_2003_corr ~ BaHa, data=val.points) abline(mod, lty=2) ############ compare 2015 carbon map with the forest change map ####################### map.carb &lt;- raster(&quot;../results/carbone/AGB_2015_REF.tif&quot;) map.gain &lt;- raster(&quot;../results/fcc/3_alldates-direct2003/level3/forest-gain.tif&quot;) map.loss &lt;- raster(&quot;../results/fcc/3_alldates-direct2003/level3/forest-loss.tif&quot;) map.loss.a2015 &lt;- map.loss; map.loss.a2015[map.loss.a2015&lt;2015] &lt;- NA map.carb.loss.a2015 &lt;- mask(map.carb, map.loss.a2015) hist(map.carb.loss.a2015[]); summary(map.carb.loss.a2015[]); sum(map.carb.loss.a2015[], na.rm=TRUE)*(30*30/10000) map.gain.b2015 &lt;- map.gain; map.gain.b2015[map.gain.b2015&gt;=2015] &lt;- NA map.gain.b2015[] &lt;- 2015 - map.gain.b2015[] map.carb.gain.b2015 &lt;- mask(map.carb, map.gain.b2015) hist(map.carb.gain.b2015[]); summary(map.carb.gain.b2015[]); sum(map.carb.gain.b2015[], na.rm=TRUE)*(30*30/10000) boxplot(map.carb.gain.b2015[] ~ map.gain.b2015[]) map.carb.loss.a2015.2 &lt;- mask(Y.loss, map.loss.a2015) # ######### Try with linear regression ############ # X$evi &lt;- evi(nir=X$NIR, r=X$R, b=X$B) # X$ndvi &lt;- nd(X$NIR, X$R) # X$ndmi &lt;- nd(X$NIR, X$SWIR1) # X$nbr &lt;- nd(X$NIR, X$SWIR2) # cor(-X$SWIR1[], Y.2015[], use=&quot;pairwise.complete.obs&quot;) # s &lt;- sample(1:ncell(Y.2015), 10000) # plot(log(Y.2015[s]) ~ log(X$SWIR1[s])) # # # val.points &lt;- raster::extract(X, val.points, sp=TRUE, fun=mean, buffer=20, normalizeWeights=TRUE) # plot(log(val.points$BaHa) ~ log(val.points$SWIR1)) # val.points$BaHa.log &lt;- log(val.points$BaHa) # val.points$SWIR1.log &lt;- log(val.points$SWIR1) # lm.swir &lt;- lm(BaHa.log ~ SWIR1.log, data=val.points@data) # abline(lm.swir) # # plot(val.points$BaHa ~ val.points$SWIR1) # lines(seq(1000, 2500, 50), exp(lm.swir$coeff[1] + lm.swir$coeff[2]*log(seq(1000, 2500, 50))), lty=4) # # Y.2015.swir &lt;- exp(lm.swir$coeff[1] + lm.swir$coeff[2]*log(X$SWIR1)) # val.points &lt;- raster::extract(Y.2015.swir, val.points, sp=TRUE, fun=mean, buffer=20, normalizeWeights=TRUE) # # plot(val.points$BaHa, val.points@data[,25]) # abline(0,1) # # # # Y.2015.log &lt;- log(Y.2015) # SWIR1.log &lt;- log(X$SWIR1) # lm.swir.map &lt;- lm(Y.2015.log[] ~ SWIR1.log[]) # abline(lm.swir.map, lty=2) # # plot(Y.2015[s] ~ X$SWIR1[s]) # lines(seq(1000, 4000, 50), exp(lm.swir.map$coeff[1] + lm.swir.map$coeff[2]*log(seq(1000, 4000, 50))), lty=4, col=&quot;red&quot;) # Y.2015.swir &lt;- exp(lm.swir.map$coeff[1] + lm.swir.map$coeff[2]*log(X$SWIR1)) # Y.2015.swir &lt;- writeRaster(Y.2015.swir, &quot;../results/carbone/AGB_2015_SWIR1.tif&quot;) # # # Y &lt;- Y.2015.swir # X &lt;- stack(&quot;../results/images/20030127_L7_B123457_L2.tif&quot;) # names(X) &lt;- c(&quot;B&quot;, &quot;G&quot;, &quot;R&quot;, &quot;NIR&quot;, &quot;SWIR1&quot;, &quot;SWIR2&quot;) # # Y.log &lt;- log(Y) # SWIR.log &lt;- log(X$SWIR1) # # lm.swir &lt;- lm(Y.log[] ~ SWIR.log[]) # Y.swir &lt;- exp(lm.swir$coeff[1] + lm.swir$coeff[2]*SWIR.log) # Y.swir &lt;- writeRaster(Y.swir, &quot;../results/carbone/AGB_2003_SWIR1.tif&quot;) # # Y.diff &lt;- Y.2015.swir - Y.swir # Y.diff &lt;- writeRaster(Y.diff, &quot;../results/carbone/AGB_diff_SWIR1.tif&quot;) ggplot(plots@data[], aes(x=cov18, y=BaHa)) + geom_point(aes(colour=OccSol), # colour depends on cond2 size=3) + facet_wrap( ~ OccSol) "],
["04_analyze-AGB.html", "", " 4.4.4 Production des résultats Description Description de la méthodologie / script AGB/5_analyze-AGB.R #################################################################### # NERF_Togo/AGB/5_analyze-AGB.R: analyze AGB maps # ------------------------------------------------------------------ # Bern University of Applied Sciences # Oliver Gardi, &lt;oliver.gardi@bfh.ch&gt; # 10 October 2019 RSR.Y &lt;- 0.563 RSR.Y.SE &lt;- 0.086 RSR.O &lt;- 0.275 RSR.O.SE &lt;- 0.003 RSR.AGB &lt;- 20 C.RATIO &lt;- 0.47 # evaluate 2015 biomass map with IFN strata ------------------ agb.2015 &lt;- raster(paste0(AGB.REF.DIR, &quot;/TGO_2015_AGB_R.tif&quot;)) strata &lt;- raster(paste0(DATA.DIR, &quot;/RapidEye/TGO_30m.tif&quot;)) zonal(agb.2015, strata, fun=&quot;mean&quot;) zonal(agb.2015, strata, fun=&quot;sd&quot;) # AGB density plots --------- files &lt;- dir(AGB.REF.DIR, pattern=&quot;^TGO.*\\\\.tif$&quot;, full.names=TRUE) maps &lt;- stack(files) names(maps) &lt;- substr(names(maps), 5, 8) maps.dat &lt;- as.data.frame(na.omit(maps[[c(&quot;X2003&quot;, &quot;X2018&quot;)]][])) dens.plot &lt;- maps.dat %&gt;% gather(variable, value, X2003, X2018) %&gt;% ggplot(aes(x=value, color=variable, linetype=variable)) + geom_density() + theme_light() # print(dens.plot) pdf(paste0(AGB.RES.DIR, &quot;/AGB-densities_03-18.pdf&quot;)) print(dens.plot) dev.off() # Analyze emissions and enhancements 2003/2018 ---------------- agb.2003 &lt;- raster(paste0(AGB.REF.DIR, &quot;/TGO_2003_AGB_R.tif&quot;)) bgb.2003 &lt;- agb.2003 * RSR.O bgb.2003[agb.2003 &lt;= RSR.AGB] &lt;- agb.2003[agb.2003 &lt;= RSR.AGB] * RSR.Y agb.2018 &lt;- raster(paste0(AGB.REF.DIR, &quot;/TGO_2018_AGB_R.tif&quot;)) bgb.2018 &lt;- agb.2018 * RSR.O bgb.2018[agb.2018 &lt;= RSR.AGB] &lt;- agb.2018[agb.2018 &lt;= RSR.AGB] * RSR.Y fc.2003 &lt;- raster(paste0(FCC.CLN.DIR, &quot;/FC30/TGO/TGO_2003_F30cf.tif&quot;)) fc.2018 &lt;- raster(paste0(FCC.CLN.DIR, &quot;/FC30/TGO/TGO_2018_F30cf.tif&quot;)) # emissions from deforestation defor &lt;- fc.2003 == 1 &amp; fc.2018 != 1 nf.agb &lt;- mean(agb.2018[fc.2018 == 3], na.rm=TRUE) # average non-forest AGB pd.agb &lt;- mean(agb.2018[defor], na.rm = TRUE) # average post-defor AGB defor.agb &lt;- mask(agb.2003, defor, maskvalue=0) - nf.agb defor.agb[defor.agb &lt; 0] &lt;- 0 nf.bgb &lt;- mean(bgb.2018[fc.2018 == 3], na.rm=TRUE) # average non-forest BGB pd.bgb &lt;- mean(bgb.2018[defor], na.rm = TRUE) # average post-defor BGB defor.bgb &lt;- mask(bgb.2003, defor, maskvalue=0) - nf.bgb defor.bgb[defor.bgb &lt; 0] &lt;- 0 # enhancements from reforestation regen &lt;- fc.2003 != 1 &amp; fc.2018 == 1 regen.agb.2003 &lt;- mask(agb.2003, regen, maskvalue=0) regen.agb.2018 &lt;- mask(agb.2018, regen, maskvalue=0) regen.agb &lt;- regen.agb.2018 - regen.agb.2003 regen.agb[regen.agb &lt; 0] &lt;- 0 regen.agb.2003 &lt;- mask(bgb.2003, regen, maskvalue=0) regen.agb.2018 &lt;- mask(bgb.2018, regen, maskvalue=0) regen.bgb &lt;- regen.bgb.2018 - regen.bgb.2003 regen.bgb[regen.bgb &lt; 0] &lt;- 0 # statistics for TGO and regions emissions &lt;- function(defor, defor.agb, defor.bgb, regen, regen.agb, regen.bgb, aoi=NULL, years=15) { if(!is.null(aoi)) { defor &lt;- mask(crop(defor, aoi), aoi) defor.agb &lt;- mask(crop(defor.agb, aoi), aoi) defor.bgb &lt;- mask(crop(defor.agb, aoi), aoi) regen &lt;- mask(crop(regen, aoi), aoi) regen.agb &lt;- mask(crop(regen.agb, aoi), aoi) regen.bgb &lt;- mask(crop(regen.bgb, aoi), aoi) } defor.co2 &lt;- (defor.agb + defor.bgb) * C.RATIO * 44/12 defor.area &lt;- sum(defor[], na.rm=TRUE) * 30^2 / 10000 defor.area.a &lt;- defor.area / years defor.agb.ha &lt;- mean(defor.agb[], na.rm=TRUE) defor.bgb.ha &lt;- mean(defor.bgb[], na.rm=TRUE) defor.co2.ha &lt;- mean(defor.co2[], na.rm=TRUE) regen.co2 &lt;- (regen.agb + regen.bgb) * C.RATIO * 44/12 regen.area &lt;- sum(regen[], na.rm=TRUE) * 30^2 / 10000 regen.area.a &lt;- regen.area / years regen.agb.ha &lt;- mean(regen.agb[], na.rm=TRUE) regen.bgb.ha &lt;- mean(regen.bgb[], na.rm=TRUE) regen.co2.ha &lt;- mean(regen.co2[], na.rm=TRUE) return(list( defor.area = defor.area, defor.area.a = defor.area.a, defor.agb.ha = defor.agb.ha, defor.agb.a = defor.agb.ha * defor.area.a, defor.bgb.ha = defor.bgb.ha, defor.bgb.a = defor.bgb.ha * defor.area.a, defor.co2.ha = defor.co2.ha, defor.co2.a = defor.co2.ha * defor.area.a, regen.area = regen.area, regen.area.a = regen.area.a, regen.agb.ha = regen.agb.ha, regen.agb.a = regen.agb.ha * regen.area.a, regen.bgb.ha = regen.bgb.ha, regen.bgb.a = regen.bgb.ha * regen.area.a, regen.co2.ha = regen.co2.ha, regen.co2.a = regen.co2.ha * regen.area.a )) } # DO THE WORK registerDoParallel(.env$numCores-1) res &lt;- foreach(i=0:length(TGO.reg), .combine=rbind) %dopar% { if(i==0) { data.frame(reg = &quot;TGO&quot;, emissions(defor, defor.agb, defor.bgb, regen, regen.agb, regen.bgb)) } else { region &lt;- TGO.reg[i,] data.frame(reg = region$NAME_1, emissions(defor, defor.agb, defor.bgb, regen, regen.agb, regen.bgb, aoi=region)) } } write.csv(res, paste0(AGB.RES.DIR, &quot;/NERF-Results.csv&quot;), row.names=FALSE) # Aggregated uncertainty (Monte Carlo Simulation) # activity data (uncertainty) mc.defor &lt;- rnorm(1000, mean.defor, sd) mc.regen &lt;- rnorm(1000, mean.regen, sd) # emission factor mean.agb.2003 &lt;- mean.agb.2018 &lt;- mean(agb.2018[defor]) mc.agb.2003 &lt;- rnorm(1000, mean(agb.2003[defor]), sqrt((RMSE^2)/n) ) mc.bgb.2003 &lt;- mc.agb.2003 * rnorm(1000, mean(bgb.2003), RSR.O.SE) mc.agb.2018 &lt;- rnorm(1000, mean(agb.2018[defor]), sqrt((RMSE^2)/n) ) mc.bgb.2018 &lt;- mc.agb.2003 * rnorm(1000, mean(bgb.2003), RSR.O.SE) "]
]
