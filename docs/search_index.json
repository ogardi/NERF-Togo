[
["index.html", "Manuel de référence Préface", " République Togolaise — Système Nationale de Surveillance des Forêts Manuel de référence Oliver Gardi, Fifonsi Dangbo, Sophie Dzigbod, Ditorgue Bakabima 2020-05-17 Préface Ce manuel de référence à comme objectif de décrire le fonctionnement du Système National de Surveillance des Forêts au Togo (SNSF). Les éléments traités sont les arrangements instutitionelles, l’implémentation de l’Inventaires Forestier National (IFN) et de Système Surveillance Terrestres par Satellite (SSTS) et l’approche technique pour en sortir les informations nécessaires pour le Niveau de Référence pour les Forêts du Togo (NRF) ainsi que pour le Monitoring, Reporting et Verification (MRV) dans le cadre de l’engagement du Togo pour le REDD+. La partie Analyses NRF/MRV décrit en détail les outils utilisés pour établir et le Niveau de Référence pour les Forêts du Togo 1.0, soumis au sécrétariat CCNUCC en Janvier 2020 et pour mettre à jour les analyses dans le cadre d’une surveillance de la biomasse forestier continue dans le cadre du Monitoring, Reporting et Verification pour la REDD+. Les résultats de ces analyses sont publiés ailleurs (liens sur les rapports sur le site CCNUCC et géoportail). En cas de questions, veuillez contacter la Coordination nationale REDD+ du Togo "],
["00_introduction.html", "1 Introduction", " 1 Introduction L’objectif du Système National de Surveillance des Forêts (SNSF) est d’évaluer régulièrement l’état des forêts togolaises et leur évolution. Dans le code forestier du Togo, la forêt est définie comme: un espace occupant une superficie de plus de 0,5 hectare avec des arbres atteignant une hauteur supérieure à 5 mètres et un couvert arboré de plus de 10 pour cent, ou avec des arbres capables d’atteindre ces seuils in situ. Pour l’évaluation du développement des zones forestières, le SNSF distingue entre terres forestières avec un couvert des houppiers ≥ 30% et les terres boisées avec un couvert des houppiers entre 10% – 30%. Actuellement, sur base des images Landsat, le SNSF enregistre que l’évolution des terres forestiers avec une couverture des houppiers ≥ 30%. Des données satellitaires de plus haute resolution seront nécessaires pour évaluer également les terres boisées. Le SNSF combine les données recueillies sur le terrain avec les données des images satellites pour fournir des informations sur lévolution du l’ensemble des forêts dans le pays. Comme illustré dans l’image au-dessous et décrit dans les sections suivantes, le SNSF consiste de trois pilliers principaux: Le Inventaire Forestier National (IFN) recueille des informations détaillées sur l’état des forêts sur un nombre limité de placettes d’échantillonnage permanentes sur le terrain. Au moyen du Système de Surveillance Terrestre par Satellite (SSTS), des informations sur la couverture et l’utilisation des sols sont recueillies sur un grand nombre de parcelles d’échantillonnage à partir d’images satellites. Avec l’aide du SSTS, les informations sur l’IFN peuvent être extrapolées à l’ensemble du pays. Le Niveau de Référence des Forêts (NRF) ainsi que le Monitoring, Reporting et Vérification (MRV) des changements dans le réservoirs carbone forestiersest une applications du SNSF pour informer la communauté internationale sur l’engagement du Togo dans le cadre du mécanisme REDD+. Dans ce cadre, les données de la IFN sont utilisées pour déterminer le stockage du carbone dans la biomasse des arbres, tandis que les données de la SSTS sont utilisées pour déterminer le changement de la superficie forestière. Ensemble, cela se traduit par les pertes de carbone dues à la déforestation et la séquestration du carbone provenant du reboisement. En future, d’autres sources de données pourraient être intégrées au SNSF, telles que le carbone organique du sol (prévu à rélever dans l’IFN-2) feux de brousse (base de données SANGE) dégradation des forêts (utilisation des images de haute résolution Sentinel-2) droits fonciers plantations exploitation du bois … Pour l’avenir, il est également prévu d’impliquer la population locale dans la surveillance des forêts, par exemple en signalant les activités irrégulières à l’aide d’une application pour smartphone. "],
["01_arrangements.html", "1.1 Arrangements institutionelles", " 1.1 Arrangements institutionelles L’arrangement institutionnel propose pour le système national de suivi des forêts se presente comme ci-dessous. Coordination: Le Ministère de l’Environnement, du Developpement Durable et de la Protection de la Nature (MEDDPN) à travers la Direction de l’Environnement (DE) est chargée de la soumission des rapports (Communication Nationale et Rapports Biennaux) à la Convention Cadre des Nations Unies sur le Changement Climatique (CCNUCC). La Cellule MRV de la Coordination nationale REDD+ située a l’ODEF est responsable de la coordination de toutes les institutions et organisations impliquées dans l’alimentation du système SNSF. Cette cellule est l’entité clé chargée de faciliter et de soutenir les communications sur NRF/NERF du Togo. Le Groupe de travail NERF/MRV et l’équipe nationale de suivi des forêts sont chargés du travail et des décisions et choix techniques sur les données, résultats et méthodologie adoptés pour le NRF/MRV. C’est la cheville ouvrière de la cellule MRV. Elles sont constituées des cadres des institutions qui interviennent dans le système national de suivi des forêts (SNSF). La Direction de l’Environnement (DE) se charge des inventaires de gaz à effet de serre (I-GES) de tous les secteurs mais assure la cohérence des données d’I-GES du secteur agriculture, foresterie et autres affectations des terres (AFAT) avec les rapports qui seront soumis à la CCNUCC. La DE se chargera d’assurer la cohérence entre la méthodologie utilisée dans le cadre du NRF avec les données d’I-GES du secteur AFAT. Données d’activités: L’Unité de gestion de bases de données cartographiques (UGBDC) de la Direction des études et de la planification (DEP), chargée de la gestion de la cartographie des domaines forestiers du Togo ainsi que la Division cartographie et Télédétection (DCT) de l’Officie de développement et d’exploitation des forêts (ODEF) chargée de la cartographie des forêts classées et plantations étatique se chargeront de produire les données d’activités à travers le système de suivi des terres par satellite (SSTS). L’Agence nationale de gestion de l’environnement (ANGE) est chargée de fournir les données sur les feux de végétation. Facteurs d’émission: La cellule de gestion de la base des données des ressources forestières et des résultats de l’inventaire forestier national (CBDR-IFN) de la Direction des ressources forestières (DRF) et la Division cartographie et télédétection (DCT) de l’ODEF sont chargé de produire les facteurs d’émission à travers les inventaires forestier nationaux et les inventaires des plantations. Données complémentaires: la Direction générale de l’énergie du ministère des mines et énergie DGE/MME se chargera de fournir les données sur la consommation en bois énergie. la Direction de la Statistique agricole de l’Informatique et de la Documentation (DCID) et l’Institut Togolais de Recherche Agronomique (ITRA) produiront des données sur l’agriculture (superficie emblavées et le cheptel). les données de recherche des universités du Togo alimenteront le mécanisme MRV ainsi que le NRF. L’Institut national de la statistique et des études économiques et démographiques (INSEED) donnera des compléments d’informations sur la démographie et autres. Contrôle de qualité / validation interne: L’assurance qualité et le contrôle qualité se fera à travers l’évaluation indépendante interne du Laboratoire de biologie et écologie végétale (LBEV) et le Laboratoire de recherche forestière (LRF) de l’université de Lomé (LBEV/UL) ainsi que la Direction générale de la cartographie (DGC). Les laboratoires universitaires LRF et LBEV évalueront les méthodes et nouveaux données au fur et à mesure qu’ils seront générés. "],
["02_how-to.html", "1.2 Pour commencer", " 1.2 Pour commencer 1.2.1 Travailler avec GitHub "],
["03_commencer.html", "", " 1.2.2 Logiciel et serveur La cartographie des surfaces forestier et de la biomasse aérienne des arbres se fait avec le logiciel R, dirigé à travers RStudio. Les scripts R sont mis à disposition dans un dépot GitHub. Les scripts dépendent des fois des outils GDAL disponible dans l’environnement, à installer pour les systèmes Linux avec apt-get install python-gdal. Le traitement et l’analyse des images satellitaires demande des ressources intensives en terme de stockage de données (plusieurs TB) et capacité de calcul (plusieurs processeurs en parallèle). Les analyses pour le Niveau des emissions de référence pour les forêts du Togo (NERF 1.0) ont été éfféctués sur l’infrastructure informatique centrale de la BFH-HAFL (http://r.gro1.bfh.science, accès limité). Une fois la nouvelle infrastructure informatique puissante avec un serveur central au Ministère pour l’Environnement et Forêts (MERF) est installé et disponible, les travaux peuvent être éffectués là. 1.2.3 Structure du répértoire Les outils R dépendent d’une certaine structure des fichiers. Le répértoire de base SNSF_Togo est structuré comme suivant: SNSF_Togo ========= ├── data # Données de base externes #################### ├── GADM # &lt;- frontières administratives ├── Landsat # &lt;- images satellitaires ├── SRTM # &lt;- données topographiques └── Worldclim # &lt;- données climatiques └── SNSF_v1.0_20200106 # Répértoire SNSF v1.0 ######################## ├── .Rprofile # [R] initialisation ├── 01_SSTS # SYSTÈME DE SURVEILLANCE TERRESTRE ========= ├── 01_data #:: images et autres données pré-traités -- └── 02_BdD #:: base de données SSTS ------------------ ├── 02_IFN # INVENTAIRE FORESTIER NATIONAL ============= └── 01_IFN-1 #:: données d&#39;inventaire IFN-1 ------------ ├── 03_NRF-MRV # NIVEAU DE REFERENCE / MRV ================ ├── 01_MCF #:: Modification Couvert Forestier -------- ├── 02_AGB #:: cartographie biomasse ----------------- └── 03_report #:: rapport NRF/MRV ----------------------- ├── 04_SNSF-manual #:: cette documentation du SNSF ============= └── docs #:: cette documentation en HTML └── SNSF_v1.x # Répértoire SNSF version actualisé ########### La structure du répertoire est définit dans le script R .Rprofile et peut être ajousté. C’est seulement les répétoires src et manual qui sont mis à diposition dans le dépôt GitHub. Les autres répétoires et données doivent être installés manuellement. 1.2.4 Création d’un nouveau projet Pour la création d’un nouveau projet avec le code le plus actuel, on clone le dépôt du projet sur GitHub par la commande git clone --single-branch https://github.com/ogardi/NERF-Togo.git NOM-REPERTOIRE. L’installation d’une version spécifique peut être fait avec git clone -b VERSION --single-branch https://github.com/ogardi/NERF-Togo.git NOM-REPERTOIRE. En travaillant avec RStudio, le plus facile est de créer directement un projet RStudio au départ due dépôt GitHub (File &gt; New Project ... &gt; Version Control &gt; Git) avec les mêmes paramètre que l’installtion directe en haut. Dans une prochaine étape, les données doivent être rendues disponibles, soit par une nouvelle acquisition, soit par une copie des répertoires existants. S’il n’est pas encore disponible, le répertoire ../data/ doit être créé et les données de base correspondantes doivent être fournies. Dans le répertoire ./01_SSTS/02_BdD/, le réseau d’échantillonnage ainsi que les données d’entraînement et de validation doivent être stockées. En outre, les données d’inventaire doivent être stockées dans le répertoire ./03_IFN/. 1.2.5 Définition des variables Le traitement des images ainsi que les différentes analyses se font via des R-Scripts. Les variables et les fonctions utilisées dans les différents scripts sont définit dans le fichier .Rprofile, qui est automatiquement chargé au démarrage de R. Si le processus de chargement a réussi, vous pouvez voir un message de bienvenue suivant sur la console R. Si aucun message n’apparaît, assurez-vous que a) R est lancé dans le répertoire et b) qu’aucun message d’erreur ne se produit. Les messages d’erreur possibles sont des paquets manquants (les paquets correspondants doivent être installés en premier) ou le fait de ne pas trouver les limites administratives de GADM dans ../data/GADM (doivent également être installés en premier). Si nécessaire les variables sont ajustées et R est redémarré. Notamment les informations sur la période analysée YEARS.ALL et les années à prendre en compte pour les différentes évaluations YEARS.JNT, YEARS.VAL et YEARS.REF. Script R: .Rprofile ############################################################################### # .Rprofile: Préparer l&#39;environnement (libraries, variables) # ----------------------------------------------------------------------------- # Bern University of Applied Sciences # Oliver Gardi, &lt;oliver.gardi@bfh.ch&gt; # 13 Mai 2020 # Charger libraries ========================================================== library(&quot;sp&quot;) # Classes et méthodes pour les données spatiales library(&quot;rgdal&quot;) # Geospatial Data Abstraction Library library(&quot;raster&quot;) # Analyse et modélisation des données géographiques library(&quot;randomForest&quot;) # Algorithme de classification et régression library(&quot;caret&quot;) # Outils pour classification et régression library(&quot;openxlsx&quot;) # Lire et écrire des fichiers Excel (xlsx) library(&quot;dplyr&quot;) # Fonctions pour manipuler des données library(&quot;tidyr&quot;) # Fonctions pour reorganiser des données library(&quot;ggplot2&quot;) # Production des figures library(&quot;foreach&quot;) # Faire des calcules en parallèle ... library(&quot;doParallel&quot;) # ... sur plusieurs processeurs # Créer un environnement caché =============================================== .snsf = new.env() # Années / Périodes ----------------------------- .snsf$YEARS.ALL &lt;- 1985:2019 # - tous .snsf$YEARS.JNT &lt;- c(1987, 2003, 2005, 2007, 2015, 2017, 2018) # - conjointes .snsf$YEARS.REF &lt;- c(1987, 2003, 2015, 2018) # - référence .snsf$YEARS.NRF &lt;- c( 2003, 2015, 2018) # - NRF # Répertoires ----------------------------------- .snsf$DIR.RAW.DAT &lt;- &quot;../data&quot; .snsf$DIR.SST &lt;- &quot;./01_SSTS&quot; .snsf$DIR.SST.DAT &lt;- paste0(.snsf$DIR.SST, &quot;/01_data&quot;) .snsf$DIR.SST.BDD &lt;- paste0(.snsf$DIR.SST, &quot;/02_BdD&quot;) .snsf$DIR.IFN &lt;- &quot;./02_IFN&quot; .snsf$DIR.IFN.DAT &lt;- paste0(.snsf$DIR.IFN, &quot;/01_field-data&quot;) .snsf$DIR.MRV &lt;- &quot;./03_NRF-MRV&quot; .snsf$DIR.MRV.MCF &lt;- paste0(.snsf$DIR.MRV, &quot;/01_MCF&quot;) .snsf$DIR.MRV.AGB &lt;- paste0(.snsf$DIR.MRV, &quot;/02_AGB&quot;) # Système de référence des coordonnées ---------- .snsf$UTM.30 &lt;- crs(&quot;+proj=utm +zone=30 +datum=WGS84 +units=m +no_defs +ellps=WGS84 +towgs84=0,0,0&quot;) .snsf$UTM.31 &lt;- crs(&quot;+proj=utm +zone=31 +datum=WGS84 +units=m +no_defs +ellps=WGS84 +towgs84=0,0,0&quot;) # Domaine d&#39;intérêt ---------------------------- .snsf$TGO &lt;- spTransform( readOGR(paste0(.snsf$DIR.RAW.DAT, &quot;/GADM/gadm36_TGO_0.shp&quot;)), .snsf$UTM.31 ) .snsf$TGO.EXT &lt;- extent(151155, 373005, 670665, 1238175) # xmin, xmax, ymin, ymax # Noms des couches de données ------------------- .snsf$SST.LSBANDS &lt;- c(&quot;B&quot;, &quot;G&quot;, &quot;R&quot;, &quot;NIR&quot;, &quot;SWIR1&quot;, &quot;SWIR2&quot;, &quot;evi&quot;, &quot;msavi&quot;, &quot;nbr&quot;, &quot;nbr2&quot;, &quot;ndmi&quot;, &quot;ndvi&quot;, &quot;savi&quot;) .snsf$SST.BIOCLIM &lt;- paste0(&quot;BIO&quot;, 1:19) # Codes pour les classes ------------------------ .snsf$NONFOR &lt;- 0 # non-forêt .snsf$PREGEN &lt;- 1 # régénération potentielle .snsf$REGEN &lt;- 2 # régénération .snsf$FOREST &lt;- 3 # forêt / forêt initiale # Divers ---------------------------------------- # Processeurs disponibles pour le calcul parallèle .snsf$CORES &lt;- detectCores() # Semence pour le générateur de nombres aléatoires .snsf$RSEED &lt;- 20191114 # Attacher l&#39;environnement attach(.snsf) # Message de bienvenue ======================================================= message(&quot; =&gt; chargé librairies et variables définit en .Rprofile .oPYo. o o .oPYo. ooooo ooooo 8 8b 8 8 8 8 `Yooo. 8`b 8 `Yooo. o8oo 8 .oPYo. .oPYo. .oPYo. `8 8 `b 8 `8 8 8 8 8 8 8 8 8 8 8 `b8 8 8 8 8 8 8 8 8 8 `YooP&#39; 8 `8 `YooP&#39; 8 8 `YooP&#39; `YooP8 `YooP&#39; :.....:..:::..:.....::..::::::::..:::.....::....8 :.....: :::::::::::::::::::::::::::::::::::::::::::::ooP&#39;.::::::: :::::::::::::::::::::::::::::::::::::::::::::...::::::::: \\n&quot;, date(), &quot;\\n&quot; ) "],
["00_STSS.html", "2 Système de Surveillance Terrestre", " 2 Système de Surveillance Terrestre Le Système de Surveillance Terrestre par Satellite (SSTS) consiste d’une base de données spatiales (images satellitaires, modèle numérique du terrain, données climatiques) d’un réseau de parcelles d’échantillonnage, dont la couverture des houppiers et l’utilisation des terres sont régulièrement enregistrées par photo-interprètes sur base des images satellitaires. Pour le moment, les attributs enregistrés dans le SSTS sont : Couverture des houppiers Occupation des terres Les répertoires et les fichiers sont structurés comme suit: 01_SSTS # SYSTÈME DE SURVEILLANCE TERRESTRE ========= ├── 01_data #:: images et autres données pré-traités -- └── ... ├── 02_BdD #:: base de données SSTS ------------------ ├── _src #:: scripts R ├── _create-grid.R # [R] création réseau SSTS ├── _create-train-plots.R # [R] création parcelles d&#39;entraînement └── _create-val-plots.R # [R] création parcelles de validation ├── 01_reseau-SSTS #:: réseau de parcelles SSTS ├── 02_train-plots #:: parcelles d&#39;entraînement └── 03_val-plots #:: parcelles de validation Actuellement, les données de base ainsi que les données générées par les photo-interprètes sont stockées dans des fichiers (tif pour les rasters et shp pour les données vecteurs). Il est prévu de gérer ces données SSTS dans une base de données géographiques (PostGIS) à l’avenir. Cette base de données est actuellement installé sur un serveur central au ministère de l’Environnement et des Forêts (MERF). "],
["01_Landsat.html", "2.1 Données de base", " 2.1 Données de base 2.1.1 Images Landsat Actuellement, l’analyse de l’évolution du couvert forestier est principalement basée sur les images satellitaires Landsat, qui sont disponibles gratuitement dans les archives de l’USGS. Les missions Landsat-4 à Landsat-8 produisent des images de résolution spatiale et radiométrique comparable depuis les années 1980. Les images brutes sont corrigées géométriquement et radiométriquement par l’USGS (USGS Collection 1 Level-2 Surface Reflection Product). La résolution spatiale des images est de 30 mètres. Les bandes spectrales utilisées sont B, G, R, NIR, SWIR-1 et SWIR-2 (voir les désignations des bandes dans la figure ci-dessous). Le territoire du Togo est couvert par un total de 9 images Landsat (scènes WRS 2). Les zones couvertes par les différentes scènes sont indiquées dans la figure ci-dessous. Les images Landsat utilisées pour l’évaluation du couvert forestier ont été prises idéalement à la fin de la saison sèche, c’est-à-dire de (Nov), Déc, Jan, (Fév) et sont disponibles en bonne qualité (sans nuages ou seulement légèrement couvertes par des nuages et des ombres) pour la même date sur l’ensemble du chemin WRS 2 respectif. Pour les années de référence, là ou on aimerait des cartes qui couvrent l’ensemble du territoire du Togo, des images correspondantes sont nécessaires pour tous les trois chemins WRS 2. Le tableau ci-dessous présente les images satellites utilisées pour l’analyse du couvert forestier au Togo. Les années de référence utilisées pour le NRF et les images correspondantes sont indiquées en caractères gras. Ce n’est que pour l’année 1991 que les images de différentes dates d’enregistrement ont été combinées pour le chemin WRS 193. L7* marque les images Landsat-7 avec des lacunes dans les données (SLC-off). La colonne GoogleEarth montre la répartition des dates des images de très haute résolution disponibles sur GoogleEarth. Seules les images de référence GoogleEarth de 2017 – 2018 ont été utilisées pour la calibration de la carte forêt/non-forêt 2018. WRS 192054,055,056 WRS 193052,053,054,055 WRS 194052,053 GoogleEarthRéférence 2019 L8 / 23.12.18 L8 / 16.02.19 L8 / 22.01.19 ++ 2018 L8 / 05.01.18 L8 / 12.01.18 L8 / 18.12.17 +++++++ 2017 L8 / 19.02.17 L8 / 25.01.17 L8 / 31.12.16 +++ 2016 — — — (+) 2015 L8 / 13.01.15 L8 / 04.01.15 L8 / 27.01.15 (+) 2014 — — — (++) 2013 L7* / 31.01.13 L7* / 23.02.13 — (+) 2012 — — L7* / 11.01.12 (++) 2011 L7* / 10.01.11 — — (+) 2010 — — L7* / 21.01.10 (+) 2009 — L7* / 27.01.09 — 2008 — — — 2007 L7* / 30.12.06 L7* / 22.01.07 L5 / 05.01.07 2006 — — — 2005 L7* / 24.12.04 L7* / 17.02.05 L7* / 22.12.04 2004 — — — 2003 L7 / 04.01.03 L7 / 26.12.02 L7 / 17.12.02 (.) 2002 — — — 2001 L7 / 13.12.00 — — (.) 2000 — L7 / 04.02.00 L7 / 26.01.00 … … … … 1997 — — L5 / 10.02.97 … … … … 1991 L4 / 03.01.91 L4 / 10.01.91 &amp; L5 / 28.11.89 — … … … … 1987 L5 / 31.12.86 L5 / 23.01.87 L5 / 29.12.86 1986 L5 / 13.01.86 L5 / 06.03.85 L5 / 11.01.86 2.1.1.1 Acquisition des images Ouvrir le site USGS Earthexplorer. Dans la fenêtre Search Criteria il faut selectionner la période pour laquelle on cherche des images (Nov - Jan). Dans la fenêtre Data Sets, les produits Landsat Level-2 (Surface Reflectance) sont séléctionnés. Dans la fenêtre Additional Criteria il faut choisir les scènes (chemin 192: 054-056 / chemin 193: 052-055 / chemin 194: 052-053). Parmi les images disponibles, on sélectionne celles qui sont disponibles à la même date et en bonne qualité pour l’ensemble du chemin WRS. On copie les identifier des images à télécharger dans un fichier txt. Ensuite on ouvre le site USGS ESPA pour commander les images choisi. On charge le fichier txt avec les identifier des images et on commande les bandes Surface Reflectance et les indices spéctrales (voir image au-dessous). Pour commander des images, il faut qu’on a un compte USGS. Une fois on est notifié par eMail que les images sont prêts, on les téléchargent manuellement ou tous ensemble avec le USGS bulkdownloader et la commande download_espa_order.py -u [nom d'utilisateur] -o ALL -d [répértoire]. On dézip les images et les rangent dans le répétoire data/Landsat sous la scène et l’année correspondante. Pour des images de l’hiver 2019/20, l’année correspondante est 2020. 2.1.1.2 Prétraitement des images Le premier traitement est la préparation des images Landsat et autres variables utilisés pour modéliser la surface forestier ou la biomasse aérienne comme les données topographique et climatiques. L’objectif est qu’on prépare avec les données brutes un jeu de données raster complet sur le même extent (territoir du Togo) et avec la même résolution spatiale de 30 mètres (résolution de base des images Landsat). On ouvre le script 1_prepare-images.R et on modifie la liste des images Landsat à utiliser (in.image.list), par exemple par ajouter les nouveaux image à considérer dans les analyses: p192.2019 = list( paste0(DATA.DIR, &quot;/Landsat/192_054/2019/LC081920542018122301T1-SC20190405164258/&quot;), paste0(DATA.DIR, &quot;/Landsat/192_055/2019/LC081920552018122301T1-SC20190405163359/&quot;), paste0(DATA.DIR, &quot;/Landsat/192_056/2019/LC081920562018122301T1-SC20190405163342/&quot;)) ... p193.2019 = list( paste0(DATA.DIR, &quot;/Landsat/193_052/2019/LC081930522019021601T1-SC20190405183839/&quot;), paste0(DATA.DIR, &quot;/Landsat/193_053/2019/LC081930532019021601T1-SC20190405181518/&quot;), paste0(DATA.DIR, &quot;/Landsat/193_054/2019/LC081930542019021601T1-SC20190405183609/&quot;), paste0(DATA.DIR, &quot;/Landsat/193_055/2019/LC081930552019021601T1-SC20190405181507/&quot;)) ... p194.2019 = list( paste0(DATA.DIR, &quot;/Landsat/194_052/2019/LC081940522019012201T1-SC20190405172019/&quot;), paste0(DATA.DIR, &quot;/Landsat/194_053/2019/LC081940532019012201T1-SC20190405172055/&quot;)) Outre la définition des images à traiter, le script définit une fonction prepare.image pour stacker les différentes bandes des images Landsat, pour les fusioner, masquer et couper les images Landsat chemin par chemin (WRS2 paths 192, 193 et 194 pour Togo). Par défaut, les images qui ont déjà été traité (filename existe déjà) ne sont plus traité (overwrite=FALSE). prepare.image(in.image.dirs, ext=NULL, filename=NULL, overwrite=FALSE, log=TRUE) Dans la deuxième partie du script, là où c’est noté # DO THE WORK ---------, on lance le traitement des images. Avec le code foreach(...) %dopar% { ... } on lance le traitement de chaque chemin pour chaque année sur des différents processeurs au parallèle. À la fin du script on transforme les images du chemin 194 du système de coordonnées UTM 30 vers UTM 31 produit des thumbnails des images Landsat Les images prétraités sont sauveguarder dans le répétoire input/1_images du projet, ensemble avec des Thumbnails des chemins. Dans une prochaîne étape, les images sont néttoyées de l’eau, nuages et ombres en utilisant les bandes Landsat de qualité des pixels. Example Images Landsat de l’année 2019: chemin p194 composé de 2 scénes du 22.01.2019 / p193 avec 4 scènes du 16.02.2019 / p192 avec 3 scènes du 23.12.2018 Script R: 01_SSTS/01_data/_src/prep-Landsat.R ############################################################################### # prep-Landsat.R: lire, nettoyer et empiler des images Landsat # ----------------------------------------------------------------------------- # Bern University of Applied Sciences # Oliver Gardi, &lt;oliver.gardi@bfh.ch&gt; # 13 Mai 2020 # Définitions des variables =================================================== IN.DIR &lt;- paste0(DIR.RAW.DAT, &quot;/Landsat&quot;) OUT.DIR &lt;- paste0(DIR.SST.DAT, &quot;/Landsat&quot;) if(!dir.exists(OUT.DIR)) dir.create(OUT.DIR) WRS &lt;- readOGR(paste0(IN.DIR, &quot;/WRS2/WRS2_descending.shp&quot;)) # Scènes WRS # Masques de nuages et d&#39;eau pour Landsat 4-7 et Landsat 8 ------------------ # voir les guides des produits de réflexion de surface Landsat 4-7 et Landsat 8 # https://www.usgs.gov/land-resources/nli/landsat/landsat-surface-reflectance QA.WATER &lt;- c( 68, 132, # Landsat 4-7 324, 388, 836, 900, 1348 # Landsat 8 ) QA.SHADOW &lt;- c( 72, 136, 328, 392, 840, 904, 1350 ) QA.ICE &lt;- c( 80, 112, 144, 176, 336, 368, 400, 432, 848, 880, 912, 944, 1352 ) QA.CLOUD &lt;- c( 96, 112, 160, 176, 224, 352, 368, 416, 432, 480, 864, 880, 928, 944, 992 ) # Liste des images à préparer et à fusionner ================================== in.image.list &lt;- list( # Chemin WRS p192 ----------------------------- p192.1986 = list(paste0(IN.DIR, &quot;/192_054/1986/LT051920541986011301T1-SC20190405164223/&quot;), paste0(IN.DIR, &quot;/192_055/1986/LT051920551986011301T1-SC20190405164227/&quot;), paste0(IN.DIR, &quot;/192_056/1986/LT051920561986011301T1-SC20190405164153/&quot;)), p192.1987 = list(paste0(IN.DIR, &quot;/192_054/1987/LT051920541986123101T1-SC20190405164150/&quot;), paste0(IN.DIR, &quot;/192_055/1987/LT051920551986123101T1-SC20190405163521/&quot;), paste0(IN.DIR, &quot;/192_056/1987/LT051920561986123101T1-SC20190405164444/&quot;)), p192.1991 = list(paste0(IN.DIR, &quot;/192_054/1991/LT041920541991010301T1-SC20190405164201/&quot;), paste0(IN.DIR, &quot;/192_055/1991/LT041920551991010301T1-SC20190405165911/&quot;), paste0(IN.DIR, &quot;/192_056/1991/LT041920561991010301T1-SC20190405163911/&quot;)), p192.2001 = list(paste0(IN.DIR, &quot;/192_054/2001/LE071920542000121301T1-SC20190405165521/&quot;), paste0(IN.DIR, &quot;/192_055/2001/LE071920552000121301T1-SC20190405165645/&quot;), paste0(IN.DIR, &quot;/192_056/2001/LE071920562000121301T1-SC20190405164029/&quot;)), p192.2003 = list(paste0(IN.DIR, &quot;/192_054/2003/LE071920542003010401T1-SC20190520111322/&quot;), paste0(IN.DIR, &quot;/192_055/2003/LE071920552003010401T1-SC20190520100402/&quot;), paste0(IN.DIR, &quot;/192_056/2003/LE071920562003010401T1-SC20190520100206/&quot;)), p192.2005 = list(paste0(IN.DIR, &quot;/192_054/2005/LE071920542004122401T1-SC20190405165520/&quot;), paste0(IN.DIR, &quot;/192_055/2005/LE071920552004122401T1-SC20190405164050/&quot;), paste0(IN.DIR, &quot;/192_056/2005/LE071920562004122401T1-SC20190405164030/&quot;)), p192.2007 = list(paste0(IN.DIR, &quot;/192_054/2007/LE071920542006123001T1-SC20190406034211/&quot;), paste0(IN.DIR, &quot;/192_055/2007/LE071920552006123001T1-SC20190406034231/&quot;), paste0(IN.DIR, &quot;/192_056/2007/LE071920562006123001T1-SC20190406034202/&quot;)), p192.2011 = list(paste0(IN.DIR, &quot;/192_054/2011/LE071920542011011001T1-SC20190406034214/&quot;), paste0(IN.DIR, &quot;/192_055/2011/LE071920552011011001T1-SC20190406034114/&quot;), paste0(IN.DIR, &quot;/192_056/2011/LE071920562011011001T1-SC20190406034155/&quot;)), p192.2013 = list(paste0(IN.DIR, &quot;/192_054/2013/LE071920542013013101T1-SC20190406034224/&quot;), paste0(IN.DIR, &quot;/192_055/2013/LE071920552013013101T1-SC20190406034046/&quot;), paste0(IN.DIR, &quot;/192_056/2013/LE071920562013013101T1-SC20190406034057/&quot;)), p192.2015 = list(paste0(IN.DIR, &quot;/192_054/2015/LC081920542015011301T1-SC20190405163446/&quot;), paste0(IN.DIR, &quot;/192_055/2015/LC081920552015011301T1-SC20190405163723/&quot;), paste0(IN.DIR, &quot;/192_056/2015/LC081920562015011301T1-SC20190405164231/&quot;)), p192.2017 = list(paste0(IN.DIR, &quot;/192_054/2017/LC081920542017021901T1-SC20190405163339/&quot;), paste0(IN.DIR, &quot;/192_055/2017/LC081920552017021901T1-SC20190405163342/&quot;), paste0(IN.DIR, &quot;/192_056/2017/LC081920562017021901T1-SC20190405163222/&quot;)), p192.2018 = list(paste0(IN.DIR, &quot;/192_054/2018/LC081920542018010501T1-SC20190405164304/&quot;), paste0(IN.DIR, &quot;/192_055/2018/LC081920552018010501T1-SC20190405163402/&quot;), paste0(IN.DIR, &quot;/192_056/2018/LC081920562018010501T1-SC20190405163250/&quot;)), p192.2019 = list(paste0(IN.DIR, &quot;/192_054/2019/LC081920542018122301T1-SC20190405164258/&quot;), paste0(IN.DIR, &quot;/192_055/2019/LC081920552018122301T1-SC20190405163359/&quot;), paste0(IN.DIR, &quot;/192_056/2019/LC081920562018122301T1-SC20190405163342/&quot;)), # Chemin WRS p193 ----------------------------- p193.1985 = list(paste0(IN.DIR, &quot;/193_052/1985/LT051930521985030601T1-SC20190520100259/&quot;), paste0(IN.DIR, &quot;/193_053/1985/LT051930531985030601T1-SC20190520100324/&quot;), paste0(IN.DIR, &quot;/193_054/1985/LT051930541985030601T1-SC20190520100340/&quot;), paste0(IN.DIR, &quot;/193_055/1985/LT051930551985030601T1-SC20190520100140/&quot;)), p193.1987 = list(paste0(IN.DIR, &quot;/193_052/1987/LT051930521987012301T1-SC20190405182322/&quot;), paste0(IN.DIR, &quot;/193_053/1987/LT051930531987012301T1-SC20190405182335/&quot;), paste0(IN.DIR, &quot;/193_054/1987/LT051930541987012301T1-SC20190405182331/&quot;), paste0(IN.DIR, &quot;/193_055/1987/LT051930551987012301T1-SC20190405182328/&quot;)), # Attention: là nous avons des images avec des dates différentes ! p193.1990.1 = list(paste0(IN.DIR, &quot;/193_052/1990/LT051930521989112801T1-SC20190520100201/&quot;), paste0(IN.DIR, &quot;/193_053/1990/LT051930531989112801T1-SC20190520100233/&quot;)), p193.1990.2 = list(paste0(IN.DIR, &quot;/193_054/1991/LT041930541991011001T1-SC20190402043117/&quot;), paste0(IN.DIR, &quot;/193_055/1991/LT041930551991011001T1-SC20190402042453/&quot;)), p193.2000 = list(paste0(IN.DIR, &quot;/193_052/2000/LE071930522000020401T1-SC20190520100729/&quot;), paste0(IN.DIR, &quot;/193_053/2000/LE071930532000020401T1-SC20190520100345/&quot;), paste0(IN.DIR, &quot;/193_054/2000/LE071930542000020401T1-SC20190402045232/&quot;), paste0(IN.DIR, &quot;/193_055/2000/LE071930552000020401T1-SC20190402043121/&quot;)), p193.2003 = list(paste0(IN.DIR, &quot;/193_052/2003/LE071930522002122601T1-SC20190405182352/&quot;), paste0(IN.DIR, &quot;/193_053/2003/LE071930532002122601T1-SC20190405182309/&quot;), paste0(IN.DIR, &quot;/193_054/2003/LE071930542002122601T1-SC20190405182226/&quot;), paste0(IN.DIR, &quot;/193_055/2003/LE071930552002122601T1-SC20190405190255/&quot;)), p193.2005 = list(paste0(IN.DIR, &quot;/193_052/2005/LE071930522005021701T1-SC20190405190117/&quot;), paste0(IN.DIR, &quot;/193_053/2005/LE071930532005021701T1-SC20190405190003/&quot;), paste0(IN.DIR, &quot;/193_054/2005/LE071930542005021701T1-SC20190405182210/&quot;), paste0(IN.DIR, &quot;/193_055/2005/LE071930552005021701T1-SC20190405190021/&quot;)), p193.2007 = list(paste0(IN.DIR, &quot;/193_052/2007/LE071930522007012201T1-SC20190405182221/&quot;), paste0(IN.DIR, &quot;/193_053/2007/LE071930532007012201T1-SC20190405182607/&quot;), paste0(IN.DIR, &quot;/193_054/2007/LE071930542007012201T1-SC20190405182139/&quot;), paste0(IN.DIR, &quot;/193_055/2007/LE071930552007012201T1-SC20190405182418/&quot;)), p193.2009 = list(paste0(IN.DIR, &quot;/193_052/2009/LE071930522009012701T1-SC20190405182143/&quot;), paste0(IN.DIR, &quot;/193_053/2009/LE071930532009012701T1-SC20190405182301/&quot;), paste0(IN.DIR, &quot;/193_054/2009/LE071930542009012701T1-SC20190405182103/&quot;), paste0(IN.DIR, &quot;/193_055/2009/LE071930552009012701T1-SC20190405182754/&quot;)), p193.2013 = list(paste0(IN.DIR, &quot;/193_052/2013/LE071930522013022301T1-SC20190405182200/&quot;), paste0(IN.DIR, &quot;/193_053/2013/LE071930532013022301T1-SC20190405182213/&quot;), paste0(IN.DIR, &quot;/193_054/2013/LE071930542013022301T1-SC20190405182152/&quot;), paste0(IN.DIR, &quot;/193_055/2013/LE071930552013022301T1-SC20190405182331/&quot;)), p193.2015 = list(paste0(IN.DIR, &quot;/193_052/2015/LC081930522015010401T1-SC20190405181512/&quot;), paste0(IN.DIR, &quot;/193_053/2015/LC081930532015010401T1-SC20190405181751/&quot;), paste0(IN.DIR, &quot;/193_054/2015/LC081930542015010401T1-SC20190402042510/&quot;), paste0(IN.DIR, &quot;/193_055/2015/LC081930552015010401T1-SC20190402042446/&quot;)), p193.2017 = list(paste0(IN.DIR, &quot;/193_052/2017/LC081930522017012501T1-SC20190405181511/&quot;), paste0(IN.DIR, &quot;/193_053/2017/LC081930532017012501T1-SC20190405181440/&quot;), paste0(IN.DIR, &quot;/193_054/2017/LC081930542017012501T1-SC20190405181458/&quot;), paste0(IN.DIR, &quot;/193_055/2017/LC081930552017012501T1-SC20190405181444/&quot;)), p193.2018 = list(paste0(IN.DIR, &quot;/193_052/2018/LC081930522018011201T1-SC20190405181524/&quot;), paste0(IN.DIR, &quot;/193_053/2018/LC081930532018011201T1-SC20190405181459/&quot;), paste0(IN.DIR, &quot;/193_054/2018/LC081930542018011201T1-SC20190405181510/&quot;), paste0(IN.DIR, &quot;/193_055/2018/LC081930552018011201T1-SC20190405181442/&quot;)), p193.2019 = list(paste0(IN.DIR, &quot;/193_052/2019/LC081930522019021601T1-SC20190405183839/&quot;), paste0(IN.DIR, &quot;/193_053/2019/LC081930532019021601T1-SC20190405181518/&quot;), paste0(IN.DIR, &quot;/193_054/2019/LC081930542019021601T1-SC20190405183609/&quot;), paste0(IN.DIR, &quot;/193_055/2019/LC081930552019021601T1-SC20190405181507/&quot;)), # Chemin WRS p194 ----------------------------- p194.1986 = list(paste0(IN.DIR, &quot;/194_052/1986/LT051940521986011101T1-SC20190405172804/&quot;), paste0(IN.DIR, &quot;/194_053/1986/LT051940531986011101T1-SC20190405172758/&quot;)), p194.1987 = list(paste0(IN.DIR, &quot;/194_052/1987/LT051940521986122901T1-SC20190405172903/&quot;), paste0(IN.DIR, &quot;/194_053/1987/LT051940531986122901T1-SC20190405174433/&quot;)), p194.1997 = list(paste0(IN.DIR, &quot;/194_052/1997/LT051940521997021001T1-SC20190405181746/&quot;), paste0(IN.DIR, &quot;/194_053/1997/LT051940531997021001T1-SC20190405173130/&quot;)), p194.2000 = list(paste0(IN.DIR, &quot;/194_052/2000/LE071940522000012601T1-SC20190405172721/&quot;), paste0(IN.DIR, &quot;/194_053/2000/LE071940532000012601T1-SC20190405172733/&quot;)), p194.2003 = list(paste0(IN.DIR, &quot;/194_052/2003/LE071940522002121701T1-SC20190405172823/&quot;), paste0(IN.DIR, &quot;/194_053/2003/LE071940532002121701T1-SC20190405172739/&quot;)), p194.2005 = list(paste0(IN.DIR, &quot;/194_052/2005/LE071940522004122201T1-SC20190405172700/&quot;), paste0(IN.DIR, &quot;/194_053/2005/LE071940532004122201T1-SC20190405172612/&quot;)), p194.2007 = list(paste0(IN.DIR, &quot;/194_052/2007/LT051940522007010501T1-SC20190405172919/&quot;), paste0(IN.DIR, &quot;/194_053/2007/LT051940532007010501T1-SC20190405172216/&quot;)), p194.2010 = list(paste0(IN.DIR, &quot;/194_052/2010/LE071940522010012101T1-SC20190405172745/&quot;), paste0(IN.DIR, &quot;/194_053/2010/LE071940532010012101T1-SC20190405173304/&quot;)), p194.2012 = list(paste0(IN.DIR, &quot;/194_052/2012/LE071940522012011101T1-SC20190405173146/&quot;), paste0(IN.DIR, &quot;/194_053/2012/LE071940532012011101T1-SC20190405172236/&quot;)), p194.2015 = list(paste0(IN.DIR, &quot;/194_052/2015/LC081940522015012701T1-SC20190405172055/&quot;), paste0(IN.DIR, &quot;/194_053/2015/LC081940532015012701T1-SC20190405172042/&quot;)), p194.2017 = list(paste0(IN.DIR, &quot;/194_052/2017/LC081940522016123101T1-SC20190405172058/&quot;), paste0(IN.DIR, &quot;/194_053/2017/LC081940532016123101T1-SC20190405172040/&quot;)), p194.2018 = list(paste0(IN.DIR, &quot;/194_052/2018/LC081940522017121801T1-SC20190405172038/&quot;), paste0(IN.DIR, &quot;/194_053/2018/LC081940532017121801T1-SC20190405174114/&quot;)), p194.2019 = list(paste0(IN.DIR, &quot;/194_052/2019/LC081940522019012201T1-SC20190405172019/&quot;), paste0(IN.DIR, &quot;/194_053/2019/LC081940532019012201T1-SC20190405172055/&quot;)) ) # Fonction pour traiter et fusionner un ensemble d&#39;images Landsat ============= # # @param in.image.dirs liste des répertoires des images à traiter # @param ext l&#39;étendue dà utiliser pour le recadrage des images # @param filename nom de fichier pour la sauvegarde de l&#39;image traitée # @param overwrite retraiter et écraser des images déjà existantes # @param log écrire les informations sur le processus dans un fichier # # @return objet raster de l&#39;image traitée (invisible) # prepare.landsat &lt;- function(in.image.dirs, ext=NULL, filename=NULL, overwrite=FALSE, log=TRUE) { # Charger le fichier, si le fichier existe et overwrite==FALSE if(!is.null(filename) &amp;&amp; file.exists(filename) &amp;&amp; overwrite==FALSE) { message(&quot;- loading from file &quot;, filename) out.image &lt;- stack(filename) } else { # Ouvrir le fichier journal si un nom de fichier et log==true if(!is.null(filename) &amp; log==TRUE) { dir.create(dirname(filename), recursive = TRUE, showWarnings = FALSE) logfile &lt;- file(sub(&quot;\\\\.[[:alnum:]]+$&quot;, &quot;.log&quot;, filename), open=&quot;wt&quot;) sink(logfile, type=&quot;output&quot;) sink(logfile, type=&quot;message&quot;) message(date()) } # Listes vides pour les images et des bandes de qualité images &lt;- list() qas &lt;- list() # Pour chaque image ... for(image.dir in in.image.dirs) { image.sensor &lt;- substr(basename(image.dir), 0,4) if(image.sensor==&quot;LC08&quot;) { regexp &lt;- &quot;^.*_(pixel_qa|band2|band3|band4|band5|band6|band7|evi|msavi|nbr|nbr2|ndmi|ndvi|savi).tif$&quot; } else { regexp &lt;- &quot;^.*_(pixel_qa|band1|band2|band3|band4|band5|band7|evi|msavi|nbr|nbr2|ndmi|ndvi|savi).tif$&quot; } image &lt;- stack(grep(regexp, dir(image.dir, full.names=TRUE), value=TRUE)) image.name &lt;- substr(names(image)[1], 1, 40) image.scene &lt;- paste0(substr(image.name, 11, 13), &quot;_&quot;, substr(image.name, 14, 16)) image.date &lt;- substr(image.name, 18, 21) image.path &lt;- as.numeric(substr(image.scene, 1, 3)) image.row &lt;- as.numeric(substr(image.scene, 5, 7)) # ... renommer les couches de l&#39;image names(image) &lt;- c(&quot;qa&quot;, SST.LSBANDS) message(&quot;- &quot;, image.name, &quot;: &quot;, appendLF = FALSE) # ... recadrer l&#39;image avec l&#39;étendue (le cas échéant) if(!is.null(ext)) { message(&quot;crop ext ... &quot;, appendLF = FALSE) image &lt;- crop(image, ext) } # ... recadrer et masquer avec WRS message(&quot;crop/mask WRS2 ... &quot;, appendLF = FALSE) wrs &lt;- spTransform(WRS[WRS$PATH==image.path &amp; WRS$ROW==image.row, ], CRS=crs(image)) image &lt;- mask(crop(image, wrs), wrs) # ... extraire la bande de qualité qa &lt;- image[[1]] image &lt;- dropLayer(image, 1) # ... supprimer les valeurs de réflectance non valides message(&quot;clean sr ... &quot;, appendLF = FALSE) for(i in 1:6) { image[[i]] &lt;- reclassify(image[[i]], cbind(-Inf, 0, NA), right=FALSE) image[[i]] &lt;- reclassify(image[[i]], cbind(10000, Inf, NA), right=TRUE) } # ... supprimer les valeurs d&#39;indices non valides for(i in 7:13) { image[[i]] &lt;- reclassify(image[[i]], cbind(-Inf, -10000, NA), right=FALSE) image[[i]] &lt;- reclassify(image[[i]], cbind(10000, Inf, NA), right=TRUE) } # ... mettre l&#39;ensemble de la pile à NA là où une seule couche est NA m &lt;- sum(image) image &lt;- mask(image, m) # ... ajouter l&#39;image et la bande de qualité aux listes correspondantes images[[length(images)+1]] &lt;- image qas[[length(qas)+1]] &lt;- qa } # Fusionner les images (scènes) dans les listes message(&quot;- merging scenes ... &quot;, appendLF = FALSE) out.image &lt;- do.call(merge, images) out.qa &lt;- do.call(merge, qas) # Sauvegarder l&#39;image fusionné dans un fichier if (!is.null(filename) &amp;&amp; (!file.exists(filename) || overwrite == TRUE)) { message(&quot;writing to file &quot;, filename, &quot; ... &quot;, appendLF = FALSE) out.image &lt;- writeRaster(out.image, filename = filename, overwrite = overwrite, datatype=&quot;INT2S&quot;, options=c(&quot;COMPRESS=NONE&quot;)) names(out.image) &lt;- SST.LSBANDS out.qa &lt;- writeRaster(out.qa, filename = sub(&quot;[.]tif$&quot;, paste0(&quot;_qa&quot;, image.sensor, &quot;.tif&quot;), filename), overwrite = overwrite, datatype=&quot;INT2S&quot;) } } message(&quot;done&quot;) print(out.image) # Fermer le fichier journal if(!is.null(filename) &amp; log==TRUE) { sink(type=&quot;output&quot;) sink(type=&quot;message&quot;) } # Retourner l&#39;image de manière invisible invisible(out.image) } # COMMENCER LE TRAITEMENT ##################################################### # Attention, peut facilement remplir le répertoire tmp ! # Peut-être seulement traiter une partie des images et ensuite redémarrer R # Étendue du Togo + 5km de tampon en UTM 30 pour le chemin p194 TGO.EXT.30 &lt;- extent(spTransform(TGO, UTM.30)) + 10000 # Pour chaque ensembles d&#39;images (année/chemin), en parallèle, ... registerDoParallel(CORES-1) foreach(i=1:length(in.image.list)) %dopar% { # ... extraire la liste des images à traiter in.image.dirs &lt;- in.image.list[[i]] name &lt;- unlist(strsplit(names(in.image.list[i]), &quot;[.]&quot;)) path &lt;- name[1] # p.ex. &quot;p192&quot; year &lt;- name[2] # p.ex. &quot;1986&quot; tile &lt;- name[3] # p.ex. &quot;NA&quot; (ou 1,2, ...) # ... étendue UTM 30 pour chemin p194 et UTM 31 pour les autres if(path == &quot;p194&quot;) tmp.ext &lt;- TGO.EXT.30 else tmp.ext &lt;- TGO.EXT # ... répertoire pour sauvegarder l&#39;image out.image.dir &lt;- paste0(OUT.DIR, &quot;/&quot;, path) if(!dir.exists(out.image.dir)) dir.create(out.image.dir) # ... nom du fichier if(is.na(tile)) { filename &lt;- paste0(out.image.dir, &quot;/&quot;, path, &quot;_&quot;, year, &quot;.tif&quot;) } else { filename &lt;- paste0(out.image.dir, &quot;/&quot;, path, &quot;_&quot;, year, &quot;_&quot;, tile, &quot;.tif&quot;) } # ... et faire le travail message(&quot;Processing &quot;, path, &quot;_&quot;, year) prepare.landsat(in.image.dirs, ext = tmp.ext, filename = filename, overwrite=FALSE, log=TRUE) } # Supprimer les fichiers temporaires dans le répertoir tmp tmp_dir &lt;- tempdir() files &lt;- list.files(tmp_dir, full.names = T, recursive=T) file.remove(files) # Fusionner les fichiers journaux for(dir in dir(paste0(OUT.DIR), full.names=TRUE)) { path &lt;- basename(dir) system(paste0(&quot;tail -n +1 &quot;, dir, &quot;/*.log &gt; &quot;, dir, &quot;/&quot;, path, &quot;.tmp&quot;)) system(paste0(&quot;rm &quot;, dir, &quot;/*.log&quot;)) system(paste0(&quot;mv &quot;, dir, &quot;/&quot;, path, &quot;.tmp &quot;, dir, &quot;/&quot;, path, &quot;.log&quot;)) } # Reprojection des images p194 de UTM 30 à UTM 31 ============================= p194.dir &lt;- paste0(OUT.DIR, &quot;/p194&quot;) # Pour chaque image p194, en parallèle, ... registerDoParallel(CORES-1) foreach(image=dir(p194.dir, pattern=&quot;.*[.]tif$&quot;)) %dopar% { image &lt;- paste0(p194.dir, &quot;/&quot;, image) image.utm30 &lt;- sub(&quot;[.]tif$&quot;, &quot;utm30.tif&quot;, image) file.rename(image, image.utm30) # ... transformer l&#39;image en utilisant l&#39;outil externe &quot;gdalwarp&quot; system(paste(&quot;gdalwarp&quot;, image.utm30, &quot;-t_srs &#39;+proj=utm +zone=31 +datum=WGS84&#39;&quot;, &quot;-tr 30 30&quot;, &quot;-te 147255 1017495 222165 1238265&quot;, image, &quot;-ot &#39;Int16&#39;&quot;, &quot;-overwrite&quot;)) # ... supprimer l&#39;image UTM 30 file.remove(image.utm30) } # Masquer images avec bande de qualité (nuages/ombres) ======================== registerDoParallel(CORES-1) foreach(file=dir(OUT.DIR, pattern=&quot;.*\\\\_[[:digit:]]+\\\\.tif$&quot;, full.names = TRUE, recursive=TRUE)) %dopar% { qa &lt;- raster(dir(dirname(file), pattern=gsub(&quot;\\\\_&quot;, &quot;\\\\_&quot;, sub(&quot;\\\\.tif&quot;, &quot;_qa*&quot;, basename(file))), full.names=TRUE)) image &lt;- mask(brick(file), qa %in% c(QA.CLOUD, QA.SHADOW, QA.WATER, QA.ICE), maskvalue=TRUE) writeRaster(image, sub(&quot;\\\\.tif&quot;, &quot;_m.tif&quot;, file), overwrite = TRUE, datatype=&quot;INT2S&quot;, options=c(&quot;COMPRESS=NONE&quot;)) } # Créer des vignettes de chaque image ========================================= registerDoParallel(CORES-1) foreach(filename=dir(OUT.DIR, pattern=&quot;p19.*[.]tif$&quot;, recursive=TRUE, full.names=TRUE)) %dopar% { image &lt;- brick(filename) jpeg(sub(&quot;[.]tif$&quot;, &quot;.jpeg&quot;, filename), width=1350, height=3000) par(plt=c(0,1,0,1)) plot(spTransform(TGO, UTM.31)) plotRGB(image, r=6, g=5, b=3, stretch=&quot;lin&quot;, add=TRUE) plot(mask(image[[1]], spTransform(TGO, UTM.31), inverse=TRUE), col=&quot;#FFFFFF66&quot;, legend=FALSE, add=TRUE) plot(spTransform(TGO, UTM.31), add=TRUE, lwd=3) dev.off() } "],
["02_Worldclim.html", "", " 2.1.2 WorldClim Les variables climatiques ont une grande influence sur les caractéristiques des forêts. Ils sont donc importants pour l’évaluation des surfaces forestières et de la biomasse sur la base d’images satellitaires. Les données climatiques de WorldClim version 2 ont servi comme base. La température annuelle moyenne (BIO1), la saisonnalité de la température (BIO4), la précipitation annuelle (BIO12) et la saisonnalité de la précipitation (BIO15) étant identifiées comme les variables WorldClim les plus importantes pour l’évaluation des forêts. 2.1.2.1 Acquisition des données Des données climatiques historiques (1970 – 2020) avec une résolution de 30 arcsecondes (environ 1 km2 sur l’équateur) disponible à WorldClim version 2 ont été utilisées. Pour l’analyse, des variables climatiques mensuelles ont été utilisées : prec: Précipitations pour les mois de janvier à décembre (mm) tmax: Température maximale pour les mois de janvier à décembre (ºC) tavg: Température moyenne pour les mois de janvier à décembre (ºC) tmin: Température minimale pour les mois de janvier à décembre (ºC) Et également les variables bioclimatiques qui en découlent : BIO1: Température moyenne annuelle BIO2: Fourchette diurne moyenne (moyenne des températures mensuelles (max temp - min temp)) BIO3: Isothermie (BIO2/BIO7) (×100) BIO4: Saisonnalité de la température (écart-type ×100) BIO5: Température maximale du mois le plus chaud BIO6: Température minimale du mois le plus froid BIO7: Gamme de température annuelle (BIO5-BIO6) BIO8: Température moyenne du trimestre le plus humide BIO9: Température moyenne du trimestre le plus sec BIO10: Température moyenne du trimestre le plus chaud BIO11: Température moyenne du trimestre le plus froid BIO12: Précipitation annuelle BIO13: Précipitation du mois le plus humide BIO14: Précipitation du mois le plus sec BIO15: Saisonalité de la précipitation (Coefficient de variation) BIO16: Précipitation du trimestre le plus humide BIO17: Précipitation du trimestre le plus sec BIO18: Précipitation du trimestre le plus chaud BIO19: Précipitation du trimestre le plus froid 2.1.2.2 Prétraitement des données Les données sont lues et reprojetées sur le raster des images Landsat (UTM 31, résolution de 30 mètres) et coupées à la taille. Example Données bioclimatiques WorldClim version 2: température annuelle moyenne (BIO1) / saisonalité de la température (BIO4) / précipitation annuelle (BIO12) / saisonalité de la précipitation (BIO15) Script R: 01_SSTS/01_data/_src/prep-Worldclim.R ############################################################################### # prep-Worldclim.R: lire et reprojeter les données de WorldClim # ----------------------------------------------------------------------------- # Bern University of Applied Sciences # Oliver Gardi, &lt;oliver.gardi@bfh.ch&gt; # 13 Mai 2020 # Définitions des variables =================================================== IN.DIR &lt;- paste0(DIR.RAW.DAT, &quot;/Worldclim&quot;) OUT.DIR &lt;- paste0(DIR.SST.DAT, &quot;/Worldclim&quot;) if(!dir.exists(OUT.DIR)) dir.create(OUT.DIR) # Reprojection images WorldClim vers Landsat (résolution 30m, UTM 31, ...) ==== foreach(file=dir(IN.DIR, pattern=&quot;.*Togo[.]tif$&quot;)) %dopar% { system(paste(&quot;gdalwarp&quot;, paste0(IN.DIR, &quot;/&quot;, file), &quot;-t_srs &#39;+proj=utm +zone=31 +datum=WGS84&#39;&quot;, &quot;-tr 30 30&quot;, paste(&quot;-te&quot;, TGO.EXT@xmin, TGO.EXT@ymin, TGO.EXT@xmax, TGO.EXT@ymax), paste0(OUT.DIR, &quot;/&quot;, file), &quot;-dstnodata -3.4e+38&quot;, &quot;-co COMPRESS=&#39;LZW&#39;&quot;, &quot;-co INTERLEAVE=&#39;BAND&#39;&quot;, &quot;-overwrite&quot;)) system(paste(&quot;gdalinfo -stats&quot;, paste0(OUT.DIR, &quot;/&quot;, file))) } # Créer des vignettes pour les donées WorldClim =============================== foreach(file=dir(OUT.DIR, pattern=&quot;.*[.]tif$&quot;)) %dopar% { image &lt;- stack(paste0(OUT.DIR, &quot;/&quot;, file)) type &lt;- unlist(strsplit(file, &quot;_&quot;))[3] if (type == &quot;prec&quot;) { zlim &lt;- c(0,320); col &lt;- rev(topo.colors(255)) } else if (type == &quot;tmin&quot;) { zlim &lt;- c(14.0,27.8); col &lt;- rev(heat.colors(255)) } else if (type == &quot;tmax&quot;) { zlim &lt;- c(24.9,37.5); col &lt;- rev(heat.colors(255)) } else if (type == &quot;tavg&quot;) { zlim &lt;- c(19.7,32.7); col &lt;- rev(heat.colors(255)) } else { zlim &lt;- NA; col &lt;- rev(cm.colors(255)) } foreach(i=1:nlayers(image)) %dopar% { jpeg(paste0(OUT.DIR, &quot;/&quot;, sub(&quot;[.]tif$&quot;, &quot;&quot;, file), &quot;-&quot;, str_pad(i, 2, &quot;left&quot;, 0), &quot;.jpeg&quot;), width=1350, height=3000) plot(image[[i]], col=col, zlim=zlim) plot(mask(image[[i]], TGO, inverse=TRUE), col=&quot;#FFFFFF66&quot;, legend=FALSE, add=TRUE) plot(TGO, add=TRUE, lwd=3) dev.off() } } "],
["03_SRTM.html", "", " 2.1.3 SRTM Les variables topographiques telles que l’élévation, la pente et l’aspect ne sont actuellement utilisées ni pour l’évaluation des forestières ni pour l’évaluation de la biomasse, car il a été constaté que ces variables ont peu d’influence sur le pouvoir explicatif des modèles de classification ou de régression. Néanmoins, les données sont conservées et disponibles pour autres analyses. 2.1.3.1 Acquisition des données Les données topographiques du SRTM ont été obtenues à partir de deux sources: Les données originales avec une résolution spatiale de 1 seconde d’arc (environ 30 mètres sur l’équateur) ont été obtenues du jeu de données SRTM 1 Arc-Second Global disponible sur USGS Earthexplorer. Ces données à haute résolution ont des fois des lacunes (manque des données). Les données SRTM ajustées avec une résolution de 3 secondes d’arc ont été obtenues à partir de CGIAR SRTM 90m Digital Elevation Database v4.1 pour combler les lacunes des données à haute résolution. 2.1.3.2 Prétraitement des données Les données de 1 seconde d’arc sont lues et les lacunes éventuelles sont comblées avec les données ajustées de 3 secondes d’arc. Enfin, les données sont reprojetées sur le raster des images Landsat (UTM 31, résolution de 30 mètres) et coupées à la taille. Script R: 01_SSTS/01_data/_src/prep-SRTM.R ############################################################################### # prep-SRTM.R: lire, nettoyer et empiler des données topographiques SRTM # ----------------------------------------------------------------------------- # Bern University of Applied Sciences # Oliver Gardi, &lt;oliver.gardi@bfh.ch&gt; # 13 Mai 2020 # Définitions des variables =================================================== IN.DIR &lt;- paste0(DIR.RAW.DAT, &quot;/SRTM&quot;) OUT.DIR &lt;- paste0(DIR.SST.DAT, &quot;/SRTM&quot;) if(!dir.exists(OUT.DIR)) dir.create(OUT.DIR) # Préparation du modèle numérique d&#39;élévation SRTM ============================ # Lire 90m SRTM MNE sans vides (source: http://srtm.csi.cgiar.org/srtmdata/), dem.90 &lt;- do.call(merge, lapply(as.list(dir(paste0(IN.DIR, &quot;/3arcsecond&quot;), pattern=&quot;.*[.]tif$&quot;, full.names=TRUE)), raster)) # Lire 30m SRTM MNE (source: USGS Earthexplorer) et remplir vides avec 90m SRTM dem.30 &lt;- foreach(tile=dir(paste0(IN.DIR, &quot;/1arcsecond&quot;), pattern=&quot;.*[.]tif$&quot;, full.names=TRUE), .combine=merge, .multicombine=TRUE) %dopar% { dem.30.t &lt;- raster(tile) dem.90.t &lt;- round(projectRaster(dem.90, dem.30.t)) merge(dem.30.t, dem.90.t) } # Reprojection d&#39;images MNE vers Landsat (résolution 30m, UTM 31, ...) writeRaster(dem.30, paste0(OUT.DIR, &quot;/SRTM-1arcsec_raw.tif&quot;), datatype=&quot;INT2S&quot;, overwrite=TRUE) system(paste(&quot;gdalwarp&quot;, paste0(OUT.DIR, &quot;/SRTM-1arcsec_raw.tif&quot;), &quot;-t_srs &#39;+proj=utm +zone=31 +datum=WGS84&#39;&quot;, &quot;-tr 30 30&quot;, paste(&quot;-te&quot;, TGO.EXT@xmin, TGO.EXT@ymin, TGO.EXT@xmax, TGO.EXT@ymax), paste0(OUT.DIR, &quot;/SRTM-1arcsec.tif&quot;), &quot;-ot &#39;Int16&#39;&quot;, &quot;-co COMPRESS=&#39;LZW&#39;&quot;, &quot;-co INTERLEAVE=&#39;BAND&#39;&quot;, &quot;-overwrite&quot;)) file.remove(paste0(OUT.DIR, &quot;/SRTM-1arcsec_raw.tif&quot;)) # Calculer les statistiques GDAL (min, max, ...) system(paste(&quot;gdalinfo -stats&quot;, paste0(OUT.DIR, &quot;/SRTM-1arcsec.tif&quot;))) # Créer une vignettes du MNE ================================================== dem &lt;- raster(paste0(OUT.DIR, &quot;/SRTM-1arcsec.tif&quot;)) jpeg(paste0(OUT.DIR, &quot;/SRTM-1arcsec.jpeg&quot;), width=1350, height=3000) par(plt=c(0,1,0,1)) plot(dem) plot(mask(dem, TGO, inverse=TRUE), col=&quot;#FFFFFF66&quot;, legend=FALSE, add=TRUE) plot(TGO, add=TRUE, lwd=3) dev.off() "],
["04_SoilGrids.html", "", " 2.1.4 SoilGrids Les données SoilGrids sur le type de sol et le carbone du sol n’ont pas encore été incluses dans les analyses de la surface forestière et des modifications du stockage du carbone dues au changement d’utilisation des terres. Ils sont disponibles pour des analyses complémentaires. 2.1.4.1 Acquisition des données Les données sur les sols proviennent des archives SoilGrids (documentation SoilGrids 2017) 2.1.4.2 Prétraitement des données Les données sont lues et reprojetées sur le raster des images Landsat (UTM 31, résolution de 30 mètres) et coupées à la taille. Example Données pédologiques SoilGrids: Typologie des sols WRB à gauche, Carbone du sol à 30 cm de profondeur (tC/ha) à droite Script R: 01_SSTS/01_data/_src/prep-SoilGrids.R ############################################################################### # prep-SoilGrids.R: lire et reprojeter des données SoilGrids # ----------------------------------------------------------------------------- # Bern University of Applied Sciences # Oliver Gardi, &lt;oliver.gardi@bfh.ch&gt; # 13 Mai 2020 # Source des données: https://files.isric.org/soilgrids/data/recent/ # Documentation: https://www.isric.org/explore/soilgrids/faq-soilgrids-2017 # Définitions des variables =================================================== IN.DIR &lt;- paste0(DIR.RAW.DAT, &quot;/SoilGrids&quot;) OUT.DIR &lt;- paste0(DIR.SST.DAT, &quot;/SoilGrids&quot;) if(!dir.exists(OUT.DIR)) dir.create(OUT.DIR) # Reprojection oilGrids vers Landsat (résolution 30m, UTM 31, ...) ============ foreach(file=dir(IN.DIR, pattern=&quot;.*[.]tif$&quot;)) %do% { system(paste(&quot;gdalwarp&quot;, paste0(IN.DIR, &quot;/&quot;, file), &quot;-t_srs &#39;+proj=utm +zone=31 +datum=WGS84&#39;&quot;, &quot;-tr 30 30&quot;, paste(&quot;-te&quot;, TGO.EXT@xmin, TGO.EXT@ymin, TGO.EXT@xmax, TGO.EXT@ymax), paste0(OUT.DIR, &quot;/&quot;, file), &quot;-dstnodata -3.4e+38&quot;, &quot;-co COMPRESS=&#39;LZW&#39;&quot;, &quot;-co INTERLEAVE=&#39;BAND&#39;&quot;, &quot;-overwrite&quot;)) system(paste(&quot;gdalinfo -stats&quot;, paste0(OUT.DIR, &quot;/&quot;, file))) } # Créer des vignettes pour les donées SoilGrids =============================== # Types de sol WRB ------------------------------ jpeg(paste0(OUT.DIR, &quot;/TAXNWRB_250m_ll.jpeg&quot;), width=675, height=1200) soil.types &lt;- raster(paste0(OUT.DIR, &quot;/TAXNWRB_250m_ll.tif&quot;)) soil.types &lt;- as.factor(soil.types) cats &lt;- levels(soil.types)[[1]] attr &lt;- read.csv(paste0(IN.DIR, &quot;/TAXNWRB_250m_ll.tif.csv&quot;)) attr &lt;- merge(cats, attr[,c(&quot;Number&quot;, &quot;Group&quot;, &quot;WRB_group&quot;, &quot;R&quot;, &quot;G&quot;, &quot;B&quot;)], by.x = &quot;ID&quot;, by.y = &quot;Number&quot;) cats[[&quot;Soil Type&quot;]] &lt;- attr$Group levels(soil.types) &lt;- cats levelplot(soil.types, col.regions=paste0(&quot;#&quot;, as.hexmode(tmp$R),as.hexmode(tmp$G),as.hexmode(tmp$B)), xlab=&quot;&quot;, ylab=&quot;&quot;) + levelplot(mask(soil.types, TGO, inverse=TRUE), col.regions=&quot;#FFFFFF66&quot;) + layer(sp.polygons(TGO, lwd=3)) dev.off() # Réservoire carbone à 30 cm -------------------- jpeg(paste0(OUT.DIR, &quot;/OCSTHA_M_30cm_250m_ll.jpeg&quot;), width=675, height=1200) image &lt;- raster(paste0(OUT.DIR, &quot;/OCSTHA_M_30cm_250m_ll.tif&quot;)) raster::plot(image, zlim=c(30,100)) raster::plot(mask(image, TGO, inverse=TRUE), col=&quot;#FFFFFF66&quot;, legend=FALSE, add=TRUE) sp::plot(TGO, add=TRUE, lwd=3) dev.off() # Tableau des surfaces des catégories de sol selon GIEC ====================== tmp &lt;- as.data.frame(table(mask(soil.types, TGO)[]) * 30^2/10000) tmp &lt;- merge(tmp, attr, by.x=&quot;Var1&quot;, by.y=&quot;ID&quot;) tmp$IPCC &lt;- NA tmp$IPCC[tmp$WRB_group %in% c(&quot;Leptosols&quot;,&quot;Vertisols&quot;,&quot;Kastanozems&quot;,&quot;Chernozems&quot;, &quot;Phaeozems&quot;,&quot;Luvisols&quot;,&quot;Alisols&quot;,&quot;Albeluvisols&quot;, &quot;Solonetz&quot;,&quot;Calcisols&quot;,&quot;Gypsisols&quot;,&quot;Umbrisols&quot;, &quot;Cambisols&quot;,&quot;Regosols&quot;)] &lt;- &quot;HAC&quot; tmp$IPCC[tmp$WRB_group %in% c(&quot;Acrisols&quot;,&quot;Lixisols&quot;,&quot;Nitisols&quot;,&quot;Ferralsols&quot;, &quot;Durisols&quot;)] &lt;- &quot;LAC&quot; tmp$IPCC[tmp$WRB_group %in% c(&quot;Arenosols&quot;)] &lt;- &quot;SAN&quot; tmp$IPCC[tmp$WRB_group %in% c(&quot;Podzols&quot;)] &lt;- &quot;POD&quot; tmp$IPCC[tmp$WRB_group %in% c(&quot;Andosols&quot;)] &lt;- &quot;VOL&quot; tmp$IPCC[tmp$WRB_group %in% c(&quot;Gleysols&quot;)] &lt;- &quot;WET&quot; tmp$IPCC[is.na(tmp$WRB_group)] &lt;- &quot;Other&quot; "],
["05_ProREDD.html", "", " 2.1.5 ProREDD Dans le cadre du premier inventaire forestier national de 2015/16, le projet GIZ “ProREDD” a réalisé une carte d’occupation des sols à partir d’images RapidEye des années 2013/14 (Rapport méthodes et résultats). La carte n’est pas utilisée pour l’analyse de l’évolution des surfaces forestières dans le cadre du NRF-MRV REDD+, mais elle est disponible comme carte comparative et pour d’autres analyses. 2.1.5.1 Acquisition des données Les cartes pour les différentes régions ont été fournies par L’Unité de gestion de bases de données cartographiques (UGBDC) de la Direction des études et de la planification (DEP) le ministère sous forme de Shapefiles. 2.1.5.2 Prétraitement des données Les shapefiles sont lus et convertis avec l’outil “grid_gridding” de SAGA GIS en données raster avec résolution originale des images RapidEye de 5 mètres, puis reprojetés sur le raster Landsat de 30 mètres. Example Script R: 01_SSTS/01_data/_src/prep-ProREDD.R ############################################################################### # prep-ProREDD.R: rasterizer la carte d&#39;occupation des terres RapidEye # ----------------------------------------------------------------------------- # Bern University of Applied Sciences # Oliver Gardi, &lt;oliver.gardi@bfh.ch&gt; # 13 Mai 2020 # Définitions des variables =================================================== IN.DIR &lt;- paste0(DIR.RAW.DAT, &quot;/RapidEye/shapefiles&quot;) OUT.DIR &lt;- paste0(DIR.SST.DAT, &quot;/ProREDD&quot;) if(!dir.exists(OUT.DIR)) dir.create(OUT.DIR) # Rasterizer Shapefiles avec l&#39;outil grid_gridding de SAGA ==================== files &lt;- dir(IN.DIR, pattern=&quot;\\\\.shp$&quot;, recursive=TRUE, full.names=TRUE) registerDoParallel(CORES - 1) foreach(file=files) %dopar%{ out.file &lt;- sub(&quot;shp$&quot;, &quot;sdat&quot;, file) system(paste0(&quot;saga_cmd grid_gridding \\&quot;Shapes to Grid\\&quot; &quot;, &quot;-TARGET_DEFINITION 0 -INPUT \\&quot;&quot;, file, &quot;\\&quot; &quot;, &quot;-FIELD \\&quot;code\\&quot; -OUTPUT 2 -MULTIPLE 0 -LINE_TYPE 0 &quot;, &quot;-POLY_TYPE 0 -GRID_TYPE 2 -TARGET_USER_SIZE 30.0 &quot;, &quot;-TARGET_USER_FITS 0 -GRID \\&quot;&quot;, out.file, &quot;\\&quot;&quot;)) } # Lire et fusionner les différentes scènes raster scenes &lt;- lapply(dir(IN.DIR, pattern=&quot;\\\\.sdat$&quot;, recursive=TRUE, full.names=TRUE), raster) scenes[&quot;tolerance&quot;] &lt;- 0.4 RE &lt;- do.call(merge, scenes) # À reviser: Reprojection sur l&#39;image Landsat AGB &lt;- raster(&quot;../output/3_forest-biomass/2_ref-maps/TGO_2015_AGB.tif&quot;) RE_resampled &lt;- resample(RE, AGB, method=&quot;ngb&quot;) writeRaster(RE_resampled, paste0(OUT.DIR, &quot;/TGO_30m.tif&quot;), datatype=&quot;INT2S&quot;) "],
["01_sampling-grid.html", "2.2 Collecte des données", " 2.2 Collecte des données 2.2.1 Réseau d’échantillonnage Les parcelles d’échantillonage couvrent une surface de 30 x 30 mètres et correspondent en taille, forme et position aux pixels des images Landsat. Les parcelles d’échantillonnage sont régulièrement réparties sur le territoire du Togo, sur une grille d’une taille de maille de 480 mètres (tous les 16 pixels Landsat) ou 4,3 parcelles d’échantillonnage par km2. Sur l’ensemble du territoire togolais, cela donne un réseau de 250 000 parcelles. Si nécessaire pour autres utilisation, la taille de maille de 480 mètres permet d’enlargir la maille à 679 ou 960 mètres. D’autre part, il est également possible de comprimer la maille à 340, 240, 170, 120, 85, … mètres. La grille d’échantillonnage est basée sur l’étendue du Togo (alignée avec la grille Landsat) et la taille du maillage. Les points d’échantillonnage résultants sont attribués avec les coordonnées x et y et l’ID correspondant. La grille de points résultante est enregistrée sous forme de shapefile (télécharger archive ZIP). Script R: 01_SSTS/02_BdD/_src/create-grid.R ############################################################################### # create-grid.R: créer une grille de points d&#39;observation SSTS # ----------------------------------------------------------------------------- # Bern University of Applied Sciences # Oliver Gardi, &lt;oliver.gardi@bfh.ch&gt; # 13 Mai 2020 # Définitions des variables =================================================== OUT.DIR &lt;- paste0(DIR.SST.BDD, &quot;/01_reseau-SSTS&quot;) if(!dir.exists(OUT.DIR)) dir.create(OUT.DIR) RES &lt;- 480 # Résolution de la grille (mètres) # Grille d&#39;observation ======================================================== # Coordonnées min/max en ligne avec images Landsat x.min &lt;- RES * extent(TGO)@xmin %/% RES x.max &lt;- RES * extent(TGO)@xmax %/% RES y.min &lt;- RES * extent(TGO)@ymin %/% RES y.max &lt;- RES * extent(TGO)@ymax %/% RES # Creation de la grille frame.points &lt;- SpatialPoints(expand.grid(seq(x.min, x.max, by=RES), seq(y.min, y.max, by=RES)), proj4string=UTM.31)[TGO] # Ajouter attribues PLOTID, xcoords et ycoords frame.points$xcoords &lt;- frame.points@coords[,1] frame.points$ycoords &lt;- frame.points@coords[,2] frame.points$PLOTID &lt;- paste0(str_pad(frame.points@xcoords, 7, &quot;left&quot;, &quot;0&quot;), &quot;_&quot;, str_pad(frame.points@ycoords, 7, &quot;left&quot;, &quot;0&quot;)) # Sauveguarder comme fichier Shapefile writeOGR(frame.points, dsn=OUT.DIR, layer=&quot;TGO_frame_480m&quot;, driver=&quot;ESRI Shapefile&quot;, overwrite=TRUE) "],
["02_training-plots.html", "", " 2.2.2 Parcelles d’entraînement 2.2.2.1 Échantillonnage Les parcelles d’entraînement sont sélectionnées de manière à couvrir la plus large gamme possible de couverture végétale sur l’ensemble du territoire du Togo. À cette fin, le NDVI comme indicateur du couvert végétal est attribué à tout les points du réseau d’échantillonnage et stratifiées en 10 classes NDVI. Un échantillon aléatoire spatialement équilibré de 1 500 échantillons est ensuite tiré de chaque strate NDVI (generalized random tesselation). 2.2.2.2 Couverture des houppiers Pour détérminer la couverture des houppiers une grille de 7 x 7 points est définit à l’intérieur de chaque parcelles d’échantillonage. Pour chaque point de ce grille, les photo-interprètes détérminent sur base des images de très haute résolution disponible en GoogleEarth, si le point touche l’houppier d’un arbre ou non. La couverture des houppiers est en suite détérminé par le nombre des points qui tombent sur un arbre par rapport au nombre total des points (n = 49). L’attribution est fait par les photo-interprètes en QGIS, avec l’image GoogleEarth comme carte de base. La source et la date d’acquistion de l’image GoogleEarth utilisé est également enregistré. Elle est obtenu par GoogleEarth Pro (plugin QGIS send2google_earth). Script R: 01_SSTS/02_BdD/_src/create-train-plots.R ############################################################################### # create-train-plots.R: Créer un ensemble de parcelles d&#39;entraînement # ----------------------------------------------------------------------------- # Bern University of Applied Sciences # Oliver Gardi, &lt;oliver.gardi@bfh.ch&gt; # 13 Mai 2020 # Préparation des variables =================================================== OUT.DIR &lt;- paste0(DIR.SST.BDD, &quot;/02_train-plots/empty&quot;) if(!dir.exists(OUT.DIR)) dir.create(OUT.DIR, recursive=TRUE) # Charger la grille d&#39;échantillonnage du SSTS frame.points &lt;- readOGR(paste0(DIR.SST.BDD, &quot;/01_reseau-SSTS/TGO_frame_480m.shp&quot;)) # Charger NDVI 2018 masquée (sans nuages, ombre, ...) ndvi.band &lt;- which(SST.LSBANDS == &quot;ndvi&quot;) ndvi &lt;- merge(raster(paste0(DIR.SST.DAT, &quot;/Landsat/p192/p192_2018_m.tif&quot;), band=ndvi.band), raster(paste0(DIR.SST.DAT, &quot;/Landsat/p193/p193_2018_m.tif&quot;), band=ndvi.band), raster(paste0(DIR.SST.DAT, &quot;/Landsat/p194/p194_2018_m.tif&quot;), band=ndvi.band)) # Extraire le NDVI pour chaque parcelle et le découper en 10 strates frame.points$ndvi &lt;- raster::extract(ndvi, frame.points) frame.points$ndvi_c &lt;- cut(frame.points$ndvi, 10, labels=paste0(&quot;s&quot;, 0:9)) # Echantillonnage des parcelles d&#39;entraînement ================================ # Initialiser le générateur de nombres aléatoires set.seed(RSEED) # Définition d&#39;échantillonnage (1500 échantillons par strate NDVI s0 - s9) Dsgn.grt &lt;- list(&quot;s0&quot;=list(panel=c(PanelOne=1500), seltype=&quot;Equal&quot;), &quot;s1&quot;=list(panel=c(PanelOne=1500), seltype=&quot;Equal&quot;), &quot;s2&quot;=list(panel=c(PanelOne=1500), seltype=&quot;Equal&quot;), &quot;s3&quot;=list(panel=c(PanelOne=1500), seltype=&quot;Equal&quot;), &quot;s4&quot;=list(panel=c(PanelOne=1500), seltype=&quot;Equal&quot;), &quot;s5&quot;=list(panel=c(PanelOne=1500), seltype=&quot;Equal&quot;), &quot;s6&quot;=list(panel=c(PanelOne=1500), seltype=&quot;Equal&quot;), &quot;s7&quot;=list(panel=c(PanelOne=1500), seltype=&quot;Equal&quot;), &quot;s8&quot;=list(panel=c(PanelOne=1500), seltype=&quot;Equal&quot;), &quot;s9&quot;=list(panel=c(PanelOne=1500), seltype=&quot;Equal&quot;) ) # Échantillonnage spatialement équilibrée (Generalized Random Tesselation) train.points &lt;- grts(sp.object=frame.points, # grille d&#39;échantillonnage src.frame=&quot;sp.object&quot;, # type de grille (objet spatiale) design=Dsgn.grt, # définition d&#39;échantillonnage stratum=&quot;ndvi_c&quot;, # colonne avec strate type.frame=&quot;finite&quot;, # type d&#39;échantillonnage DesignID=&quot;train&quot; # préfixe pour chaque point ) # Appliquer le système de référence des coordonnées proj4string(train.points) &lt;- proj4string(frame.points) # Mélanger les points d&#39;entraînement et ajouter un identifiant train.points &lt;- train.points[sample(1:nrow(train.points)), ] train.points$SAMPLEID &lt;- paste0(&quot;trn-&quot;, str_pad(string=1:nrow(train.points), width = 4, pad = &quot;0&quot;, side = &quot;left&quot;)) # Conversion des points en parcelles et ajouter des attributs ================= # Grille Landsat landsat.grid &lt;- raster(ndvi) values(landsat.grid) &lt;- 1 # Selectionner les pixels Landsat correspondantes et convertir en polygone train.plots &lt;- rasterToPolygons(mask(landsat.grid, train.points)) # Extraire les attributs des points d&#39;entraînement ... train.plots@data &lt;- over(train.plots, train.points[, c(&quot;PLOTID&quot;, &quot;SAMPLEID&quot;, &quot;xcoords&quot;, &quot;ycoords&quot;, &quot;ndvi&quot;, &quot;stratum&quot;)]) # ... et compléter avec autres attributs train.plots$ccov &lt;- as.character(NA) # couverture houppiers train.plots$img_src &lt;- train.plots$img_date &lt;- as.character(NA) # source et date d&#39;image train.plots$author &lt;- train.plots$mod_date &lt;- as.character(NA) # auteur et date # Sauvegarder parcelles sous format Shapefile et KML writeOGR(train.plots, dsn=OUT.DIR, layer=&quot;COV_parcelles&quot;, driver=&quot;ESRI Shapefile&quot;, overwrite=TRUE) writeKML(train.plots, kmlname=&quot;COV_parcelles&quot;, filename=paste0(OUT.DIR, &quot;/COV_parcelles.kml&quot;)) # Créer une grille d&#39;échantillon 7x7 dans chaque parcelle ===================== # Détérminer la grille grid.size &lt;- 7 res &lt;- res(landsat.grid)[1] offset &lt;- c(res/grid.size/2 + (0:(grid.size-1))*res/grid.size) # Diviser les parcelles pour un traitement parallèle subsets &lt;- split(train.plots, f=1:(CORES-1)) registerDoParallel(CORES-1) train.grids &lt;- foreach(subset=subsets, .combine=rbind, .multicombine=TRUE) %dopar% { # Créer un couche de points vides ... grids &lt;- SpatialPointsDataFrame(data.frame(x = 0, y = 0), data=data.frame(PLOTID = 0, SAMPLEID = 0, GRIDPOINT = 0))[-1,] # ... et ajoute la grille d&#39;échantillon pour chaque parcelle for(p in 1:length(subset)) { plot &lt;- subset[p,] ext &lt;- extent(plot) grids &lt;- bind(grids, SpatialPointsDataFrame(expand.grid(ext@xmin+offset, ext@ymin+offset), data=data.frame(PLOTID = plot$PLOTID, SAMPLEID = plot$SAMPLEID, GRIDPOINT = 1:grid.size^2))) } grids } # Appliquer le système de référence des coordonnées proj4string(train.grids) &lt;- proj4string(train.plots) # Ajouter un attribut &quot;arbre&quot; (1: oui, 0: non) train.grids$tree &lt;- as.integer(NA) # Sauveguarder les grille d&#39;échantillon sous format Shapefile writeOGR(train.grids, dsn=OUT.DIR, layer=&quot;COV_parcelles_grid&quot;, driver=&quot;ESRI Shapefile&quot;, overwrite=TRUE) # Diviser parcelles et grilles d&#39;échantillon en 10 sous-ensembles ============= # pour le traitement par différents photo-interprètes subsets &lt;- split(train.plots, f=1:10) for(i in 1:length(subsets)) { # Sauvegarder parcelles sous format Shapefile et KML writeOGR(subsets[[i]], dsn=OUT.DIR, layer=paste0(&quot;COV_parcelles_&quot;, i), driver=&quot;ESRI Shapefile&quot;, overwrite=TRUE) writeKML(subsets[[i]], kmlname=paste0(&quot;COV_parcelles_&quot;, i) , filename=paste0(OUT.DIR, &quot;/COV_parcelles_&quot;, i, &quot;.kml&quot;)) # Sauvegarder les grilles correspondantes sous format Shapefile subset.grids &lt;- train.grids[train.grids$PLOTID %in% subsets[[i]]$PLOTID,] writeOGR(subset.grids, dsn=OUT.DIR, layer=paste0(&quot;COV_parcelles_&quot;, i, &quot;_grid&quot;), driver=&quot;ESRI Shapefile&quot;, overwrite=TRUE) } "],
["03_validation-plots.html", "", " 2.2.3 Parcelles de validation 2.2.3.1 Échantillonnage La sélection des parcelles de validation est basée sur les cartes forêt/non-forêt, notamment leurs transitions entre les années 1987 – 2003 – 2015 – 2018. Tout d’abord les transitions sont déterminées pour chaque point du réseau d’échantillonnage. Ensuite un échantillon aléatoire stratifié est tirré avec une répartition de l’échantillon aux strates (transitions) qui est la valeur moyenne d’une répartition proportionnelle à la taille des strates et d’une répartion égale. 2.2.3.2 Occupation des terres L’occupation des terres et le changement de l’occupation des terres est déterminé sur base des images Landsat. Trois différentes catégories sont distingués: forêt (couverture houppier ≥ 30%) terre boisée (couverture houppier entre 10% et 30%) non-forêt (couverture houppier &lt; 10%) La couverture des houppiers est utilisé pour détérminer l’occupation des terres sur l’image Landsat de la date correspondate. À partir de cette référence, l’occupation des terres est détérminé pour les autres dates de référence 1987 – 2003 – 2015 – 2018. La figure suivante illustre la procédure: Script R: 01_SSTS/02_BdD/_src/create-val-plots.R ############################################################################### # create-val-points.R: Créer un ensemble de parcelles de validation # ----------------------------------------------------------------------------- # Bern University of Applied Sciences # Oliver Gardi, &lt;oliver.gardi@bfh.ch&gt; # 13 Mai 2020 # Préparation des variables =================================================== OUT.DIR &lt;- paste0(DIR.SST.BDD, &quot;/03_val-plots/empty&quot;) if(!dir.exists(OUT.DIR)) dir.create(OUT.DIR, recursive=TRUE) IN.DIR &lt;- paste0(DIR.MRV.MCF, &quot;/2_raw-maps/FC30/TGO&quot;) # Fusionner les cartes des années de référence (1987, 2003, 2015 et 2018) maps &lt;- merge(raster(paste0(IN.DIR, &quot;/TGO_1987_F30r.tif&quot;)), raster(paste0(IN.DIR, &quot;/TGO_2003_F30r.tif&quot;)), raster(paste0(IN.DIR, &quot;/TGO_2015_F30r.tif&quot;)), raster(paste0(IN.DIR, &quot;/TGO_2018_F30r.tif&quot;))) # Créer carte des changements (p.ex. FFFN pour déforestation entre 2015 et 2018) change.map &lt;- maps[[1]] + maps[[2]]*10 + maps[[3]]*100 + maps[[4]]*1000 # Charger la grille d&#39;échantillonnage du SSTS frame.points &lt;- readOGR(paste0(DIR.SST.BDD, &quot;/01_reseau-SSTS/TGO_frame_480m.shp&quot;)) # Extraire la transition pour chaque parcelle frame.points$trans &lt;- raster::extract(change.map, frame.points) # Allocation des points de validation ========================================= n &lt;- 4000 # la taille de l&#39;échantillon alloc &lt;- freq(change.map)[1:16,] # fréquence des transitions (p. ex FFFN) # Allocation proportionelle et égale alloc &lt;- cbind(alloc, prop=round(n*alloc[,&quot;count&quot;]/sum(alloc[,&quot;count&quot;])), equal=round(n/nrow(alloc))) # Allocation balancée (moyenne entre proportionelle et égale) alloc &lt;- cbind(alloc, balanced=round((alloc[,&quot;prop&quot;] + alloc[,&quot;equal&quot;])/ 2)) # Echantillonnage des parcelles de validation ================================ # Ajoute les attributs des parcelles d&#39;entraînement train.plots &lt;- readOGR(paste0(DIR.SST.BDD, &quot;/02_train-plots/assessed/COV_parcelles.shp&quot;)) frame.points &lt;- merge(frame.points, train.plots@data[,c(&quot;PLOTID&quot;, &quot;img_date&quot;, &quot;img_src&quot;, &quot;mod_date&quot;, &quot;author&quot;, &quot;ccov&quot;)], by=&quot;PLOTID&quot;, all.x=TRUE) # Créer une couche de points vide val.points &lt;- frame.points[0,] # Initialiser le générateur de nombres aléatoires set.seed(RSEED) # Pour chaque transition ... for(i in 1:nrow(alloc)) { strat.n &lt;- alloc[i, &quot;balanced&quot;] # nombre d&#39;échantillons à tirer sample.ids &lt;- NULL # vecteur pour les ID à échantillonner # Tout d&#39;abord, prend les parcelles avec ... ids.tp &lt;- which(!is.na(frame.points$trans) &amp; frame.points$trans==alloc[i, &quot;value&quot;] &amp; !is.na(frame.points$ccov)) # couverture houppier connue n.tp &lt;- min(length(ids.tp), strat.n) if (n.tp &gt; 0) sample.ids &lt;- c(sample.ids, sample(ids.tp, n.tp)) # ... et completer avec autres échantillons de la grille avec ids.r &lt;- which(!is.na(frame.points$trans) &amp; frame.points$trans==alloc[i, &quot;value&quot;] &amp; is.na(frame.points$ccov)) # couverture houppier inconnue n.r &lt;- min(length(ids.r), strat.n - n.tp) if (n.r &gt; 0) sample.ids &lt;- c(sample.ids, sample(ids.r, n.r)) # aléatoirement # Ajouter aux points de validation val.points &lt;- rbind(val.points, frame.points[sample.ids, ]) } # Mélanger les points de validation et ajouter un identifiant val.points &lt;- val.points[sample(1:nrow(val.points)), ] val.points$SAMPLEID &lt;- paste0(&quot;val-&quot;, str_pad(string=1:nrow(val.points), width = 4, pad = &quot;0&quot;, side = &quot;left&quot;)) # Conversion des points en parcelles et ajouter des attributs ================= # Grille Landsat landsat.grid &lt;- raster(change.map) values(landsat.grid) &lt;- 1 # Selectionner les pixels Landsat correspondantes et convertir en polygone val.plots &lt;- rasterToPolygons(mask(landsat.grid, val.points)) # Extraire les attributs des points d&#39;entraînement ... val.plots@data &lt;- over(val.plots, val.points[, c(&quot;PLOTID&quot;, &quot;SAMPLEID&quot;, &quot;xcoords&quot;, &quot;ycoords&quot;, &quot;trans&quot;, &quot;ccov&quot;, &quot;img_src&quot;, &quot;img_date&quot;, &quot;author&quot;, &quot;mod_date&quot;)]) # ... convertir couverture des houppiers disponibles en %, les autres NA val.plots$ccov &lt;- format(round(100*val.plots$ccov, 1)) val.plots$ccov[val.plots$ccov == &quot; NA&quot;]&lt;- NA # ... et compléter avec attributs occupation des terre 1987, 2003, 2015 et 2018 val.plots$lc_18 &lt;- val.plots$lc_15 &lt;- as.character(NA) val.plots$lc_03 &lt;- val.plots$lc_87 &lt;- as.character(NA) # Sauvegarder parcelles sous format Shapefile et KML writeOGR(val.plots, dsn=&quot;.&quot;, layer=&quot;UOT_parcelles&quot;, driver=&quot;ESRI Shapefile&quot;, overwrite=TRUE) writeKML(val.plots, kmlname=&quot;UOT_parcelles&quot;, filename=&quot;UOT_parcelles.kml&quot;) # Créer une grille d&#39;échantillon 7x7 dans chaque parcelle ===================== # Détérminer la grille grid.size &lt;- 7 res &lt;- res(landsat.grid)[1] offset &lt;- c(res/grid.size/2 + (0:(grid.size-1))*res/grid.size) # Diviser les parcelles pour un traitement parallèle subsets &lt;- split(val.plots, f=1:86) registerDoParallel(CORES-1) val.grids &lt;- foreach(subset=subsets, .combine=bind, .multicombine=TRUE) %dopar% { # Créer un couche de points vides ... grids &lt;- SpatialPointsDataFrame(data.frame(x = 0, y = 0), data=data.frame(PLOTID = 0, SAMPLEID = 0, GRIDPOINT = 0))[-1,] # ... et ajoute la grille d&#39;échantillon pour chaque parcelle for(p in 1:length(subset)) { plot &lt;- subset[p,] ext &lt;- extent(plot) grids &lt;- bind(grids, SpatialPointsDataFrame(expand.grid(ext@xmin+offset, ext@ymin+offset), data=data.frame(PLOTID = plot$PLOTID, SAMPLEID = plot$SAMPLEID, GRIDPOINT = 1:grid.size^2))) } grids } # Appliquer le système de référence des coordonnées proj4string(val.grids) &lt;- proj4string(val.plots) # fusionner avec les attributs (arbre, oui ou non?) déjà collectés train.grids &lt;- readOGR(paste0(DIR.SST.BDD, &quot;/02_train-plots/assessed/COV_parcelles_grid.shp&quot;)) val.grids &lt;- merge(val.grids, train.grids@data[, c(&quot;PLOTID&quot;, &quot;GRIDPOINT&quot;, &quot;tree&quot;)], all.x=TRUE) # Sauveguarder les grille d&#39;échantillon sous format Shapefile writeOGR(val.grids, dsn=OUT.DIR, layer=&quot;UOT_parcelles_grid&quot;, driver=&quot;ESRI Shapefile&quot;, overwrite=TRUE) # Diviser parcelles et grilles d&#39;échantillon en 10 sous-ensembles ============= # pour le traitement par différents photo-interprètes subsets &lt;- split(val.plots, f=1:10) for(i in 1:length(subsets)) { # Sauvegarder parcelles sous format Shapefile et KML writeOGR(subsets[[i]], dsn=OUT.DIR, layer=paste0(&quot;UOT_parcelles_&quot;, i), driver=&quot;ESRI Shapefile&quot;, overwrite=TRUE) writeKML(subsets[[i]], kmlname=paste0(&quot;UOT_parcelles_&quot;, i) , filename=paste0(&quot;UOT_parcelles_&quot;, i, &quot;.kml&quot;)) # Sauvegarder les grilles correspondantes sous format Shapefile subset.grids &lt;- val.grids[val.grids$PLOTID %in% subsets[[i]]$PLOTID,] writeOGR(subset.grids, dsn=OUT.DIR, layer=paste0(&quot;UOT_parcelles_&quot;, i, &quot;_grid&quot;), driver=&quot;ESRI Shapefile&quot;, overwrite=TRUE) } "],
["04_outil-QGIS.html", "", " 2.2.4 Outil QGIS Actuellement, les parcelles d’échantillonnage du SSTS ainsi que la grille de points pour déterminer la couverture des houppiers sont stockées sous forme des fichiers Shapefile. L’acquisition des attributs est effectuée par les photo-interprètes à l’aide d’un formulaire défini dans un projet QGIS. Un SSTS basé sur des fichiers limite la complexité de la structure des données (par exemple, l’enregistrement récurrent des mêmes parcelles sur des images de différentes années) ainsi que le travail parallèle de différents photo-interprètes sur les mêmes données. Actuellement, des travaux sont en cours pour transférer le SSTS vers une base de données géographiques (PostGIS) sur un serveur central au Ministère de l’Environnement et des Ressources Forestières (MERF) et pour définir un formulaire QGIS qui permet à plusieurs personnes de travailler simultanément sur les données. "],
["00_IFN.html", "3 Inventaire Forestier National", " 3 Inventaire Forestier National L’inventaire forestier national, combine les données collectées directement sur le terrain avec des références forestières provenant de différentes sources. Le seul ensemble de données collectées est actuellement le premier IFN-1 national inverntaire forestier qui a été réalisé par le GIZ en 2015/16. Il est utilisé dans le cadre du NRF-MRV pour déterminer les facteurs d’émission de la biomasse des arbres. Un deuxième inverntaire forestier national (IFN-2) sur base des parcelles permanentes installées par IFN-1 est en cours de planification. Ce deuxième inventaire sera realise par la direction de l’environnment du Mininstère de l’environnment, du developpement durable et de la protection de la nature. En outre, d’autres ensembles de données seront intégrés dans la base de données au cours des prochaines années: inventaires réalisés par les universités du Togo et autres acteurs inventaire des plantations de l’ODEF données sur les feux de végétation de l’ANGE données sur l’agriculture (superficie emblavées et le cheptel) de DCID et ITRA données démographiques de INSEED … Les données et les documents des enquêtes respectives sont structurés comme suit: 02_IFN # INVENTAIRE FORESTIER NATIONAL ============= ├── 01_IFN-1 # Premier IFN 2015/16 --------------------- ├── 01_data # données brutes ├── 02_aux # données auxiliaires, scripts, analyses └── 03_reports # rapports (méthode, résultats, ...) └── ... # Autres données (IFN-2, Plantations, Feu) "],
["01_IFN-1.html", "3.1 IFN-1 (2015/16)", " 3.1 IFN-1 (2015/16) Le premier inventaire forestier national a ete realisé par la GIZ et le MERF en 2015/16. 3.1.1 Méthode L’inventaire IFN-1 a installé xx parcelles permanentes au Togo. Les parcelles ont été installée … 3.1.2 Données 3.1.3 Résultats Les résultats … "],
["00_NRF-MRV.html", "4 Analyses NRF/MRV", " 4 Analyses NRF/MRV Le Togo a soumis son premier Niveau de Référence pour les Forêts (NRF 1.0) en Janvier 2020 (Soumissions du Togo sur la plateforme web REDD+ de la CCNUCC). Il est basé sur l’évolution de la couverture forestier entre 2003 – 2018 (pertes et gains des terres forestières à une couverture du houppier ≥ 30%) et le stockage de carbone dans la biomasse aérienne des arbres (incl. bois mort sur pied) Les données de base qu’on a utilisé pour effectuer ce travail sont: les images Landsat de l’archive USGS (1985 – 2019) et les données climatiques WorldClim version 2 pour la la cartographie de l’évolution des surfaces forestiers et la cartographie de la biomasse les données du SSTS (état 2019) sur la couverture du houppier et l’utilisation des terres pour la calibration et la validation des cartes sur l’évolution des surfaces forestier les données dendrométriques du IFN-1 (2015/16) pour déterminer le stockage de carbone dans la biomasse aérienne par parcelle et la calibration des cartes de la biomasse Ce chapitre décrit l’approche méthodologique et les outils techniques utilisés pour établir ce NRF. C’est un travail en cours. Le même approche sera utilisés a) pour améliorer le NRF avec des nouvelles données et/ou mèthodes et b) pour mettre à jour régulièrement les analyses dans le cadre du Monitoring, reporting et vérification (MRV). Chaque section commence par une description de la méthodologie utilisée et des références aux points clés dans le script R correspondant, suivie par une présentation exemplaire des résultats de cette étappe et enfin du code R commenté. Section 2.1 décrit l’acquisition et la préparation des données de télédétection dans le cadre du Système de Surveillance Terrestre par Satellite SSTS (Images Landsat, données WorldClim, …). Section 2.2 décrit la collecte de données d’entraînement et de validation dans le cadre du SSTS. Section 4.1 décrit les différentes étapes nécessaires pour la cartographie des surfaces forestières et leur évolution: la classification des séries de cartes, leur néttoyage et validation. Section 4.2 décrit les différentes étapes nécessaires pour la cartographie des la biomasse aérienne: l’évaluation des données de l’IFN, la calibration des cartes de biomasse leur néttoyage et l’analyse de l’évolution de la biomasse dans le temps. "],
["00_analyse-FCC.html", "4.1 Analyse surfaces forestiers", " 4.1 Analyse surfaces forestiers L’analyse des surfaces forestiers est fait par une classification supervisée. Les données du SSTS sur la couverture des houppiers (Section 2.2.2.2) est utilisé pour calibrer un modèle de classification Random Forest sur base des images satellitaires Landsat et les données climatiques Worldclim v2. Tous les parcelles d’entraînement avec une couverture des houppiers ≥ 30% sont considérées comme forêt, les autres comme non-forêts. Le schéma suivant montre les étapes pour la production des cartes forêt/non-forêt sur toute la période 1986 – 2019: Vue que les données sur la couverture des houppier et seulement disponible pour les années récentes (à cause d’une disponibilité limité des images de très haute résolution), c’est seulement la carte forêt/non-forêt 2018 qui a été produit sur base des parcelles d’entraînement du SSTS. Cette carte de référence 2018 est utilisé comme base pour la calibration des modèles de classification pour les autres dates pour lesquelles des images Landsat sont disponible. Tout d’abord la carte de référence 2018 est utilisé pour calibrer la classification d’une carte forêt/non-forêt en 2003. C’est cette carte de référence 2003 qu’on a utilisé pour calibrer les autres cartes de 1986 – 2018. On a choisi de faire la classification de toute la serie des images sur la carte de référence 2003, parce que les images Landsat 2003 sont de très bonne qualité (et donc la carte est probablement aussi de bonne qualité) et parce que l’année 2003 se trouve au milieu de la période analysée. Si on prend directement la carte forêt/non-forêt 2018 comme référence pour la calibration de la série 1986 – 2018, on observe une généralisation de la surface forestier le plus on s’éloigne de la date 2018, donc un changement de la surface forestier qui est plutôt un artefact de la méthode que une changement d’occupation des terres. Dans la suite, la série des cartes forêts/non-forêts 1986 – 2018 est nettoyé par une lyssage temporelle, pixel par pixel, avec un filtre majoritaire (fenêtre coulissante d’une taille de 5). Finalement, une reforestation est seulement constaté si on a observé la forêt pour une période de 10 ans et plus. Les cartes nettoyés de 2003, 2015 et 2018 sont validés avec les données SSTS sur l’occupation des terres (Section 2.2.3.2) et les matrices d’erreurs sont utilisées pour détérminer la précision des cartes individuelles et des différents changements d’occupation des terres, en utilisant la méthode de Olofsson et al. (2014) "],
["01_create-FC-maps.html", "", " 4.1.1 Production des cartes Forêt/Non-Forêt La production de la carte forêt/non-forêt 2018 est basée sur la couverture des houppiers déterminée dans les parcelles d’entraînement, les images satellitaires Landsat (bandes et indices B, G, R, NIR, SWIR1, SWIR2, nbr, ndmi, ndvi, evi) et les données climatiques Worldclim (BIO1, BIO4, BIO12, BIO15). Dans une première étape, les variables Landsat et Worldclim sont extraites pour toutes les parcelles d’entraînement de la période de référence 1.1.2017 - 31.12.2018 et les parcelles d’entraînement sont classées comme “forestières” ou “non forestières” en fonction de leur couverture des houppiers (forêt ≥ 30% de couverture des houppiers). Par la suite, des cartes forêt/non-forêt 2018 sont produites pour chaque chemin WRS. Pour cela, la fonction classify.image() est utilisée, qui crée un modèle de classification en utilisant l’algorithme RandomForest et une carte correspondante. Pour le chemin WRS central p193, l’algorithme de classification est calibré uniquement sur la base des parcelles d’entraînement. Pour les chemins p192 et p194, des données d’entraînement supplémentaires basées sur la carte p193 sont utilisées pour assurer la calibration entre les chemins. Les cartes de référence générées pour 2018 sont maintenant utilisées pour générer les cartes correspondantes pour l’année 2003. Les données d’entraînement sont tirées de la carte de référence 2018, autour de la lisière des forêts. Pour les chemins p192 et p194, de nouves des données d’entraînement supplémentaires du chemin p193 sont utilisées. Sur la base des cartes forêt/non-forêt de 2003, les cartes de toutes les années avec des images Landsat disponibles sont produites selon la même procédure. Le calibrage de toute la série sur la base d’une année de référence garantit une différenciation consistante entre les classes forêt et non-forêt. Enfin, les chemins p192, p193 et p194 sont fusionnés pour les années clés 1987, 2003, 2015 et 2018. Example La figure suivante illustre la série des 13 cartes forêt/non-forêt brutes dans une région au sud de Kpalimé. Les pixels en rose vif sont des pixels dont les données sont manquantes (nuages, ombres, L7 SLC-off). Voir série des cartes nettoyées pour comparaison. Script R: 03_NRF-MRV/01_MCF/_src/01_create-FC-maps.R ############################################################################### # 01_create-fc-maps.R: créer des cartes brutes du couvert forestier # ----------------------------------------------------------------------------- # Bern University of Applied Sciences # Oliver Gardi, &lt;oliver.gardi@bfh.ch&gt; # 13 Mai 2020 # Définitions des variables =================================================== COV.FC &lt;- 10 # Couverture des houppiers forêt/non-forêt SAMPLE.DIST &lt;- 1 # Largeur de la lisière forêt à considérer N.PIXELS &lt;- NA # Number of non-NA cells (will be determined later) SAMPLE.RATIO &lt;- 0.0025 # Share of non-NA cells to sample CAL.RATIO &lt;- 0.75 # Use same amount of ref-points from cal.map as from ref.map / train.points PREDICTORS &lt;- c(&quot;B&quot;, &quot;G&quot;, &quot;R&quot;, &quot;NIR&quot;, &quot;SWIR1&quot;, &quot;SWIR2&quot;, &quot;nbr&quot;, &quot;ndmi&quot;, &quot;ndvi&quot;, &quot;evi&quot;, &quot;BIO1&quot;, &quot;BIO4&quot;, &quot;BIO12&quot;, &quot;BIO15&quot;) # &quot;savi&quot;, &quot;nbr2&quot;, &quot;msavi&quot;, &quot;x&quot;, &quot;y&quot; # Définitions des fonctions =================================================== # Charger un image Landsat ---------------------------------------------------- # # @param filename Chemin du fichier landsat # # @return Image Landsat avec bandes nommées # load.image &lt;- function(filename) { image &lt;- brick(paste0(IMAGES.DIR, filename)) names(image) &lt;- SST.LSBANDS return(image) } # Tirer des points d&#39;entraînement d&#39;une carte autour de la lisière forêt ------ # # @param map Carte forêt/non-forêt à échantillonner # @param n Nombre d&#39;échantillons à tirer # # @return Points d&#39;échantillon avec aatribut forêt/non-forêt # sample.map &lt;- function(map, n) { tmp.src &lt;- tempfile(pattern = &quot;&quot;, fileext = &quot;.tif&quot;) # temporary file for masked reference map tmp.dst1 &lt;- tempfile(pattern = &quot;&quot;, fileext = &quot;.tif&quot;) # temporary file for forest edge tmp.dst3 &lt;- tempfile(pattern = &quot;&quot;, fileext = &quot;.tif&quot;) # temporary file for map &lt;- writeRaster(map, tmp.src) # write it to the disk # draw sample points around forest edge system(paste(&quot;gdal_proximity.py&quot;, tmp.src, tmp.dst1, # forest and 3 pixel non-forest edge (distance to nearest pixel of value 1) &quot;-values 1 -use_input_nodata YES -maxdist &quot;, SAMPLE.DIST, &quot; -fixed-buf-val 3&quot;)) dst1 &lt;- raster(tmp.dst1) NAvalue(dst1) &lt;- 65535 cat(&quot; &quot;) system(paste(&quot;gdal_proximity.py&quot;, tmp.src, tmp.dst3, # non-forest and 3 pixel forest edge (distance to nearest pixel of value 3) &quot;-values 3 -use_input_nodata YES -maxdist &quot;, SAMPLE.DIST, &quot; -fixed-buf-val 1&quot;)) dst3 &lt;- raster(tmp.dst3) NAvalue(dst3) &lt;- 65535 map &lt;- mask(map, dst1) map &lt;- mask(map, dst3) unlink(c(tmp.src, tmp.dst1, tmp.dst3)) # delete temporary files n.classes &lt;- length(unique(map)) # number of classes (should be two) cat(paste0(&quot; -Sampling map (n=&quot;, n.classes, &quot;*&quot;, round(n/n.classes), &quot;) ... &quot;)) sample.pts &lt;- sampleStratified(map, round(n/n.classes), sp=TRUE)[,-1] # stratified sampling (same number of samples for each class) names(sample.pts) &lt;- &quot;CLASS&quot; cat(&quot;done\\n&quot;) return(sample.pts) } # Function for classifying an image ---------------------------------------- classify.image &lt;- function(image, filename, bioclim=NULL, train.pts=NULL, ref.map=NULL, n.ref.map=NULL, cal.map=NULL, n.cal.map=NULL, mask=NULL, preds=NULL, type=&quot;classification&quot;, crossval=FALSE, prob=FALSE, n.cores=8) { txtfile &lt;- paste0(sub(&quot;[.]tif$&quot;, &quot;&quot;, filename), &quot;.txt&quot;) cat(&quot;-- Image classification: &quot;, basename(filename), &quot;/&quot;, date(), &quot; --\\n&quot;, file=txtfile) if(!is.null(train.pts)) { cat(&quot; -Loading training points ... &quot;) train.pts &lt;- train.pts[,1] # only use first column in the attribute table (class) names(train.pts) &lt;- &quot;CLASS&quot; set_ReplCRS_warn(FALSE) proj4string(train.pts) &lt;- proj4string(image) cat(&quot;done\\n&quot;) cat(&quot;Training points:&quot;, nrow(train.pts), &quot;\\n&quot;, file=txtfile, append=TRUE) } # add training data from ref.map if provided if(!is.null(ref.map)) { cat(paste0(&quot; -Masking / buffering reference map ... \\n&quot;)) ref.map &lt;- mask(crop(ref.map, image[[1]]), crop(image[[1]], ref.map)) # crop/mask ref.map with image if(!is.null(mask)) ref.map &lt;- mask(ref.map, mask) # mask with additional mask, if provided if(!is.null(cal.map)) { tmp &lt;- extend(crop(cal.map, ref.map), ref.map) # cut out the piece of the calibration map that overlaps ref map and extend to refmap ref.map &lt;- mask(ref.map, tmp, inverse=TRUE) } cat(&quot; &quot;) ref.pts &lt;- sample.map(ref.map, n.ref.map) cat(&quot;Ref-map points: &quot;, nrow(ref.pts), &quot;/&quot;, ref.map@file@name, &quot;/&quot;, SAMPLE.DIST, &quot;px\\n&quot;, file=txtfile, append=TRUE) if(is.null(train.pts)) { train.pts &lt;- ref.pts # use it as training points or add to existing training points } else { train.pts &lt;- rbind(train.pts, ref.pts) } } if(!is.null(cal.map)) { cat(paste0(&quot; -Masking / buffering calibration map ... \\n&quot;)) cal.map &lt;- mask(crop(cal.map, image[[1]]), crop(image[[1]], cal.map)) # crop/mask ref.map with image if(!is.null(mask)) cal.map &lt;- mask(cal.map, mask) # mask with additional mask, if provided cat(&quot; &quot;) cal.pts &lt;- sample.map(cal.map, n.cal.map) cat(&quot;Cal-map points: &quot;, nrow(cal.pts), &quot;from&quot;, cal.map@file@name, &quot;/&quot;, SAMPLE.DIST, &quot;px\\n&quot;, file=txtfile, append=TRUE) if(is.null(train.pts)) { train.pts &lt;- cal.pts # use it as training points or add to existing training points } else { train.pts &lt;- rbind(train.pts, cal.pts) } } cat(&quot;Total points: &quot;, nrow(train.pts), &quot;\\n&quot;, file=txtfile, append=TRUE) # extract spectral values if(is.null(preds)) { preds &lt;- names(image) if(!is.null(bioclim)) preds &lt;- c(preds, names(bioclim)) } cat(&quot; -Extracting pixel values for bands:&quot;, preds, &quot;... &quot;) train.pts &lt;- raster::extract(image, train.pts, sp=TRUE) if(!is.null(bioclim)) train.pts &lt;- raster::extract(bioclim, train.pts, sp=TRUE) train.dat &lt;- na.omit(train.pts@data)[, c(&quot;CLASS&quot;, preds)] if(type==&quot;classification&quot;) train.dat[,1] &lt;- as.factor(train.dat[,1]) cat(&quot;done\\n&quot;) # calibrate RandomForest classifier cat(&quot; -Calibrating RandomForest ... &quot;) sink(txtfile, append=TRUE) if(crossval) { map.model.cv &lt;- train(y = train.dat[,1], x = train.dat[,-1], method = &quot;rf&quot;, importance = TRUE, trControl = trainControl( method = &quot;repeatedcv&quot;, number = 10, repeats = 3)) print(map.model.cv) map.model &lt;- map.model.cv$finalModel print(map.model) cat(&quot;\\n&quot;) print(varImp(map.model, scale=FALSE)) } else { map.model &lt;- randomForest(y=train.dat[,1], x=train.dat[,-1], importance=TRUE) # , do.trace=100) # Parallelization of RandomForest: confusion, err.rate, mse and rsq will be NULL # https://stackoverflow.com/questions/14106010/parallel-execution-of-random-forest-in-r # map.model &lt;- foreach(ntree=rep(100, 5), .combine=randomForest::combine, .multicombine=TRUE, .packages=&#39;randomForest&#39;) %dopar% { # randomForest(x=ref.pts[,!(names(ref.pts) == &quot;CLASS&quot;)], y=ref.pts$CLASS, importance=TRUE, ntree=ntree) # print(map.model) cat(&quot;\\n&quot;) print(varImp(map.model)) } sink() if(type==&quot;treecover&quot;) { cat(&quot;R2:&quot;, round(map.model$rsq[500], 2), &quot;RMSE:&quot;, round(sqrt(map.model$mse[500]), 2), &quot;\\n&quot;) } else { cat(&quot;OOB error rate:&quot;, round(map.model$err.rate[500,1], 2), &quot;\\n&quot;) } # write model results dir.create(dirname(filename), recursive=TRUE, showWarnings=FALSE) # save RandomForest Model (too large) # save(map.model, file=paste0(sub(&quot;[.]tif$&quot;, &quot;&quot;, filename), &quot;r_rf.RData&quot;)) # classify image cat(&quot; -Creating map ... &quot;) if(!is.null(bioclim)) image &lt;- stack(image, crop(bioclim, image)) beginCluster(n=n.cores) map &lt;- clusterR(image, predict, args=list(model=map.model)) endCluster() # save map of classified image cat(&quot;writing map ... &quot;) if(type==&quot;treecover&quot;) map &lt;- floor(map*100) map &lt;- writeRaster(map, filename=filename, format=&quot;GTiff&quot;, datatype=&quot;INT2U&quot;, overwrite=TRUE) cat(&quot;done\\n&quot;) # create probability map if(prob==TRUE) { cat(&quot; -Creating probability map ... &quot;) beginCluster(n=n.cores) prob.map &lt;- clusterR(image, predict, args=list(model=map.model, type=&quot;prob&quot;)) endCluster() cat(&quot;writing map ... &quot;) writeRaster(prob.map, filename=sub(&quot;\\\\.tif&quot;, &quot;_prob.tif&quot;, filename), format=&quot;GTiff&quot;, overwrite=TRUE) cat(&quot;done\\n&quot;) } else { prob.map &lt;- NULL } cat(&quot;-- Done: &quot;, basename(filename), &quot;/&quot;, date(), &quot; --\\n&quot;, file=txtfile, append=TRUE) invisible(list( &quot;model&quot; = map.model, &quot;map&quot; = map, &quot;prob&quot; = prob.map )) } ### DO THE WORK ########################################################### # create directories if the don&#39;t exist dir.create(FCC.REF.DIR, recursive=TRUE, showWarnings=FALSE) dir.create(FCC.RAW.DIR, recursive=TRUE, showWarnings=FALSE) # load 2018 images -------------------------------------------------------- ref.p192 &lt;- brick(paste0(IMAGES.DIR, &quot;/p192/p192_2018_m.tif&quot;)) ref.p193 &lt;- brick(paste0(IMAGES.DIR, &quot;/p193/p193_2018_m.tif&quot;)) ref.p194 &lt;- brick(paste0(IMAGES.DIR, &quot;/p194/p194_2018_m.tif&quot;)) names(ref.p192) &lt;- names(ref.p193) &lt;- names(ref.p194) &lt;- BANDS ref.images &lt;- list(p192=ref.p192, p193=ref.p193, p194=ref.p194) N.PIXELS &lt;- list(p192 = ncell(ref.p192[[&quot;B&quot;]]) - summary(ref.p192)[&quot;NA&#39;s&quot;,&quot;B&quot;], p193 = ncell(ref.p193[[&quot;B&quot;]]) - summary(ref.p193)[&quot;NA&#39;s&quot;,&quot;B&quot;], p194 = ncell(ref.p194[[&quot;B&quot;]]) - summary(ref.p194)[&quot;NA&#39;s&quot;,&quot;B&quot;]) bioclim.p192 &lt;- brick(paste0(IMAGES.DIR, &quot;/p192/p192_bioclim.tif&quot;)) bioclim.p193 &lt;- brick(paste0(IMAGES.DIR, &quot;/p193/p193_bioclim.tif&quot;)) bioclim.p194 &lt;- brick(paste0(IMAGES.DIR, &quot;/p194/p194_bioclim.tif&quot;)) names(bioclim.p192) &lt;- names(bioclim.p193) &lt;- names(bioclim.p194) &lt;- BIOCLIM bioclim &lt;- list(p192=bioclim.p192, p193=bioclim.p193, p194=bioclim.p194) # load training plots ------------------------------------------------------ train.plots &lt;- readOGR(paste0(TRNPTS.DIR, &quot;/COV_parcelles.shp&quot;)) train.plots &lt;- train.plots[!is.na(train.plots$ccov), c(&quot;PLOTID&quot;, &quot;ccov&quot;, &quot;img_date&quot;, &quot;author&quot;)] train.plots$author &lt;- as.factor(sub(&quot;^.*\\\\/\\\\/&quot;, &quot;&quot;, train.plots$author)) # convert plot polygons to spatial points (centroids) train.points &lt;- SpatialPointsDataFrame(gCentroid(train.plots, byid=TRUE), data.frame(author=train.plots$author, ccov=train.plots$ccov, img_date=as.Date(train.plots$img_date))) # select only those with image date between 1.1.2017 and 31.12.2019 train.points &lt;- train.points[!is.na(train.points$img_date) &amp; train.points$img_date &gt; as.Date(&quot;2017-01-01&quot;) &amp; train.points$img_date &lt;= as.Date(&quot;2019-12-31&quot;), ] pdf(paste0(FCC.REF.DIR, &quot;/training-pts_2018.pdf&quot;)) plot(train.points) dev.off() # extract image values for train.points registerDoParallel(.env$numCores-1) train.points &lt;- foreach(i=1:length(ref.images), .combine=rbind) %dopar% { pts &lt;- raster::extract(ref.images[[i]], train.points, sp=TRUE) pts &lt;- raster::extract(bioclim[[i]], pts, sp=TRUE) pts$image &lt;- names(ref.images[i]) pts[, c(&quot;author&quot;, &quot;image&quot;, &quot;ccov&quot;, BANDS, BIOCLIM)] } # remove rows with NA&#39;s train.points &lt;- train.points[!is.na(rowSums(train.points@data[,-(1:2)])), ] # discard points from authors that add confusion # train.points &lt;- train.points[!train.points$author %in% c(&quot;6_Eric_AGBESSI&quot;, &quot;2_Mamalnassoh_ABIGUIME&quot;, &quot;7_Aklasson_TOLEBA&quot;, &quot;1_Ditorgue_BAKABIMA&quot;, &quot;8_Yawo_KONKO&quot;), ] # for(author in unique(train.points$author[!train.points$author %in% discard])) { # print(author) # map.model &lt;- randomForest(y=train.points@data[!train.points$author %in% c(author, discard),&quot;ccov&quot;], x=train.points@data[!train.points$author %in% c(author, discard),-(1:3)]) # print(map.model) # } train.points@data &lt;- cbind(train.points@data[,c(&quot;image&quot;, &quot;ccov&quot;)], F10=cut(train.points$ccov, breaks=c(0.0,0.1,1.0), labels=c(3, 1), right=FALSE, include.lowest=TRUE), F30=cut(train.points$ccov, breaks=c(0.0,0.3,1.0), labels=c(3, 1), right=FALSE, include.lowest=TRUE), train.points@data[,c(BANDS, BIOCLIM)]) # Parameter selection ----------------------------------------------------- cov.varsel &lt;- rfe(y=train.points@data[train.points$image==&quot;p193&quot;, &quot;ccov&quot;], x=train.points@data[train.points$image==&quot;p193&quot;, PREDICTORS], sizes = c(4, 6, 8, 10), rfeControl=rfeControl( functions=rfFuncs, # use RandomForest method = &quot;repeatedcv&quot;, # repeated cross-validation number = 10, # 10-fold repeats = 3)) # 3 repeats print(cov.varsel) predictors(cov.varsel) plot(cov.varsel, type=c(&quot;g&quot;, &quot;o&quot;)) # 2018 reference tree cover map (TODO) ------------------------------------- # Tree cover map for 2018 for p193 set.seed(RSEED) p193.2018.cov &lt;- classify.image(image = load.image(&quot;/p193/p193_2018_m.tif&quot;), bioclim = bioclim[[&quot;p193&quot;]], filename = paste0(FCC.REF.DIR, &quot;/p193_2018_COV_R.tif&quot;), train.pts = train.points[train.points$image == &quot;p193&quot;, &quot;ccov&quot;], preds = PREDICTORS, type = &quot;treecover&quot;, crossval = TRUE, n.cores = 32) # plot observed vs. predicted pdf(paste0(FCC.REF.DIR, &quot;/p193_2018_COV_R.pdf&quot;)) plot(p193.2018.cov[[&quot;model&quot;]]$y, p193.2018.cov[[&quot;model&quot;]]$predicted, xlim=c(0,1), ylim=c(0,1), main=&quot;Couverture houppier p193&quot;, xlab=&quot;Observation&quot;, ylab=&quot;Prédiction&quot;) abline(0,1) dev.off() # 2018 reference forest cover maps ----------------------------------------- # Forest cover map for 2018 for p193 set.seed(RSEED) classify.image(image = load.image(&quot;/p193/p193_2018_m.tif&quot;), bioclim = bioclim[[&quot;p193&quot;]], filename = paste0(FCC.REF.DIR, &quot;/FC&quot;, COV.FC, &quot;/p193_2018_FC&quot;, COV.FC, &quot;_R.tif&quot;), train.pts = train.points[train.points$image == &quot;p193&quot;, paste0(&quot;F&quot;, COV.FC)], preds = PREDICTORS, prob = TRUE, crossval = TRUE, n.cores = 32) # Forest cover maps 2018 for p192 and p194, calibrating with p193 set.seed(RSEED) registerDoParallel(.env$numCores-1) foreach(path=c(&quot;p192&quot;, &quot;p194&quot;)) %dopar% { train.pts &lt;- train.points[train.points$image == path, paste0(&quot;F&quot;, COV.FC)] classify.image(image = load.image(paste0(&quot;/&quot;, path, &quot;/&quot;, path, &quot;_2018_m.tif&quot;)), bioclim = bioclim[[path]], filename = paste0(FCC.REF.DIR, &quot;/FC&quot;, COV.FC, &quot;/&quot;, path, &quot;_2018_FC&quot;, COV.FC, &quot;_R.tif&quot;), train.pts = train.pts, cal.map = raster(paste0(FCC.REF.DIR, &quot;/FC&quot;, COV.FC, &quot;/p193_2018_FC&quot;, COV.FC, &quot;_R.tif&quot;)), n.cal.map = max(2000, nrow(train.pts)/CAL.RATIO), preds = PREDICTORS, mask = TGO, prob = TRUE, n.cores = 32) } # 2003 reference forest cover maps ----------------------------------------- # Forest cover map for 2003 for p193 set.seed(RSEED) classify.image(image = load.image(&quot;/p193/p193_2003_m.tif&quot;), bioclim = bioclim[[&quot;p193&quot;]], filename = paste0(FCC.REF.DIR, &quot;/FC&quot;, COV.FC, &quot;/p193_2003_FC&quot;, COV.FC, &quot;_R.tif&quot;), ref.map = raster(paste0(FCC.REF.DIR, &quot;/FC&quot;, COV.FC, &quot;/p193_2018_FC&quot;, COV.FC, &quot;_R.tif&quot;)), n.ref.map = 2 * SAMPLE.RATIO * N.PIXELS[[&quot;p193&quot;]], preds = PREDICTORS, mask = TGO, n.cores = 32) # # Recalibrate forest cover map for 2018, based on 2003 for p193 # # set.seed(RSEED) # classify.image(image = load.image(&quot;/p193/p193_2018.tif&quot;), # bioclim = bioclim[[&quot;p193&quot;]], # filename = paste0(REFMAPS.DIR, &quot;/p193_2018_FC&quot;, COV.FC, &quot;.tif&quot;), # ref.map = raster(paste0(REFMAPS.DIR, &quot;/p193_2003_FC&quot;, COV.FC, &quot;_R.tif&quot;)), # n.ref.map = SAMPLE.RATIO * N.PIXELS[[&quot;p193&quot;]], # preds = PREDICTORS, # mask = TGO, # n.cores = 32) # Forest cover maps 2003 for p192 and p194, calibrating with p193 set.seed(RSEED) registerDoParallel(.env$numCores-1) foreach(path=c(&quot;p192&quot;, &quot;p194&quot;)) %dopar% { classify.image(image = load.image(paste0(&quot;/&quot;, path, &quot;/&quot;, path, &quot;_2003_m.tif&quot;)), bioclim = bioclim[[path]], filename = paste0(FCC.REF.DIR, &quot;/FC&quot;, COV.FC, &quot;/&quot;, path, &quot;_2003_FC&quot;, COV.FC, &quot;_R.tif&quot;), ref.map = raster(paste0(FCC.REF.DIR, &quot;/FC&quot;, COV.FC, &quot;/&quot;, path, &quot;_2018_FC&quot;, COV.FC, &quot;_R.tif&quot;)), n.ref.map = 2 * (1 - CAL.RATIO) * SAMPLE.RATIO * N.PIXELS[[path]], cal.map = raster(paste0(FCC.REF.DIR, &quot;/FC&quot;, COV.FC, &quot;/p193_2003_FC&quot;, COV.FC, &quot;_R.tif&quot;)), n.cal.map = 2 * CAL.RATIO * SAMPLE.RATIO * N.PIXELS[[path]], preds = PREDICTORS, mask = TGO, n.cores = 32) } # Forest cover maps for all dates ------------------------------------------ # Forest cover maps for p193 set.seed(RSEED) registerDoParallel(.env$numCores-1) foreach(file=dir(paste0(IMAGES.DIR, &quot;/p193&quot;), pattern=&quot;\\\\_[[:digit:]]+\\\\_m\\\\.tif&quot;)) %dopar% { # foreach(file=c(&quot;p193_2017_m.tif&quot;, &quot;p193_2019_m.tif&quot;)) %dopar% { classify.image(image = load.image(paste0(&quot;/p193/&quot;, file)), bioclim = bioclim[[&quot;p193&quot;]], filename = paste0(FCC.RAW.DIR, &quot;/FC&quot;, COV.FC, &quot;/p193/&quot;, sub(&quot;\\\\_m\\\\.tif$&quot;, paste0(&quot;_F&quot;, COV.FC, &quot;r.tif&quot;), file)), ref.map = raster(paste0(FCC.REF.DIR, &quot;/FC&quot;, COV.FC, &quot;/p193_2003_FC&quot;, COV.FC, &quot;_R.tif&quot;)), n.ref.map = SAMPLE.RATIO * N.PIXELS[[&quot;p193&quot;]], preds = PREDICTORS, mask = TGO, n.cores = 6) # n.cores = 32) } # merge the two p193_1990 tiles merge(raster(paste0(FCC.RAW.DIR, &quot;/FC&quot;, COV.FC, &quot;/p193/p193_1990_1_F&quot;, COV.FC, &quot;r.tif&quot;)), raster(paste0(FCC.RAW.DIR, &quot;/FC&quot;, COV.FC, &quot;/p193/p193_1990_2_F&quot;, COV.FC, &quot;r.tif&quot;)), filename=paste0(FCC.RAW.DIR, &quot;/FC&quot;, COV.FC, &quot;/p193/p193_1990_F&quot;, COV.FC, &quot;r.tif&quot;), format=&quot;GTiff&quot;, datatype=&quot;INT2U&quot;, overwrite=TRUE) # Forest cover maps for p192 and p194 using p193 maps for calibration set.seed(RSEED) registerDoParallel(.env$numCores-1) # foreach(file=c(dir(paste0(IMAGES.DIR, &quot;/p192&quot;), pattern=&quot;\\\\_[[:digit:]]+\\\\_m\\\\.tif&quot;), # dir(paste0(IMAGES.DIR, &quot;/p194&quot;), pattern=&quot;\\\\_[[:digit:]]+\\\\_m\\\\.tif&quot;))) %dopar% { foreach(file=c(&quot;p194_1997_m.tif&quot;, &quot;p194_2007_m.tif&quot;, &quot;p194_2015_m.tif&quot;, &quot;p194_2018_m.tif&quot;)) %dopar% { path &lt;- sub(&quot;\\\\_.*&quot;, &quot;&quot;, file) if(file.exists(paste0(FCC.RAW.DIR, &quot;/FC&quot;, COV.FC, &quot;/p193/&quot;, sub(&quot;\\\\_m\\\\.tif$&quot;, paste0(&quot;_F&quot;, COV.FC, &quot;r.tif&quot;), sub(path, &quot;p193&quot;, file))))) { cal.map &lt;- raster(paste0(FCC.RAW.DIR, &quot;/FC&quot;, COV.FC, &quot;/p193/&quot;, sub(&quot;\\\\_m\\\\.tif$&quot;, paste0(&quot;_F&quot;, COV.FC, &quot;r.tif&quot;), sub(path, &quot;p193&quot;, file)))) n.cal.map &lt;- CAL.RATIO * SAMPLE.RATIO * N.PIXELS[[path]] } else { cal.map &lt;- NULL n.cal.map &lt;- NULL } classify.image(image = load.image(paste0(&quot;/&quot;, path, &quot;/&quot;, file)), bioclim = bioclim[[path]], filename = paste0(FCC.RAW.DIR, &quot;/FC&quot;, COV.FC, &quot;/&quot;, path, &quot;/&quot;, sub(&quot;\\\\_m\\\\.tif$&quot;, paste0(&quot;_F&quot;, COV.FC, &quot;r.tif&quot;), file)), ref.map = raster(paste0(FCC.REF.DIR, &quot;/FC&quot;, COV.FC, &quot;/&quot;, path, &quot;_2003_FC&quot;, COV.FC, &quot;_R.tif&quot;)), n.ref.map = (1 - CAL.RATIO) * SAMPLE.RATIO * N.PIXELS[[path]], cal.map = cal.map, n.cal.map = n.cal.map, preds = PREDICTORS, mask = TGO, n.cores = 16) # n.cores = 6) } # Merging key date maps --------------------------------------------------- for(year in VAL.YEARS) { merge(mask(crop(brick(paste0(FCC.RAW.DIR, &quot;/FC&quot;, COV.FC, &quot;/p193/p193_&quot;, year, &quot;_F&quot;, COV.FC, &quot;r.tif&quot;)),TGO), TGO), mask(crop(brick(paste0(FCC.RAW.DIR, &quot;/FC&quot;, COV.FC, &quot;/p192/p192_&quot;, year, &quot;_F&quot;, COV.FC, &quot;r.tif&quot;)),TGO), TGO), mask(crop(brick(paste0(FCC.RAW.DIR, &quot;/FC&quot;, COV.FC, &quot;/p194/p194_&quot;, year, &quot;_F&quot;, COV.FC, &quot;r.tif&quot;)),TGO), TGO), filename=paste0(FCC.RAW.DIR, &quot;/FC&quot;, COV.FC, &quot;/TGO/TGO_&quot;, year, &quot;_F&quot;, COV.FC, &quot;r.tif&quot;), overwrite=TRUE) } # and reference maps for(map in c(&quot;2018_FC10_R&quot;, &quot;2018_FC10_R_prob&quot;,&quot;2003_FC10_R&quot;)) { # for(map in c(&quot;2018_FC30_R&quot;, &quot;2018_FC30_R_prob&quot;,&quot;2003_FC30_R&quot;)) { merge(mask(crop(brick(paste0(FCC.REF.DIR, &quot;/FC&quot;, COV.FC, &quot;/p193_&quot;, map, &quot;.tif&quot;)),TGO), TGO), mask(crop(brick(paste0(FCC.REF.DIR, &quot;/FC&quot;, COV.FC, &quot;/p192_&quot;, map, &quot;.tif&quot;)),TGO), TGO), mask(crop(brick(paste0(FCC.REF.DIR, &quot;/FC&quot;, COV.FC, &quot;/p194_&quot;, map, &quot;.tif&quot;)),TGO), TGO), filename=paste0(FCC.REF.DIR, &quot;/FC&quot;, COV.FC, &quot;/TGO_&quot;, map, &quot;.tif&quot;), overwrite=TRUE) } "],
["02_clean-FC-maps.html", "", " 4.1.2 Nettoyage des cartes brutes Les series temporelles des cartes forêt/non-forêt brutes sont lissées et filtrées pixel par pixel pour les raisons suivantes: Remplissage des données manquantes (nuages, ombres, L7 SLC-off), Lissage des bruits (pixels qui changent entre forêt et non-forêt plusieurs fois) Filtrer la régénération temporaire par les fourrés, observée sur les jachères. La première étappe, est la remplissage des données manquantes: les données manquantes entre deux observations de forêt deviennent forêt, celles entre deux observations de non-forêt deviennent non-forêt. Ensuite, une fenêtre coulissante de taille 5 est appliquée, c’est-à-dire qu’une observation est attribuée à la classe qui est observé le plus fréquemment dans la fenêtre qui inclu les deux observations précédentes et les deux suivantes. Pour l’évaluation de la deuxième et de l’avant-dernière observation, une fenêtre coulissante de taille 3 est appliquée. La première et la dernière observation ne sont pas ajustées, il faut les ignorer dans la suite. Les observations manquantes sont remplis dans la mesure possible. Toute la procédure est répétée jusqu’à ce qu’il n’y a plus de changements. Les années manquantes sont ajoutées pour créer une série annuelle. Les observations manquantes sont remplacées par la classe de l’observation précédente. Lorsque les observations initiales sont manquantes, elles sont remplacées par la classe de l’observation suivante. Dans les situations de régénération (forêt suit non-forêt), les 9 premières années sont marquées comme “reboisement potentiel”. Si la forêt disparaît à nouveau après moins de 10 ans, les observations sont remplacées par la classe non-forêt. Si la régénération reste, elle est considérée comme un reboisement après 10 ans. Après la nettoyage des séries temporelles pixel par pixel, une nettoyage spatiale est réalisé pour éliminer les surfaces forestières &lt; 0,5 hectares. Pour cela, une carte est créée de tous les pixels qui étaient une fois observées comme forêt sur toute la série des cartes. Sur cette carte, toutes les surfaces forestières ayant moins de 6 pixels liés (soit moins de 0,54 hectares) sont éliminées. Ce masque forestier est ensuite appliqué à toutes les cartes forêt/non-forêt de la série. Cela signifie que seuls les pixels forestier qui ont fait partie d’une surface forestière ≥ 0,54 hectares dans la série des cartes sont retenus comme forêt. Example La figure suivante montre les cartes forêt/non-forêt après la nettoyage temporelle et spatiale pour une région au sud de Kpalimé (voir série des cartes brutes pour comparaison). MCF/02_clean-fc-maps.R ########################################################################## # NERF_Togo/FCC/4_clean-fc-maps_orig.R: clean time-serie of raw forest cover maps # ------------------------------------------------------------------------ # Bern University of Applied Sciences # Oliver Gardi, &lt;oliver.gardi@bfh.ch&gt; # 20 May 2019 # Default parameters ------------------------------------------------------- COV.FC &lt;- 30 # Function for temporal cleaning of path ------------------------------------ clean.temporal &lt;- function(path) { # Preparation ------------------------------------------------------------- maps &lt;- stack(dir(paste0(FCC.RAW.DIR, &quot;/FC&quot;, COV.FC, &quot;/&quot;, path), pattern=&quot;.*[[:digit:]]{4}\\\\_F.*\\\\.tif$&quot;, full.names=TRUE)) map.names &lt;- sub(&quot;r$&quot;, &quot;&quot;, names(maps)) map.cols &lt;- sub(paste0(path, &quot;\\\\_&quot;), &quot;X&quot;, sub(&quot;\\\\_[[:alnum:]]+$&quot;, &quot;M&quot;, map.names)) no.map.cols &lt;- paste0(&quot;X&quot;, PERIOD[!PERIOD %in% gsub(&quot;[[:alpha:]]&quot;, &quot;&quot;, map.cols)], &quot;_&quot;) col.order &lt;- c(map.cols, no.map.cols)[order(c(map.cols, no.map.cols))] maps.values &lt;- values(maps) # extract vector with cell values (huge matrix, takes some time) colnames(maps.values) &lt;- map.cols nsubsets &lt;- numCores - 1 # define subsets for parallel processing subsets &lt;- c(0, floor((1:nsubsets)*(nrow(maps.values)/nsubsets))) # Parallel cleaning of pixel trajectories ############################### registerDoParallel(.env$numCores-1) maps.values.clean &lt;- foreach(i=1:nsubsets, .combine=rbind) %dopar% { # 0. get subset from maps.values val &lt;- maps.values[(subsets[i]+1):subsets[i+1], ] # get one tile of the matrix val[!(is.na(val) | val %in% c(1,3))] &lt;- NA # set everything to NA that is not NA already or 1,3 # 1. remove isolated NAs str &lt;- apply(val, 1, paste, collapse=&quot;&quot;) # convert to strings str.c &lt;- gsub(&quot;NA&quot;, &quot;9&quot;, str) # replace NA with 9 while(!identical(str, str.c)) { # clean until convergence str &lt;- str.c str.c &lt;- gsub(&quot;^(.*3)9(9*3.*)$&quot;, &quot;\\\\13\\\\2&quot;, str.c) # set NA to the corrsponding class str.c &lt;- gsub(&quot;^(.*1)9(9*1.*)$&quot;, &quot;\\\\11\\\\2&quot;, str.c) } # convert back to numeric vector val &lt;- matrix(as.numeric(unlist(strsplit(str.c, &quot;&quot;))), ncol=ncol(val), byrow=TRUE) val[val==9] &lt;- NA # and set NAs again colnames(val) &lt;- map.cols # 2. clean trajectories with sliding window val.c &lt;- val val.o &lt;- val[] &lt;- 0 iter &lt;- 0 # clean until convergence while(!identical(val, val.c) &amp; !identical(val.o, val.c)) { iter &lt;- iter+1 message(&quot; -Clean modal: iteration &quot;, iter, &quot; ... &quot;, appendLF = FALSE) val.o &lt;- val val &lt;- val.c for(l in 3:(ncol(val)-2)) { # 3rd - 3rd last year: modal window size 5 val.c[,l] &lt;- apply(val[,(l-2):(l+2)], 1, modal, na.rm=TRUE, ties=&#39;NA&#39;) } for(l in c(2, ncol(val)-1)) { # 2nd and 2nd last year: modal window size 3 val.c[,l] &lt;- apply(val.c[,(l-1):(l+1)], 1, modal, na.rm=TRUE, ties=&#39;NA&#39;) } message(&quot;done&quot;) } val &lt;- val.c # 3. final regexp cleaning val &lt;- cbind(val, matrix(nrow=nrow(val), # add years with no observation ncol=length(no.map.cols), dimnames=list(NULL, no.map.cols))) val &lt;- val[, col.order] # order correctly str &lt;- apply(val, 1, paste, collapse=&quot;&quot;) # convert to strings str.c &lt;- gsub(&quot;NA&quot;, &quot;9&quot;, str) # replace NA with 9 # replace remaining NAs with preceeding land-cover while(!identical(str, str.c)) { # clean until convergence str &lt;- str.c str.c &lt;- gsub(&quot;39&quot;, &quot;33&quot;, str.c) # set NA to preceeding class str.c &lt;- gsub(&quot;19&quot;, &quot;11&quot;, str.c) } str &lt;- &quot;&quot; # replace trailing NAs with following land-cover while(!identical(str, str.c)) { # clean until convergence str &lt;- str.c str.c &lt;- gsub(&quot;93&quot;, &quot;33&quot;, str.c) # set NA to following class str.c &lt;- gsub(&quot;91&quot;, &quot;11&quot;, str.c) } str &lt;- &quot;&quot; # removing isolated 1s (regeneration that is lost again, not) while(!identical(str, str.c)) { str &lt;- str.c str.c &lt;-gsub(&quot;^(.*3)1(1{0,9}3.*)$&quot;, &quot;\\\\13\\\\2&quot;, str.c) # removing series of 1s up to length 10 #str.c &lt;-gsub(&quot;^(.*33)1(1*33.*)$&quot;, &quot;\\\\13\\\\2&quot;, str.c) } str &lt;- &quot;&quot; # consider regeneration only as forest after 10 years (before it is considered as potential regeneration 2) while(!identical(str, str.c)) { str &lt;- str.c # irgendetwas -&gt; nichtwald -&gt; (potentielle regen, max 8) -&gt; Wald -&gt; irgendetwas str.c &lt;-gsub(&quot;^(.*32{0,8})1(.*)$&quot;, &quot;\\\\12\\\\2&quot;, str.c) } str &lt;- &quot;&quot; val &lt;- matrix(as.numeric(unlist(strsplit(str.c, &quot;&quot;))), ncol=ncol(val), byrow=TRUE) # convert back to numeric vector val[val==9] &lt;- NA # and set NAs again colnames(val) &lt;- col.order val[,grepl(&quot;M$&quot;, colnames(val))] # and get data for map layers # in order to be considered as forest in 2003, a pixels needs to be forest as well in 2000 and 1991 # in order to be considered as &quot;regeneration&quot; in 2018, a pixel needs to be forest from 2008 onwards } # Write the result to the disk -------------------------------------------- # without the &quot;uncleaned&quot; first and the last layer values(maps) &lt;- maps.values.clean writeRaster(dropLayer(maps, c(1,nlayers(maps))), filename=paste0(FCC.CLN.DIR, &quot;/FC&quot;, COV.FC, &quot;/&quot;, path, &quot;/&quot;, map.names[2:(nlayers(maps)-1)], &quot;c.tif&quot;), bylayer=TRUE, format=&quot;GTiff&quot;, datatype=&quot;INT2U&quot;, overwrite=TRUE) # write trajectories to textfile maps.strings &lt;- apply(maps.values.clean, 1, paste, collapse=&quot;&quot;) sink(paste0(FCC.CLN.DIR, &quot;/FC&quot;, COV.FC, &quot;/&quot;, path, &quot;/Trajectories.txt&quot;)) print(table(maps.strings)) sink() } # Functions for spatial cleaning of a set of images ----------------------- # Remove forest/defor/regen pixels that NEVER has been part of &quot;forest &gt; 0.5ha&quot; since 2003 clean.spatial.forest &lt;- function(maps, exclude = NULL, size=6, connectedness=8){ # default: at least 6 connected pixels (0.54 ha) with 8-connectedness tmp1 &lt;- tempfile(pattern = &quot;&quot;, fileext = &quot;.tif&quot;) tmp2 &lt;- tempfile(pattern = &quot;&quot;, fileext = &quot;.tif&quot;) fcc.map &lt;- apply(maps[[-exclude]][], 1, paste, collapse=&quot;&quot;) onceforest.map &lt;- raster(maps) onceforest.map[] &lt;- NA # onceforest.map &lt;- fcc.map # create &quot;once-forest&quot; map onceforest.map[grepl(&quot;^3*$&quot;, fcc.map)] &lt;- 3 onceforest.map[grepl(&quot;^.*1.*$&quot;, fcc.map)] &lt;- 1 onceforest.map[grepl(&quot;^.*2.*$&quot;, fcc.map)] &lt;- 1 writeRaster(onceforest.map, tmp1) # remove isolated forest patches &lt; xy ha system(paste0(&quot;gdal_sieve.py -st &quot;, size, &quot; -&quot;, connectedness, &quot; -nomask &quot;, tmp1, &quot; &quot;, tmp2)) onceforest.map.clean &lt;- raster(tmp2) onceforest.map.clean[onceforest.map.clean == -2147483648] &lt;- NA # mask the maps with this cleaned forest map maps.clean &lt;- mask(maps, onceforest.map.clean, maskvalue=3, updatevalue=3) writeRaster(maps.clean, filename=paste0(FCC.CLN.DIR, &quot;/FC&quot;, COV.FC, &quot;/TGO/&quot;, names(maps.clean), &quot;f.tif&quot;), bylayer=TRUE, format=&quot;GTiff&quot;, datatype=&quot;INT2U&quot;, overwrite=TRUE) } clean.spatial.fcc &lt;- function(maps, size=3, connectedness=8){ # at least 3 connected pixels (8-connectedness) # combine the maps 2003, 2005 and 2018 # change: 0 &quot;321&quot; &quot;112&quot; &quot;122&quot; &quot;322&quot; &quot;132&quot; &quot;332&quot; &quot;113&quot; &quot;213&quot; &quot;313&quot; # stable non-forest: 3 &quot;333&quot; # stable forest: 1 &quot;111&quot; # filter the map (forest islands and non-forest islands are also filtered out) system(&quot;/Library/Frameworks/GDAL.framework/Programs/gdal_sieve.py -st 3 -8 -nomask ./results/forest.tif ./results/forest_ha.tif&quot;) } ### DO THE WORK ########################################################### # change extent of p192_2019 to the extent of other p192 images (TODO: to be done already in 1_prepare-images.R) extend(brick(paste0(FCC.RAW.DIR, &quot;/FC&quot;, COV.FC, &quot;/p192/p192_2019_F&quot;, COV.FC, &quot;r.tif&quot;)), raster(paste0(FCC.RAW.DIR, &quot;/FC&quot;, COV.FC, &quot;/p192/p192_2018_F&quot;, COV.FC, &quot;r.tif&quot;)), filename=paste0(FCC.RAW.DIR, &quot;/FC&quot;, COV.FC, &quot;/p192/p192_2019_F&quot;, COV.FC, &quot;rt.tif&quot;), format=&quot;GTiff&quot;, datatype=&quot;INT2U&quot;) file.rename(paste0(FCC.RAW.DIR, &quot;/FC&quot;, COV.FC, &quot;/p192/p192_2019_F&quot;, COV.FC, &quot;rt.tif&quot;), paste0(FCC.RAW.DIR, &quot;/FC&quot;, COV.FC, &quot;/p192/p192_2019_F&quot;, COV.FC, &quot;r.tif&quot;)) # Temporal cleaning of paths ---------------------------------------------- for(path in c(&quot;p192&quot;, &quot;p193&quot;, &quot;p194&quot;)) { clean.temporal(path) } # Merging paths ----------------------------------------------------------- for(year in JNT.YEARS) { merge(mask(crop(brick(paste0(FCC.CLN.DIR, &quot;/FC&quot;, COV.FC, &quot;/p193/p193_&quot;, year, &quot;_F&quot;, COV.FC, &quot;c.tif&quot;)), TGO), TGO), mask(crop(brick(paste0(FCC.CLN.DIR, &quot;/FC&quot;, COV.FC, &quot;/p192/p192_&quot;, year, &quot;_F&quot;, COV.FC, &quot;c.tif&quot;)), TGO), TGO), mask(crop(brick(paste0(FCC.CLN.DIR, &quot;/FC&quot;, COV.FC, &quot;/p194/p194_&quot;, year, &quot;_F&quot;, COV.FC, &quot;c.tif&quot;)), TGO), TGO), filename=paste0(FCC.CLN.DIR, &quot;/FC&quot;, COV.FC, &quot;/TGO/TGO_&quot;, year, &quot;_F&quot;, COV.FC, &quot;c.tif&quot;), overwrite=TRUE) } # merge(mask(crop(brick(paste0(FCC.CLN.DIR, &quot;/p193/p193_1990_F30c.tif&quot;)), TGO), TGO), # mask(crop(brick(paste0(FCC.CLN.DIR, &quot;/p192/p192_1991_F30c.tif&quot;)), TGO), TGO), # mask(crop(brick(paste0(FCC.CLN.DIR, &quot;/p194/p194_1997_F30c.tif&quot;)), TGO), TGO), # filename=paste0(FCC.CLN.DIR, &quot;/TGO/1_clean/TGO_1991_F30c.tif&quot;), overwrite=TRUE) # # merge(mask(crop(brick(paste0(FCC.CLN.DIR, &quot;/p193/p193_2000_F30c.tif&quot;)), TGO), TGO), # mask(crop(brick(paste0(FCC.CLN.DIR, &quot;/p192/p192_2001_F30c.tif&quot;)), TGO), TGO), # mask(crop(brick(paste0(FCC.CLN.DIR, &quot;/p194/p194_2000_F30c.tif&quot;)), TGO), TGO), # filename=paste0(FCC.CLN.DIR, &quot;/TGO/1_clean/TGO_2000_F30c.tif&quot;), overwrite=TRUE) # # merge(mask(crop(brick(paste0(FCC.CLN.DIR, &quot;/p193/p193_2009_F30c.tif&quot;)), TGO), TGO), # mask(crop(brick(paste0(FCC.CLN.DIR, &quot;/p192/p192_2011_F30c.tif&quot;)), TGO), TGO), # mask(crop(brick(paste0(FCC.CLN.DIR, &quot;/p194/p194_2010_F30c.tif&quot;)), TGO), TGO), # filename=paste0(FCC.CLN.DIR, &quot;/TGO/1_clean/TGO_2010_F30c.tif&quot;), overwrite=TRUE) # # merge(mask(crop(brick(paste0(FCC.CLN.DIR, &quot;/p193/p193_2013_F30c.tif&quot;)), TGO), TGO), # mask(crop(brick(paste0(FCC.CLN.DIR, &quot;/p192/p192_2013_F30c.tif&quot;)), TGO), TGO), # mask(crop(brick(paste0(FCC.CLN.DIR, &quot;/p194/p194_2012_F30c.tif&quot;)), TGO), TGO), # filename=paste0(FCC.CLN.DIR, &quot;/TGO/1_clean/TGO_2013_F30c.tif&quot;), overwrite=TRUE) # Spatial cleaning of results (only from 2003 onwards) ----------------------------------- clean.spatial.forest(maps = stack(dir(paste0(FCC.CLN.DIR, &quot;/FC&quot;, COV.FC, &quot;/TGO&quot;), pattern = &quot;c\\\\.tif&quot;, full.names = TRUE)), exclude = c(1), size = 6, connectedness = 8) # exclude the first layer (1987) for creating the forest / non-forest mask # # and put together in one stack # tmp &lt;- stack(dir(paste0(FCC.CLN.DIR, &quot;/TGO/2_clean_05ha&quot;), pattern=&quot;.*[[:digit:]]{4}.*&quot;, full.names=TRUE)) # writeRaster(tmp, paste0(FCC.CLN.DIR, &quot;/TGO/2_clean_05ha/TGO_stack_F30c_05ha.tif&quot;), overwrite=TRUE) # # # Loading PHCF-Filter ----------------------------------------------------- # # pixels with value 1 will only remain if they are in squares of 4 or adjacent to that # # otherwise, if they are adjacent to a pixel with value &#39;2&#39; they will be converted to &#39;2&#39; # # otherwise, they will be converted to zero # # # compiling and loading C code that implements the PHCF filter # system(&quot;R CMD SHLIB ./phcf_filter/phcf_filter.c&quot;) # dyn.load(&quot;../phcf_filter/phcf_filter.so&quot;) # # # defining function for facilitating the use of the C function # phcf_pt &lt;- function(defor) { # res &lt;- .C(&quot;phcf_filter&quot;, nrow(defor), ncol(defor), as.integer(defor[])) # return(res[[3]]) # } # # clean.spatial.fcc(maps = stack(raster(paste0(FCC.CLN.DIR, &quot;/TGO/2_clean_05ha/TGO_2003_F30c_05ha.tif&quot;)), # raster(paste0(FCC.CLN.DIR, &quot;/TGO/2_clean_05ha/TGO_2015_F30c_05ha.tif&quot;)), # raster(paste0(FCC.CLN.DIR, &quot;/TGO/2_clean_05ha/TGO_2018_F30c_05ha.tif&quot;))), # size = 3, connectedness = 8) "],
["03_validate-FC-maps.html", "", " 4.1.3 Validation des cartes La validation des cartes forêt/non-forêt se fait à l’aide de parcelles de validation sur lesquelles l’occupation du sol (forêt/terre boisée/non-forêt) a été déterminée par des photo-interprètes sur la base d’images Landsat pour les années 1987, 2003, 2015 et 2018. Pour ces parcelles, dans un premier temps, les classes correspondantes sont lues à partir des cartes. Ensuite, des matrices d’erreur sont générées, celles de la classification forêt/non-forestière pour les différentes années, ainsi que celles de l’évolution de la couverture terrestre sur différentes périodes. Ces matrices d’erreurs constituent la base de l’analyse de la précision des cartes. Example Le tableau suivant montre la matrice d’erreur des transitions de l’occupation du sol 2003 – 2018. Dans les colonnes sont les classes attribuées par les photo-interprètes, dans les lignes les classes selon les cartes forêt/non-forêt nettoyées. xFxF xFxN xNxF xNxN xFxF 536 52 80 105 xFxN 36 80 3 52 xNxF 23 0 73 29 xNxN 35 84 50 1175 MCF/03_validate-fc-maps.R ########################################################################## # NERF_Togo/FCC/6_validate-fc-maps.R: validate clean forest cover maps # ------------------------------------------------------------------------ # Bern University of Applied Sciences # Oliver Gardi, &lt;oliver.gardi@bfh.ch&gt; # 20 May 2019 # Variables ------------------------------------------------------- COV.FC &lt;- 30 ct &lt;- list() # List of confusion tables to be written to xls file # Load maps, training points -------------------------------- # Load raw and clean forest / non-forest maps maps &lt;- stack(c(paste0(FCC.RAW.DIR, &quot;/FC&quot;, COV.FC, &quot;/TGO/TGO_&quot;, VAL.YEARS, &quot;_F&quot;, COV.FC, &quot;r.tif&quot;), paste0(FCC.CLN.DIR, &quot;/FC&quot;, COV.FC, &quot;/TGO/TGO_&quot;, VAL.YEARS, &quot;_F&quot;, COV.FC, &quot;c.tif&quot;))) names(maps) &lt;- sub(&quot;TGO\\\\_&quot;, &quot;X&quot;, sub(&quot;\\\\_F[[:digit:]]{2}&quot;, &quot;&quot;, names(maps))) # Load training points used for calibration and extract 2018 values from the maps train.plots &lt;- readOGR(paste0(TRNPTS.DIR, &quot;/COV_parcelles.shp&quot;)) train.points &lt;- SpatialPointsDataFrame(gCentroid(train.plots, byid=TRUE), # convert polygons to spatial points (centroids) data.frame(COV=train.plots$ccov)) # with only COV in the attribute table train.points &lt;- raster::extract(maps[[c(&quot;X2018r&quot;,&quot;X2018c&quot;)]], train.points, sp=TRUE) # delete rows with NAs train.points &lt;- train.points[rowSums(is.na(train.points@data)) == 0, ] # reading validation plots ------------------------ # Load validation points, clean and extract map values val.plots &lt;- readOGR(paste0(VALPTS.DIR, &quot;/UOT_parcelles.shp&quot;)) val.points &lt;- SpatialPointsDataFrame(gCentroid(val.plots, byid=TRUE), data.frame(author=sub(&quot;^.*\\\\/&quot;, &quot;&quot;, val.plots$author), V1987=as.numeric(substr(val.plots$lc_87, 1, 1)), V2003=as.numeric(substr(val.plots$lc_03, 1, 1)), V2015=as.numeric(substr(val.plots$lc_15, 1, 1)), V2018=as.numeric(substr(val.plots$lc_18, 1, 1)))) val.points &lt;- raster::extract(maps, val.points, sp=TRUE) # delete rows with NAs val.points &lt;- val.points[rowSums(is.na(val.points@data)) == 0, ] # adjusting classes of reference points ---------------------- # if(COV.FC == 30) { # # change the woodland (2) to non-forest # val.points$V1987[val.points$V1987 == 2] &lt;- 3 # val.points$V2003[val.points$V2003 == 2] &lt;- 3 # val.points$V2015[val.points$V2015 == 2] &lt;- 3 # val.points$V2018[val.points$V2018 == 2] &lt;- 3 # } if(COV.FC == 10) { # change the woodland (2) to forest val.points$V1987[val.points$V1987 == 2] &lt;- 1 val.points$V2003[val.points$V2003 == 2] &lt;- 1 val.points$V2015[val.points$V2015 == 2] &lt;- 1 val.points$V2018[val.points$V2018 == 2] &lt;- 1 } # set cloud/shadow class (4) to forest or non-forest, according to the map val.points$V1987r &lt;- val.points$V1987; val.points$V1987r[val.points$V1987 %in% c(2,4)] &lt;- val.points$X1987r[val.points$V1987 %in% c(2,4)] val.points$V2003r &lt;- val.points$V2003; val.points$V2003r[val.points$V2003 %in% c(2,4)] &lt;- val.points$X2003r[val.points$V2003 %in% c(2,4)] val.points$V2015r &lt;- val.points$V2015; val.points$V2015r[val.points$V2015 %in% c(2,4)] &lt;- val.points$X2015r[val.points$V2015 %in% c(2,4)] val.points$V2018r &lt;- val.points$V2018; val.points$V2018r[val.points$V2018 %in% c(2,4)] &lt;- val.points$X2018r[val.points$V2018 %in% c(2,4)] # same thing for cleaned map val.points$V1987c &lt;- val.points$V1987; val.points$V1987c[val.points$V1987 %in% c(2,4)] &lt;- val.points$X1987c[val.points$V1987 %in% c(2,4)] val.points$V2003c &lt;- val.points$V2003; val.points$V2003c[val.points$V2003 %in% c(2,4)] &lt;- val.points$X2003c[val.points$V2003 %in% c(2,4)] val.points$V2015c &lt;- val.points$V2015; val.points$V2015c[val.points$V2015 %in% c(2,4)] &lt;- val.points$X2015c[val.points$V2015 %in% c(2,4)] val.points$V2018c &lt;- val.points$V2018; val.points$V2018c[val.points$V2018 %in% c(2,4)] &lt;- val.points$X2018c[val.points$V2018 %in% c(2,4)] # adjusting classes of maps ---------------------- # for the clean maps, set regeneration (2) of the clean maps to forest/non-forest according the validation points val.points$X1987c[val.points$X1987c == 2] &lt;- val.points$V1987c[val.points$X1987c == 2] val.points$X2003c[val.points$X2003c == 2] &lt;- val.points$V2003c[val.points$X2003c == 2] val.points$X2015c[val.points$X2015c == 2] &lt;- val.points$V2015c[val.points$X2015c == 2] val.points$X2018c[val.points$X2018c == 2] &lt;- val.points$V2018c[val.points$X2018c == 2] # set to non-forest where a 2 remains (regeneration on the maps and woodland in validation points) val.points$X1987c[val.points$X1987c == 2] &lt;- val.points$V1987c[val.points$X1987c == 2] &lt;- 3 val.points$X2003c[val.points$X2003c == 2] &lt;- val.points$V2003c[val.points$X2003c == 2] &lt;- 3 val.points$X2015c[val.points$X2015c == 2] &lt;- val.points$V2015c[val.points$X2015c == 2] &lt;- 3 val.points$X2018c[val.points$X2018c == 2] &lt;- val.points$V2018c[val.points$X2018c == 2] &lt;- 3 val.points@data &lt;- mutate_all(val.points@data, as.factor) ## DO THE WORK ------------------------------------------------------- # Compare forest cover map 2018 with reference 2018 --------- ref.map &lt;- raster(paste0(FCC.REF.DIR, &quot;/FC&quot;, COV.FC, &quot;/TGO_2018_FC&quot;, COV.FC, &quot;_R.tif&quot;)) ct[[&quot;MAP_REF.18r&quot;]] &lt;- confusionMatrix(as.factor(maps$X2018r[]), as.factor(ref.map[])) pdf(paste0(FCC.VAL.DIR, &quot;/FC2018_vs_COV2018_FC&quot;, COV.FC, &quot;.pdf&quot;)) plot(factor(train.points$X2018r, labels=c(&quot;Forest&quot;, &quot;Non-Forest&quot;)) ~ train.points$COV, xlab=&quot;Tree Cover 2018&quot;, ylab=&quot;Forest Cover Map 2018&quot;) dev.off() pdf(paste0(FCC.VAL.DIR, &quot;/COV2018_vs_FC2018_FC&quot;, COV.FC, &quot;.pdf&quot;)) plot(train.points$COV ~ factor(train.points$X2018r, labels=c(&quot;Forest&quot;, &quot;Non-Forest&quot;)), xlab=&quot;Forest Cover Map 2018&quot;, ylab=&quot;Tree Cover 2018&quot;) dev.off() # Map validation / confusion matrices ---------------------------------- confMat &lt;- function(xtrans, vtrans) { levels &lt;- unique(c(xtrans, vtrans)) return(confusionMatrix(factor(xtrans, levels=levels[order(levels)]), factor(vtrans, levels=levels[order(levels)]))) } # val.points.bu &lt;- val.points # backup val.points &lt;- val.points[!val.points$author %in% c(&quot;7_Aklasson_TOLEBA&quot;, &quot;8_Yawo_KONKO&quot;, &quot;2_Mamalnassoh_ABIGUIME&quot;), ] # check for authors whose validation points reduce precision # print(confMat(paste0(&quot;x&quot;, tmp$X2003c, tmp$X2015c, tmp$X2018c), # paste0(&quot;x&quot;, tmp$VX2003, tmp$VX2015, tmp$VX2018))$overall) # # for(author in unique(tmp$author)) { # print(author) # print(confMat(paste0(&quot;x&quot;, tmp$X2003c[tmp$author != author], tmp$X2015c[tmp$author != author], tmp$X2018c[tmp$author != author]), # paste0(&quot;x&quot;, tmp$VX2003[tmp$author != author], tmp$VX2015[tmp$author != author], tmp$VX2018[tmp$author != author]))$overall) # } for(t in c(&quot;r&quot;, &quot;c&quot;)) { # confusion matrices for individual dates ct[[paste0(&quot;MAP_VAL.87&quot;, t)]] &lt;- confMat(paste0(val.points[[paste0(&quot;X1987&quot;, t)]], &quot;x&quot;, &quot;x&quot;, &quot;x&quot; ), paste0(val.points[[paste0(&quot;V1987&quot;, t)]], &quot;x&quot;, &quot;x&quot;, &quot;x&quot; )) ct[[paste0(&quot;MAP_VAL.03&quot;, t)]] &lt;- confMat(paste0(&quot;x&quot;, val.points[[paste0(&quot;X2003&quot;, t)]], &quot;x&quot;, &quot;x&quot; ), paste0(&quot;x&quot;, val.points[[paste0(&quot;V2003&quot;, t)]], &quot;x&quot;, &quot;x&quot; )) ct[[paste0(&quot;MAP_VAL.15&quot;, t)]] &lt;- confMat(paste0(&quot;x&quot;, &quot;x&quot;, val.points[[paste0(&quot;X2015&quot;, t)]], &quot;x&quot; ), paste0(&quot;x&quot;, &quot;x&quot;, val.points[[paste0(&quot;V2015&quot;, t)]], &quot;x&quot; )) ct[[paste0(&quot;MAP_VAL.18&quot;, t)]] &lt;- confMat(paste0(&quot;x&quot;, &quot;x&quot;, &quot;x&quot;, val.points[[paste0(&quot;X2018&quot;, t)]]), paste0(&quot;x&quot;, &quot;x&quot;, &quot;x&quot;, val.points[[paste0(&quot;V2018&quot;, t)]])) # confusion matrices for 2-date transitions ct[[paste0(&quot;MAP_VAL.87.03&quot;, t)]] &lt;- confMat(paste0(val.points[[paste0(&quot;X1987&quot;, t)]], val.points[[paste0(&quot;X2003&quot;, t)]], &quot;x&quot;, &quot;x&quot; ), paste0(val.points[[paste0(&quot;V1987&quot;, t)]], val.points[[paste0(&quot;V2003&quot;, t)]], &quot;x&quot;, &quot;x&quot; )) ct[[paste0(&quot;MAP_VAL.03.15&quot;, t)]] &lt;- confMat(paste0(&quot;x&quot;, val.points[[paste0(&quot;X2003&quot;, t)]], val.points[[paste0(&quot;X2015&quot;, t)]], &quot;x&quot; ), paste0(&quot;x&quot;, val.points[[paste0(&quot;V2003&quot;, t)]], val.points[[paste0(&quot;V2015&quot;, t)]], &quot;x&quot; )) ct[[paste0(&quot;MAP_VAL.15.18&quot;, t)]] &lt;- confMat(paste0(&quot;x&quot;, &quot;x&quot;, val.points[[paste0(&quot;X2015&quot;, t)]], val.points[[paste0(&quot;X2018&quot;, t)]]), paste0(&quot;x&quot;, &quot;x&quot;, val.points[[paste0(&quot;V2015&quot;, t)]], val.points[[paste0(&quot;V2018&quot;, t)]])) ct[[paste0(&quot;MAP_VAL.87.18&quot;, t)]] &lt;- confMat(paste0(val.points[[paste0(&quot;X1987&quot;, t)]], &quot;x&quot;, &quot;x&quot;, val.points[[paste0(&quot;X2018&quot;, t)]]), paste0(val.points[[paste0(&quot;V1987&quot;, t)]], &quot;x&quot;, &quot;x&quot;, val.points[[paste0(&quot;V2018&quot;, t)]])) ct[[paste0(&quot;MAP_VAL.03.18&quot;, t)]] &lt;- confMat(paste0(&quot;x&quot;, val.points[[paste0(&quot;X2003&quot;, t)]], &quot;x&quot;, val.points[[paste0(&quot;X2018&quot;, t)]]), paste0(&quot;x&quot;, val.points[[paste0(&quot;V2003&quot;, t)]], &quot;x&quot;, val.points[[paste0(&quot;V2018&quot;, t)]])) # confusion matrices for 3-date transition ct[[paste0(&quot;MAP_VAL.03.15.18&quot;, t)]] &lt;- confMat(paste0(&quot;x&quot;,val.points[[paste0(&quot;X2003&quot;, t)]], val.points[[paste0(&quot;X2015&quot;, t)]], val.points[[paste0(&quot;X2018&quot;, t)]]), paste0(&quot;x&quot;, val.points[[paste0(&quot;V2003&quot;, t)]], val.points[[paste0(&quot;V2015&quot;, t)]], val.points[[paste0(&quot;V2018&quot;, t)]])) # confusion matrices for 4-date transition ct[[paste0(&quot;MAP_VAL.87.03.15.18&quot;, t)]] &lt;- confMat(paste0(val.points[[paste0(&quot;X1987&quot;, t)]], val.points[[paste0(&quot;X2003&quot;, t)]], val.points[[paste0(&quot;X2015&quot;, t)]], val.points[[paste0(&quot;X2018&quot;, t)]]), paste0(val.points[[paste0(&quot;V1987&quot;, t)]], val.points[[paste0(&quot;V2003&quot;, t)]], val.points[[paste0(&quot;V2015&quot;, t)]], val.points[[paste0(&quot;V2018&quot;, t)]])) } # Validation RapidEye map 2015 -------------------------------- rey &lt;- raster(paste0(DATA.DIR, &quot;/RapidEye/TGO_30m.tif&quot;)) names(rey) &lt;- &quot;R2015&quot; val.points &lt;- raster::extract(rey, val.points, sp=TRUE) val.points$Rr2015 &lt;- 3 val.points$Rr2015[val.points$R2015 %in% c(11, 12, 16, 18, 19)] &lt;- 1 val.points$VR2015 &lt;- val.points$V2015 val.points$VR2015[val.points$V2015 == 2] &lt;- val.points$Rr2015[val.points$V2015 == 2] ct[[&quot;REY_VAL.15&quot;]] &lt;- confMat(paste0(&quot;x&quot;, &quot;x&quot;, val.points$Rr2015, &quot;x&quot; ), paste0(&quot;x&quot;, &quot;x&quot;, val.points$VR2015, &quot;x&quot; )) # Write confusion tables ------------------------------------- save(ct, file=paste0(FCC.VAL.DIR, &quot;/FC&quot;, COV.FC, &quot;_flex_ConfTab.RData&quot;)) # write error matrices to Excel File write.xlsx(lapply(ct, &quot;[[&quot;, &quot;table&quot;), file=paste0(FCC.VAL.DIR, &quot;/FC&quot;, COV.FC, &quot;_flex_ConfTab.xlsx&quot;), colnames=TRUE, overwrite=TRUE) "],
["04_FC-maps-accuracy.html", "", " 4.1.4 Analyse de la précision La précision des cartes est déterminée selon la procédure de Olofsson et al. (2014). Le script utilisé est basé sur une implémentation des méthodes dans OpenForis. Le calcul de la précision se fait sur la matrice d’erreur d’une part et les surfaces couvertes par les catégories correspondantes d’autre part. Avec les informations sur les surfaces, la matrice d’erreur est extrapolée en une matrice d’erreur proportionnelle, qui indique dans les cellules les proportions de la surface totale. Sur cette base les indices de précision sont calculés: concrètement la précision globale, le Kappa de Cohen, la précision du producteur et la précision de l’utilisateur, y compris la variance et l’intervalle de confiance à 95 %. D’autre part, la matrice d’erreur proportionnelle est extrapolée à la superficie totale pour obtenir des estimations de superficie des différentes catégories, y compris les erreurs types et les intervalles de confiance. Example Le nombre de pixels cartographiés et matrice d’erreur des transitions de l’occupation du sol 2003 – 2018. Dans les colonnes sont les classes attribuées par les photo-interprètes, dans les lignes les classes selon les cartes forêt/non-forêt nettoyées. Pixels xFxF xFxN xNxF xNxN xFxF 12 590 594 536 52 80 105 xFxN 2 509 971 36 80 3 52 xNxF 1 637 331 23 0 73 29 xNxN 46 599 650 35 84 50 1175 La matrice d’erreur proportionelle aux surfaces cartographiées. La somme de tous les cellules est 1 (100%). xFxF xFxN xNxF xNxN xFxF 0.138 0.013 0.021 0.027 xFxN 0.008 0.019 0.001 0.012 xNxF 0.005 0 0.015 0.006 xNxN 0.019 0.046 0.027 0.643 Les indices de précision globales sont: Précision globale: 81.5% (± 1.5% intervalle de confiance) Cohen’s Kappa: 59.3% Indices de précision et le surfaces estimées par catégories (± intervalle de confiance à 95%) Surface cartes Prop. cartes Proportion ajustée Surface ajustée Précision utilisateur Précision producteur xFxF 1’133’153 0.20 0.17 ± 0.01 969’621 ± 54’104 0.69 ± 0.03 0.81 ± 0.03 xFxN 225’897 0.04 0.08 ± 0.01 444’034 ± 60’300 0.47 ± 0.08 0.24 ± 0.04 xNxF 147’360 0.03 0.06 ± 0.01 363’320 ± 50’777 0.58 ± 0.09 0.24 ± 0.04 xNxN 4’193’969 0.74 0.69 ± 0.01 3’923’405 ± 81’517 0.87 ± 0.02 0.94 ± 0.01 MCF/04_fc-maps-accuracy.R ########################################################################## # NERF_Togo/FCC/7_fc-maps-accuracy.R: validate clean forest cover maps # ------------------------------------------------------------------------ # Bern University of Applied Sciences # Oliver Gardi, &lt;oliver.gardi@bfh.ch&gt; # 20 May 2019 # based on OpenForis implementation of Olofsson et al. (2014), written by # Antonia Ortmann, 20 October, 2014 # Source: https://github.com/openforis/accuracy-assessment/blob/master/Rscripts/error_matrix_analysis.R VALSET &lt;- &quot;FC30_flex&quot; # Function for estimating accuracies ---------------------------------- accuracy.estimate &lt;- function(areas.map, error.matrix, filename=NULL, pixelsize=30^2/10000) { # remove &quot;x&quot; from the category colnames(error.matrix) &lt;- rownames(error.matrix) &lt;- gsub(&quot;x&quot;, &quot;&quot;, colnames(error.matrix)) # match category order maparea &lt;- areas.map[match(rownames(error.matrix), names(areas.map))] ma &lt;- error.matrix dyn &lt;- names(maparea) # calculate the area proportions for each map class aoi &lt;- sum(maparea) propmaparea &lt;- maparea/aoi # convert the absolute cross tab into a probability cross tab ni. &lt;- rowSums(ma) # number of reference points per map class propma &lt;- as.matrix(ma/ni. * as.vector(propmaparea)) propma[is.nan(propma)] &lt;- 0 # for classes with ni. = 0 # estimate the accuracies now OA &lt;- sum(diag(propma)) # overall accuracy (Eq. 1 in Olofsson et al. 2014) pe &lt;- 0 # Agreement by chance ... for (i in 1:length(dyn)) { pe &lt;- pe + sum(propma[i,]) * sum(propma[,i]) } K &lt;- (OA - pe) / (1 - pe) # ... for Cohen&#39;s Kappa UA &lt;- diag(propma) / rowSums(propma) # user&#39;s accuracy (Eq. 2 in Olofsson et al. 2014) PA &lt;- diag(propma) / colSums(propma) # producer&#39;s accuracy (Eq. 3 in Olofsson et al. 2014) # estimate confidence intervals for the accuracies V_OA &lt;- sum(as.vector(propmaparea)^2 * UA * (1 - UA) / (ni. - 1), na.rm=T) # variance of overall accuracy (Eq. 5 in Olofsson et al. 2014) V_UA &lt;- UA * (1 - UA) / (rowSums(ma) - 1) # variance of user&#39;s accuracy (Eq. 6 in Olofsson et al. 2014) # variance of producer&#39;s accuracy (Eq. 7 in Olofsson et al. 2014) N.j &lt;- array(0, dim=length(dyn)) aftersumsign &lt;- array(0, dim=length(dyn)) for(cj in 1:length(dyn)) { N.j[cj] &lt;- sum(maparea / ni. * ma[, cj], na.rm=T) aftersumsign[cj] &lt;- sum(maparea[-cj]^2 * ma[-cj, cj] / ni.[-cj] * ( 1 - ma[-cj, cj] / ni.[-cj]) / (ni.[-cj] - 1), na.rm = T) } V_PA &lt;- 1/N.j^2 * ( maparea^2 * (1-PA)^2 * UA * (1-UA) / (ni.-1) + PA^2 * aftersumsign ) V_PA[is.nan(V_PA)] &lt;- 0 # proportional area estimation propAreaEst &lt;- colSums(propma) # proportion of area (Eq. 8 in Olofsson et al. 2014) AreaEst &lt;- propAreaEst * sum(maparea) # estimated area # standard errors of the area estimation (Eq. 10 in Olofsson et al. 2014) V_propAreaEst &lt;- array(0, dim=length(dyn)) for (cj in 1:length(dyn)) { V_propAreaEst[cj] &lt;- sum((as.vector(propmaparea) * propma[, cj] - propma[, cj] ^ 2) / ( rowSums(ma) - 1)) } V_propAreaEst[is.na(V_propAreaEst)] &lt;- 0 # produce result tables res &lt;- list() res$PREDICTED_PX &lt;- as.table(maparea) res$ERROR_MATRIX &lt;- ma res$ERROR_MATRIX_PROP &lt;- round(propma, 3) res$OVERALL_ACC &lt;- data.frame(accuracy=round(c(OA, K), 3), CI=round(c(1.96 * sqrt(V_OA), NA), 3), row.names=c(&quot;OA&quot;, &quot;Kappa&quot;)) res$CLASSES_ACC &lt;- data.frame(maparea=round(maparea * pixelsize, 3)) # in ha res$CLASSES_ACC$prop_maparea &lt;- round(propmaparea, 3) res$CLASSES_ACC$adj_proparea &lt;- round(propAreaEst, 3) res$CLASSES_ACC$CI_adj_proparea &lt;- round(1.96 * sqrt(V_propAreaEst), 3) res$CLASSES_ACC$adj_area &lt;- round(propAreaEst * aoi * pixelsize, 3) # in ha res$CLASSES_ACC$CI_adj_area &lt;- round(1.96 * sqrt(V_propAreaEst) * aoi * pixelsize, 3) # in ha res$CLASSES_ACC$UA &lt;- round(UA, 3) res$CLASSES_ACC$CI_UA &lt;- round(1.96 * sqrt(V_UA), 3) res$CLASSES_ACC$PA &lt;- round(PA, 3) res$CLASSES_ACC$CI_PA &lt;- round(1.96 * sqrt(V_PA), 3) # write results to Excel File if(!is.null(filename)) { write.xlsx(res, file=filename, col.names=TRUE, row.names=TRUE, overwrite=TRUE) } return(res) } # DO THE WORK ---------------------------------- # load the error matrices (R object ct) load(paste0(FCC.VAL.DIR, &quot;/ConfTab_&quot;, VALSET, &quot;.RData&quot;)) # load the predictions and convert &quot;potential regeneration (2)&quot; to &quot;non-forest (3)&quot; fc.2003 &lt;- brick(paste0(FCC.CLN.DIR, &quot;/FC30/TGO/TGO_2003_F30cf.tif&quot;)); fc.2003[fc.2003==2] &lt;- 3 fc.2015 &lt;- brick(paste0(FCC.CLN.DIR, &quot;/FC30/TGO/TGO_2015_F30cf.tif&quot;)); fc.2015[fc.2015==2] &lt;- 3 fc.2018 &lt;- brick(paste0(FCC.CLN.DIR, &quot;/FC30/TGO/TGO_2018_F30cf.tif&quot;)); fc.2018[fc.2018==2] &lt;- 3 # create 3-date transition map fcc &lt;- 100 * fc.2003 + 10 * fc.2015 + 1 * fc.2018 # get pixel counts (takes time) and separate for different dates / transitions freq &lt;- table(fcc[]) tmp &lt;- as.numeric(freq); names(tmp) &lt;- names(freq); freq &lt;- tmp freq.03.15.18 &lt;- c(freq[&quot;111&quot;], freq[&quot;113&quot;], freq[&quot;131&quot;], freq[&quot;133&quot;], freq[&quot;311&quot;], freq[&quot;313&quot;], freq[&quot;331&quot;], freq[&quot;333&quot;]) names(freq.03.15.18) &lt;- c(&quot;111&quot;, &quot;113&quot;, &quot;131&quot;, &quot;133&quot;, &quot;311&quot;, &quot;313&quot;, &quot;331&quot;, &quot;333&quot;); freq.03.15.18[is.na(freq.03.15.18)] &lt;- 0 freq.03.18 &lt;- c(sum(freq[&quot;111&quot;],freq[&quot;131&quot;], na.rm=T), sum(freq[&quot;113&quot;],freq[&quot;133&quot;], na.rm=T), sum(freq[&quot;311&quot;],freq[&quot;331&quot;], na.rm=T), sum(freq[&quot;313&quot;],freq[&quot;333&quot;], na.rm=T)) names(freq.03.18) &lt;- c(&quot;11&quot;, &quot;13&quot;, &quot;31&quot;, &quot;33&quot;); freq.03.18[is.na(freq.03.18)] &lt;- 0 freq.03.15 &lt;- c(sum(freq[&quot;111&quot;],freq[&quot;113&quot;], na.rm=T), sum(freq[&quot;131&quot;],freq[&quot;133&quot;], na.rm=T), sum(freq[&quot;311&quot;],freq[&quot;313&quot;], na.rm=T), sum(freq[&quot;331&quot;],freq[&quot;333&quot;], na.rm=T)) names(freq.03.15) &lt;- c(&quot;11&quot;, &quot;13&quot;, &quot;31&quot;, &quot;33&quot;); freq.03.15[is.na(freq.03.15)] &lt;- 0 freq.15.18 &lt;- c(sum(freq[&quot;111&quot;],freq[&quot;311&quot;], na.rm=T), sum(freq[&quot;113&quot;],freq[&quot;313&quot;], na.rm=T), sum(freq[&quot;131&quot;],freq[&quot;331&quot;], na.rm=T), sum(freq[&quot;133&quot;],freq[&quot;333&quot;], na.rm=T)) names(freq.15.18) &lt;- c(&quot;11&quot;, &quot;13&quot;, &quot;31&quot;, &quot;33&quot;); freq.15.18[is.na(freq.15.18)] &lt;- 0 freq.03 &lt;- c(sum(freq[&quot;111&quot;],freq[&quot;131&quot;], freq[&quot;113&quot;],freq[&quot;133&quot;], na.rm=T), sum(freq[&quot;311&quot;],freq[&quot;331&quot;], freq[&quot;313&quot;],freq[&quot;333&quot;], na.rm=T)) names(freq.03) &lt;- c(&quot;1&quot;, &quot;3&quot;); freq.03[is.na(freq.03)] &lt;- 0 freq.15 &lt;- c(sum(freq[&quot;111&quot;],freq[&quot;311&quot;], freq[&quot;113&quot;],freq[&quot;313&quot;], na.rm=T), sum(freq[&quot;131&quot;],freq[&quot;331&quot;], freq[&quot;133&quot;],freq[&quot;333&quot;], na.rm=T)) names(freq.15) &lt;- c(&quot;1&quot;, &quot;3&quot;); freq.15[is.na(freq.15)] &lt;- 0 freq.18 &lt;- c(sum(freq[&quot;111&quot;],freq[&quot;311&quot;], freq[&quot;131&quot;],freq[&quot;331&quot;], na.rm=T), sum(freq[&quot;113&quot;],freq[&quot;313&quot;], freq[&quot;133&quot;],freq[&quot;333&quot;], na.rm=T)) names(freq.18) &lt;- c(&quot;1&quot;, &quot;3&quot;); freq.18[is.na(freq.18)] &lt;- 0 # create accuracy maps ------------------------------------ accuracy.estimate(freq.18, ct$MAP_VAL.18c$table , filename=paste0(FCC.VAL.DIR, &quot;/&quot;, VALSET, &quot;_Acc_18.xlsx&quot;)) accuracy.estimate(freq.15, ct$MAP_VAL.15c$table , filename=paste0(FCC.VAL.DIR, &quot;/&quot;, VALSET, &quot;_Acc_15.xlsx&quot;)) accuracy.estimate(freq.03, ct$MAP_VAL.03c$table , filename=paste0(FCC.VAL.DIR, &quot;/&quot;, VALSET, &quot;_Acc_03.xlsx&quot;)) accuracy.estimate(freq.03.15, ct$MAP_VAL.03.15c$table , filename=paste0(FCC.VAL.DIR, &quot;/&quot;, VALSET, &quot;_Acc_03-15.xlsx&quot;)) accuracy.estimate(freq.15.18, ct$MAP_VAL.15.18c$table , filename=paste0(FCC.VAL.DIR, &quot;/&quot;, VALSET, &quot;_Acc_15-18.xlsx&quot;)) accuracy.estimate(freq.03.18, ct$MAP_VAL.03.18c$table , filename=paste0(FCC.VAL.DIR, &quot;/&quot;, VALSET, &quot;_Acc_03-18.xlsx&quot;)) accuracy.estimate(freq.03.15.18, ct$MAP_VAL.03.15.18c$table , filename=paste0(FCC.VAL.DIR, &quot;/&quot;, VALSET, &quot;_Acc_03-15-18.xlsx&quot;)) # calculate forest cover, loss and gains --------------------- res &lt;- data.frame(year = c(2003, 2015, 2018), total.ha = c(freq.03[&quot;1&quot;], freq.15[&quot;1&quot;], freq.18[&quot;1&quot;]) * 30^2/10000, defor.ha = c(NA, freq.03.15[&quot;13&quot;], freq.15.18[&quot;13&quot;]) * 30^2/10000, regen.ha = c(NA, freq.03.15[&quot;31&quot;], freq.15.18[&quot;31&quot;]) * 30^2/10000) # Example from Olofsson et al. (2014) # ----------------------------------- # ct &lt;- matrix(data=c(66, 0, 5, 4, # 0, 55, 8, 12, # 1, 0, 153, 11, # 2, 1, 9, 313), # nrow=4, # byrow=TRUE, # dimnames = list(Prediction=c(&quot;FN&quot;, &quot;NF&quot;, &quot;FF&quot;, &quot;NN&quot;), Reference=c(&quot;FN&quot;, &quot;NF&quot;, &quot;FF&quot;, &quot;NN&quot;))) # # px &lt;- c(FN=200000, NF=150000, FF=3200000, NN=6450000) # # accuracy.estimate(px, ct) "],
["05_analyse-FC-maps.html", "", " 4.1.5 Analyse des cartes Les cartes forêt/non-forêt nettoyées de 1987 à 2018 sont utilisées pour créer des cartes des changements forestiers. En utilisant la fonction fc() les pixels qui passent d’une situation “non-forêt” ou “régénération potentielle” à “régénération” sont enregistrés comme gains de surface forestière dans l’année correspondante, les pixels qui passent d’une situation “forêt” ou “régénération” à “non-forêt” sont enregistrés comme perte de surface forestière. Sur cette base, pour chaque année disponible dans la série, une compilation des surfaces forestières, de la surface régénérée et de la régénération potentielle, ainsi que des gains et pertes de forêts dans la période précédente est effectuée. Le tableau de la superficie forestière sert à son tour de base pour déterminer les changements annuelles de la surface forestière sur différentes périodes de temps à l’aide de la fonction fcc(). En plus des changements annuelles absolues, les taux de changement correspondants sont calculés selon la formule suivante : \\(r = (1/(t2 - t1)) \\times ln(A2/A1)\\) (Puyravaud, 2003). La fonction plot.fc() affiche graphiquement le tableau des surfaces forestières et montre comment la surface forestière et la régénération se développent. La fonction plot.fcc() crée un diagramme en barres des gains et des pertes annuelles de la surface forestières sur certaines périodes de temps. Enfin, des cartes de la perte de forêts, de la régénération potentielle et de la régénération sont créées, avec les années de changement observées comme valeurs de pixel. Example Tableau des superfaces forestières pour le Togo La surface forestière initiale (existant depuis 1987), la superficie forestière secondaire (régénérée depuis 1987), la surface de la régénération potentielle (&lt; 10 ans) et la déforestation et l’accroissement enregistrés dans la période précédente. Tous les chiffres sont exprimés en hectares. Année Surface f. totale Surface f. initiale Surface f. secondaire Régén. potentielle Pertes surface f. Gains surface f. 1987 1’265’377 1’265’377 0 133’038 — — 2003 1’359’051 1’193’731 165’320 90’972 -71’646 165’320 2005 1’321’963 1’166’156 155’807 122’718 -37’088 0 2007 1’281’909 1’136’373 145’536 161’787 -40’154 101 2015 1’290’948 1’059’301 231’647 156’237 -99’560 108’600 2017 1’290’615 1’035’724 254’891 169’171 -39’503 39’170 2018 1’280’513 1’019’489 261’024 188’715 -27’027 16’925 Changement annuelle de la superficie forestière au Togo, pour 2003 – 2015, 2015 – 2018 et toute la période 2003 – 2018, montrant les pertes et les gains brutes des forêts, et le changement net de la surface forestière en hectares par an et les taux correspondants en %/an. Période Pertes (ha/an) Gains (ha/an) Ch. net (ha/an) Pertes (%/an) Gains (%/an) Ch. net (%/an) 2003 – 2015 -14’734 9’058 -5’675 -1.2 0.6 -0.4 2015 – 2018 -22’177 18’699 -3’478 -1.8 1.4 -0.3 2003 – 2018 -16’222 10’986 -5’236 -1.3 0.8 -0.4 Changement de la surface forestière du Togo 2003 – 2018 Changement de la superficie forestière (à gauche) ansi que les gains et les pertes annuelles pour les périodes 2003 – 2015 et 2015 – 2018 (à droite). Complexité de l’évolution de la superficie forestière en prenant l’exemple d’une petite partie de la région des Plateaux. Pertes de forêts 2003 – 2018 (jaune à orange) et croissance des forêts au cours de la même période (bleu à blanc). Le fond noir est non-forêt sur toute la période. MCF/05_analyse-fc-maps.R ########################################################################## # NERF_Togo/FCC/8_analyse-fc-maps.R: analyze clean forest cover maps # ------------------------------------------------------------------------ # Bern University of Applied Sciences # Oliver Gardi, &lt;oliver.gardi@bfh.ch&gt; # 20 May 2019 COV.FC &lt;- 30 # Function for building forest cover table --------------------------------- fc &lt;- function(map, aoi=NULL) { if(!is.null(aoi)) map &lt;- mask(crop(map, aoi), aoi) # convert non-forest to 0 # potential regeneration to 1 # regeneration to 2 # forest to 3 tab.fc &lt;- map[] tab.fc[tab.fc == 3] &lt;- 0 tab.fc[tab.fc == 1] &lt;- 3 tab.fc[tab.fc == 2] &lt;- 1 for(i in 2:ncol(tab.fc)) { tab.fc[!is.na(tab.fc[,i]) &amp; tab.fc[,i] == 3 &amp; !is.na(tab.fc[,i-1]) &amp; tab.fc[,i-1] &lt; 3, i] &lt;- 2 # mark as regeneration when it was non-forest or regeneration before } tab.fcc &lt;- tab.fc[,1:ncol(tab.fc)-1] tab.fcc[] &lt;- 0 for(i in 1:ncol(tab.fcc)) { tab.fcc[tab.fc[,i] &lt;= 1 &amp; tab.fc[,i+1] &gt;= 2, i] &lt;- 1 # mark forest gain tab.fcc[tab.fc[,i] &gt;= 2 &amp; tab.fc[,i+1] &lt;= 1, i] &lt;- 2 # mark forest loss } dates &lt;- as.numeric(substr(names(map), 2, 5)) fc &lt;- data.frame(year = dates, initi.ha = colSums(tab.fc==3, na.rm=TRUE) * 30^2/10000, secon.ha = colSums(tab.fc==2, na.rm=TRUE) * 30^2/10000, poten.ha = colSums(tab.fc==1, na.rm=TRUE) * 30^2/10000) fc$total.ha &lt;- fc$initi.ha + fc$secon.ha fc$defor.ha &lt;- -c(colSums(tab.fcc==2, na.rm=TRUE) * 30^2/10000, NA) fc$regen.ha &lt;- c(colSums(tab.fcc==1, na.rm=TRUE) * 30^2/10000, NA) return(fc) } fcc &lt;- function(fc.tab, years=fc.tab$year) { for(i in 1:(length(years)-1)) { fc.tab$defor.ha.yr[fc.tab$year==years[i]] &lt;- sum(fc.tab$defor.ha[fc.tab$year &gt;= years[i] &amp; fc.tab$year &lt; years[i+1]])/(years[i+1]-years[i]) fc.tab$regen.ha.yr[fc.tab$year==years[i]] &lt;- sum(fc.tab$regen.ha[fc.tab$year &gt;= years[i] &amp; fc.tab$year &lt; years[i+1]])/(years[i+1]-years[i]) fc.tab$netch.ha.yr[fc.tab$year==years[i]] &lt;- fc.tab$defor.ha.yr[fc.tab$year==years[i]] + fc.tab$regen.ha.yr[fc.tab$year==years[i]] fc.tab$defor.pc.yr[fc.tab$year==years[i]] &lt;- round(100 * 1/(years[i+1]-years[i]) * log((fc.tab$total.ha[fc.tab$year==years[i+1]] - sum(fc.tab$regen.ha[fc.tab$year &gt;= years[i] &amp; fc.tab$year &lt; years[i+1]]))/fc.tab$total.ha[fc.tab$year==years[i]]), 3) fc.tab$regen.pc.yr[fc.tab$year==years[i]] &lt;- round(100 * 1/(years[i+1]-years[i]) * log((fc.tab$total.ha[fc.tab$year==years[i+1]] - sum(fc.tab$defor.ha[fc.tab$year &gt;= years[i] &amp; fc.tab$year &lt; years[i+1]]))/fc.tab$total.ha[fc.tab$year==years[i]]), 3) fc.tab$netch.pc.yr[fc.tab$year==years[i]] &lt;- round(100 * 1/(years[i+1]-years[i]) * log(fc.tab$total.ha[fc.tab$year==years[i+1]]/fc.tab$total.ha[fc.tab$year==years[i]]), 3) } return(fc.tab) } # Function for plotting evolution of forest cover ----------------------------------------- plot.fc &lt;- function(fc, zone, filename=NULL) { if(zone==&quot;TGO&quot;) { title &lt;- &quot;Togo: Évolution de la couverure forestière 2003 - 2018&quot; ylim &lt;- c(0, 1400000) ybreaks &lt;- seq(0, 1400000, 200000) ymbreaks &lt;- seq(0, 1400000, 100000) } else { title &lt;- paste0(zone, &quot;: Évolution de la couverure forestière 2003 - 2018&quot;) ylim &lt;- c(0, 500000) ybreaks &lt;- seq(0, 500000, 100000) ymbreaks &lt;- seq(0, 500000, 50000) } if(!is.null(filename)) pdf(filename) print( fc %&gt;% gather(variable, value, secon.ha, initi.ha) %&gt;% ggplot(aes(x = year, y=value, fill=variable)) + geom_area(position=position_stack(reverse=TRUE)) + scale_fill_manual(name = NULL, breaks=c(&quot;secon.ha&quot;, &quot;initi.ha&quot;), values=c(alpha(&quot;#009E73&quot;, 0.8), alpha(&quot;#00BFC4&quot;, 0.5)), labels=c(&quot;Régéneration&quot;, &quot;Forêt depuis 1987&quot;)) + xlab(&quot;Année&quot;) + ylab(&quot;Hectares&quot;) + ggtitle(title) + scale_x_continuous(minor_breaks = NULL, breaks=c(1991, 2000, 2005, 2010, 2015, 2018)) + scale_y_continuous(breaks=ybreaks, minor_breaks = ymbreaks) + coord_cartesian(ylim=ylim) + theme_light() + theme(legend.position=c(0.6,0.2), legend.box = &quot;horizontal&quot;, legend.justification=c(-0.2,1.2)) ) if(!is.null(filename)) dev.off() } # Function for plotting forest cover change ----------------------------------- plot.fcc &lt;- function(fc, zone=NULL, breaks=NULL, filename=NULL) { my.fcc &lt;- fcc(fc) fcc.breaks &lt;- my.fcc if(!is.null(breaks)) fcc.breaks &lt;- fcc(fc, breaks)[fc$year %in% breaks,] my.fcc$period &lt;- c(my.fcc$year[2:nrow(my.fcc)] - my.fcc$year[1:(nrow(my.fcc)-1)], NA) my.fcc$center &lt;- my.fcc$year + my.fcc$period/2 fcc.breaks$period &lt;- c(fcc.breaks$year[2:nrow(fcc.breaks)] - fcc.breaks$year[1:(nrow(fcc.breaks)-1)], NA) fcc.breaks$center &lt;- fcc.breaks$year + fcc.breaks$period/2 if(zone==&quot;TGO&quot;) { title &lt;- &quot;Togo: Changements bruts et nets des surfaces forestières 2003 - 2018&quot; ylim &lt;- c(-35000, 20000) ybreaks &lt;- seq(-35000, 20000, 5000) ymbreaks &lt;- seq(-35000, 20000, 2500) } else { title &lt;- paste0(zone, &quot;: Changements bruts et nets des surfaces forestières 2003 - 2018&quot;) ylim &lt;- c(-15000, 10000) ybreaks &lt;- seq(-15000, 10000, 5000) ymbreaks &lt;- seq(-15000, 10000, 2500) } if(!is.null(filename)) pdf(filename) fcc.breaks$netdefor.ha.yr &lt;- fcc.breaks$netch.ha.yr fcc.breaks$netdefor.ha.yr[fcc.breaks$netdefor.ha.yr &gt; 0] &lt;- 0 fcc.breaks$netregen.ha.yr &lt;- fcc.breaks$netch.ha.yr fcc.breaks$netregen.ha.yr[fcc.breaks$netregen.ha.yr &lt; 0] &lt;- 0 print( fcc.breaks[-nrow(fcc.breaks),] %&gt;% gather(variable, value, defor.ha.yr, regen.ha.yr, netdefor.ha.yr, netregen.ha.yr) %&gt;% ggplot(aes(y=value, x = center, width=period-0.7)) + geom_bar(data=. %&gt;% filter(variable %in% c(&quot;defor.ha.yr&quot;, &quot;regen.ha.yr&quot;)), aes(fill=variable), stat = &quot;identity&quot;) + geom_bar(data=. %&gt;% filter(variable %in% c(&quot;netdefor.ha.yr&quot;, &quot;netregen.ha.yr&quot;)), aes(fill=variable), stat = &quot;identity&quot;) + geom_errorbar(data=my.fcc[1:5,], aes(x=center, y=NULL, ymax=defor.ha.yr, ymin=defor.ha.yr, width=period-0.7), colour=alpha(&quot;#F8766D&quot;, 1)) + geom_hline(aes(yintercept=0)) + scale_fill_manual(name = NULL, breaks = c(&quot;defor.ha.yr&quot;, &quot;netdefor.ha.yr&quot;, &quot;netregen.ha.yr&quot;, &quot;regen.ha.yr&quot;), values=c(&quot;regen.ha.yr&quot; = alpha(&quot;#00BFC4&quot;, 0.4), &quot;netregen.ha.yr&quot; = &quot;#00BFC4&quot;, &quot;netdefor.ha.yr&quot; = &quot;#F8766D&quot;, &quot;defor.ha.yr&quot; = alpha(&quot;#F8766D&quot;, 0.4)), labels=c(&quot;Perte brutte&quot;, &quot;Perte nette&quot;, &quot;Gain net&quot;, &quot;Gain brut&quot;)) + xlab(&quot;Année&quot;) + ylab(&quot;Hectares par année&quot;) + ggtitle(title) + scale_x_continuous(minor_breaks = NULL, breaks=c(1991, 2000, 2005, 2010, 2015, 2018)) + scale_y_reverse(breaks=ybreaks, minor_breaks = ymbreaks) + coord_cartesian(ylim=ylim) + theme_light() + theme(legend.position=c(0,1), legend.box = &quot;horizontal&quot;, legend.justification=c(-0.2,1.2)) ) if(!is.null(filename)) dev.off() } # DO THE WORK ############################## # Load and rename forest-cover maps ---------------------------- maps &lt;- stack(dir(paste0(FCC.CLN.DIR, &quot;/FC&quot;, COV.FC, &quot;/TGO&quot;), pattern = &quot;cf.tif&quot;, full.names = TRUE)) names(maps) &lt;- paste0(&quot;X&quot;, substr(names(maps), 5, 8)) # Create forest cover tables for different periods ------------------------------------ fc.all &lt;- fc(maps) write.csv(fc.all, paste0(FCC.RES.DIR, &quot;/TGO/TGO_fc.csv&quot;), row.names=FALSE) fc.all &lt;- read.csv(paste0(FCC.RES.DIR, &quot;/TGO/TGO_fc.csv&quot;)) fc.cln &lt;- fc.all fc.cln &lt;- fc.cln[!fc.cln$year %in% c(1987, 1991, 2000), ] fcc(fc.cln) fcc(fc.cln, c(2003, 2018)) fcc(fc.cln, c(2003, 2015, 2017, 2018)) write.xlsx(list(&quot;Total&quot; = fcc(fc.cln)), file = paste0(FCC.RES.DIR, &quot;/TGO/TGO_fcc-all-dates.xlsx&quot;)) write.xlsx(list(&quot;Total&quot; = fcc(fc.cln, c(2003, 2018))), file = paste0(FCC.RES.DIR, &quot;/TGO/TGO_fcc-03-18.xlsx&quot;)) write.xlsx(list(&quot;Total&quot; = fcc(fc.cln, c(2003, 2015, 2018))), file = paste0(FCC.RES.DIR, &quot;/TGO/TGO_fcc-03-15-18.xlsx&quot;)) plot.fc(fc.cln, &quot;TGO&quot;, paste0(FCC.RES.DIR, &quot;/TGO/TGO_fc.pdf&quot;)) plot.fcc(fc = fc.cln, zone = &quot;TGO&quot;, breaks=c(2003, 2015, 2018), filename=paste0(FCC.RES.DIR, &quot;/TGO/TGO_fcc.pdf&quot;)) registerDoParallel(.env$numCores-1) foreach(i=1:length(TGO.reg)) %do% { region &lt;- TGO.reg[i,] # fc.all &lt;- fc(maps, aoi=region) # write.csv(fc.all, paste0(RESULTS.DIR, &quot;/tables/&quot;, region$NAME_1, &quot;_fc.csv&quot;), row.names=FALSE) fc.all &lt;- read.csv(paste0(RESULTS.DIR, &quot;/tables/&quot;, region$NAME_1, &quot;_fc.csv&quot;)) fc.cln &lt;- fc.all[!fc.all$year %in% c(1987, 1991, 2000), ] plot.fc(fc.cln, region$NAME_1, paste0(RESULTS.DIR, &quot;/figures/&quot;, region$NAME_1, &quot;_fc.pdf&quot;)) plot.fcc(fc.cln, region$NAME_1, breaks=c(2003, 2015, 2018), paste0(RESULTS.DIR, &quot;/figures/&quot;, region$NAME_1, &quot;_fcc.pdf&quot;)) write.xlsx(list(&quot;Total&quot; = fcc(fc.cln)), file = paste0(RESULTS.DIR, &quot;/tables/&quot;, region$NAME_1, &quot;_fcc-all-dates.xlsx&quot;)) write.xlsx(list(&quot;Total&quot; = fcc(fc.cln, c(2003, 2018))), file = paste0(RESULTS.DIR, &quot;/tables/&quot;, region$NAME_1, &quot;_fcc-03-18.xlsx&quot;)) write.xlsx(list(&quot;Total&quot; = fcc(fc.cln, c(2003, 2015, 2018))), file = paste0(RESULTS.DIR, &quot;/tables/&quot;, region$NAME_1, &quot;_fcc-03-15-18.xlsx&quot;)) } # creating GIS layers for forest loss and forest gain per date --------------------------------------------- forest.loss &lt;- raster(maps) forest.gain &lt;- raster(maps) # create empty rasters from stack forest.pote &lt;- raster(maps) for(i in 1:(nlayers(maps)-1)) { print(paste0(&quot;Checking deforestation / reforestation in year &quot;, maps.dates[i])) forest.loss[is.na(forest.loss) &amp; is.na(forest.gain) &amp; maps[[i]] == 1 &amp; maps[[i+1]] == 3] &lt;- maps.dates[i] # take the first date of forest loss observed and only if no regeneration before that forest.gain[maps[[i]] %in% c(2,3) &amp; maps[[i+1]] == 1] &lt;- maps.dates[i] forest.pote[maps[[i]] ==3 &amp; maps[[i+1]] == 2] &lt;- maps.dates[i] } # special layer for forest gain in areas where loss has been observed before loss.gain &lt;- raster(maps) loss.gain[!is.na(forest.loss) &amp; !is.na(forest.gain) &amp; forest.gain &gt; forest.loss] &lt;- forest.gain[!is.na(forest.loss) &amp; !is.na(forest.gain) &amp; forest.gain &gt; forest.loss] forest.2018 &lt;- maps$X2018 forest.2018[forest.2018==2] &lt;- 3 writeRaster(forest.2018, paste0(RESULTS.DIR, &quot;/maps/forest-2018.tif&quot;), datatype=&quot;INT2U&quot;, overwrite=TRUE) writeRaster(forest.loss, paste0(RESULTS.DIR, &quot;/maps/forest-loss.tif&quot;), datatype=&quot;INT2U&quot;, overwrite=TRUE) writeRaster(forest.gain, paste0(RESULTS.DIR, &quot;/maps/forest-gain.tif&quot;), datatype=&quot;INT2U&quot;, overwrite=TRUE) writeRaster(forest.pote, paste0(RESULTS.DIR, &quot;/maps/forest-potential.tif&quot;), datatype=&quot;INT2U&quot;, overwrite=TRUE) writeRaster(loss.gain, paste0(RESULTS.DIR, &quot;/maps/loss-gain.tif&quot;), datatype=&quot;INT2U&quot;, overwrite=TRUE) "],
["00_analyse-AGB.html", "4.2 Analyse biomasse aérienne", " 4.2 Analyse biomasse aérienne "],
["01_compile-IFN.html", "", " 4.2.1 Analyse des données IFN Description Description de la méthodologie / script AGB/2_compile-IFN.R #################################################################### # NERF_Togo/AGB/2_compile-IFN.R: evaluating AGB of IFN plots # ------------------------------------------------------------------ # Bern University of Applied Sciences # Oliver Gardi, &lt;oliver.gardi@bfh.ch&gt; # 3 December 2019 PLOT.SIZE &lt;- 20^2*pi # Plot size in square meters RSR.Y &lt;- 0.563 # Root-shoot ratios according Mokany RSR.Y.SE &lt;- 0.086 RSR.O &lt;- 0.275 RSR.O.SE &lt;- 0.003 RSR.AGB &lt;- 20 # read inventory data # ------------------- plots &lt;- read.xlsx(paste0(DATA.DIR, &quot;/IFN/IFN-Togo-2015.xlsx&quot;), &quot;placettes&quot;)[,1:4] names(plots) &lt;- c(&quot;PlotID&quot;, &quot;X&quot;, &quot;Y&quot;, &quot;LULC&quot;) trees &lt;- read.xlsx(paste0(DATA.DIR, &quot;/IFN/IFN-Togo-2015.xlsx&quot;), &quot;arbres&quot;)[,c(1,4:6,8)] names(trees) &lt;- c(&quot;PlotID&quot;, &quot;Species&quot;, &quot;Status&quot;, &quot;DBH&quot;, &quot;H&quot;) # merge with specific wood densities used by from Fonton et al. 2018 species &lt;- as.data.frame(table(trees$Species)) names(species) &lt;- c(&quot;Species&quot;, &quot;Count&quot;) fonton &lt;- read.csv2(paste0(DATA.DIR, &quot;/IFN/donnees_Tg_Fonton.csv&quot;), encoding=&quot;latin1&quot;)[,c(4,7)] fonton &lt;- aggregate(list(D=fonton$wsg), by=list(Species=fonton$NOM), FUN=modal) species &lt;- merge(species[,c(&quot;Species&quot;, &quot;Count&quot;)], fonton[,c(&quot;Species&quot;, &quot;D&quot;)], by=&quot;Species&quot;) species$Source &lt;- &quot;Fonton&quot; # merge tree table with species densities # ------------------------------------------------------------------------- trees &lt;- merge(trees, species, by=&quot;Species&quot;, all.x=TRUE) # estimer la biomasse des arbres avec la fonction de Chave et al. (2014) # ---------------------------------------------------------------------- trees$AGB &lt;- 0.0673 * (trees$D * trees$DBH^2 * trees$H)^0.976 trees$AGBm &lt;- trees$AGBv &lt;- trees$AGB # copier les valeurs dans une colonne bois mort trees$AGBv[trees$Status != &quot;V&quot;] &lt;- 0 # mettre ?? zero la biomass l?? o?? l&#39;arbre n&#39;est pas vivant trees$AGBm[trees$Status == &quot;V&quot;] &lt;- 0 # mettre ?? zero le bois mort l?? o?? l&#39;arbre est vivant # aggregation de la biomasse est le bois mort par plots (somme) # ---------------------------------------------------------------- plots &lt;- merge(plots, aggregate(trees[,c(&quot;AGBv&quot;, &quot;AGBm&quot;)], by=list(PlotID=trees$PlotID), FUN=function(x) sum(x) * 10000 / PLOT.SIZE / 1000), by=&quot;PlotID&quot;, all.x=TRUE) # joindre tmp avec notre tableau plotss plots$AGBv[is.na(plots$AGBv)] &lt;- 0 # mettre ?? zero la biomasse et le bois mort pour les plotss sans valeurs (NA) plots$AGBm[is.na(plots$AGBm)] &lt;- 0 plots$AGB &lt;- plots$AGBv + plots$AGBm # estimer la biomasse racinaire par plot avec les facteurs root-shoot de Mokany et al. (2006) pour les for??ts tropicales s??ches # ----------------------------------------------------------------------------------------------------------------------------- plots$BGB[plots$AGBv &lt;= RSR.AGB] &lt;- plots$AGBv[plots$AGBv &lt;= RSR.AGB] * RSR.Y plots$BGB[plots$AGBv &gt; RSR.AGB] &lt;- plots$AGBv[plots$AGBv &gt; RSR.AGB] * RSR.O plots$BM &lt;- plots$AGB + plots$BGB # write plot coordinates and biomass to file # ------------------------------------------ write.csv(plots, paste0(AGB.REF.DIR, &quot;/IFN-plots.csv&quot;), row.names = FALSE) #, fileEncoding = &quot;macintosh&quot;) # produire le tableau crois?? avec les biomasses par strate (fonctionalit?? de l&#39;extension dplyr) # --------------------------------------------------------------------------------------------- pdf(paste0(AGB.REF.DIR, &quot;/AGB-vs-LULC.pdf&quot;)) par(mar=c(11,5,1,1), cex.axis=0.7) boxplot(plots$AGBv~plots$LULC, las=2, ylab=&quot;AGB (t/ha)&quot;, xlab=NULL) dev.off() pdf(paste0(AGB.REF.DIR, &quot;/AGBm-vs-LULC.pdf&quot;)) par(mar=c(11,5,1,1), cex.axis=0.7) boxplot(plots$AGBm~plots$LULC, las=2, ylab=&quot;Bmort (t/ha)&quot;, xlab=NULL) dev.off() # biomass per LU/LC category bm.lulc.tab &lt;- plots %&gt;% group_by(LULC) %&gt;% # grouper par strate summarise(n=length(AGB), AGBv.mean=mean(AGBv), AGBv.sd=sd(AGBv), # definir les colonnes et le calcul des valeurs BGB.mean=mean(BGB), BGB.sd=sd(BGB), AGBm.mean=mean(AGBm), AGBm.sd=sd(AGBm), # definir les colonnes et le calcul des valeurs BM.mean =mean(BM), BM.sd =sd(BM)) write.csv(bm.lulc.tab, paste0(AGB.REF.DIR, &quot;/AGB_LULC.csv&quot;), row.names = FALSE) #, fileEncoding = &quot;macintosh&quot;) # differences biomass per LU/LC conversion dbm.lulc.tab &lt;- bm.lulc.tab %&gt;% expand(FROM=nesting(LULC, AGB.mean, AGB.sd, BGB.mean, BGB.sd, BM.mean, BM.sd), TO=nesting(LULC, AGB.mean, AGB.sd, BGB.mean, BGB.sd, BM.mean, BM.sd)) %&gt;% mutate(from = FROM$LULC, to = TO$LULC, dAGB.mean=TO$AGB.mean - FROM$AGB.mean, dAGB.sd=sqrt(FROM$AGB.sd^2 + TO$AGB.sd^2), dBGB.mean=TO$BGB.mean - FROM$BGB.mean, dBGB.sd=sqrt(FROM$BGB.sd^2 + TO$BGB.sd^2), dBM.mean =TO$BM.mean - FROM$BM.mean, dBM.sd =sqrt(FROM$BM.sd^2 + TO$BM.sd^2)) %&gt;% select(c(from, to, dAGB.mean, dAGB.sd, dBGB.mean, dBGB.sd, dBM.mean, dBM.sd)) write.csv(dbm.lulc.tab, paste0(AGB.REF.DIR, &quot;/AGB_LULC-diff.csv&quot;), row.names = FALSE) #, fileEncoding = &quot;macintosh&quot;) "],
["02_create-AGB-maps.html", "", " 4.2.2 Calibration et prédiction Description Description de la méthodologie / script AGB/3_create-AGB-maps.R #################################################################### # NERF_Togo/AGB/3_create-agb-maps.R: create AGB maps for different dates # ------------------------------------------------------------------ # Bern University of Applied Sciences # Oliver Gardi, &lt;oliver.gardi@bfh.ch&gt; # 10 October 2019 ### DEFINITIONS ############################################################ # Default parameters ------------------------------------------------------- AGB.STRATA &lt;- 10 # Number of strata for sampling AGB from reference maps / calibration maps N.PIXELS &lt;- NA # Number of non-NA cells (will be determined later) SAMPLE.RATIO &lt;- 0.001 # Share of non-NA cells to sample CAL.RATIO &lt;- 1 # Use same amount of ref-points from cal.map as from ref.map / train.points PREDICTORS &lt;- c(&quot;B&quot;, &quot;G&quot;, &quot;R&quot;, &quot;NIR&quot;, &quot;SWIR1&quot;, &quot;SWIR2&quot;, &quot;nbr&quot;, &quot;ndmi&quot;, &quot;ndvi&quot;, &quot;evi&quot;, &quot;BIO1&quot;, &quot;BIO4&quot;, &quot;BIO12&quot;, &quot;BIO15&quot;) # &quot;savi&quot;, &quot;nbr2&quot;, &quot;msavi&quot;, &quot;x&quot;, &quot;y&quot; SEED &lt;- 20191114 # Function for loading an image -------------------------------------------- load.image &lt;- function(filename) { image &lt;- brick(paste0(IMAGES.DIR, filename)) names(image) &lt;- BANDS return(image) } # function for creating biomass map based on image and training data ------------ agb.map &lt;- function(image, filename, bioclim=NULL, train.dat=NULL, ref.map=NULL, n.ref.map=NULL, cal.map=NULL, n.cal.map=NULL, mask=NULL, preds=NULL, crossval=FALSE, bias.corr=TRUE, n.cores=8) { name &lt;- sub(&quot;[.]tif$&quot;, &quot;&quot;, filename) txtfile &lt;- paste0(sub(&quot;[.]tif$&quot;, &quot;&quot;, filename), &quot;.txt&quot;) cat(&quot;-- Biomass map: &quot;, basename(filename), &quot;/&quot;, date(), &quot; --\\n&quot;, file=txtfile) # add training data from ref.map if provided if(!is.null(ref.map)) { cat(paste0(&quot; -Masking / buffering reference map ... \\n&quot;)) ref.map &lt;- mask(crop(ref.map, image[[1]]), crop(image[[1]], ref.map)) # crop/mask ref.map with image if(!is.null(mask)) ref.map &lt;- mask(ref.map, mask) # mask with additional mask, if provided if(!is.null(cal.map)) { tmp &lt;- extend(crop(cal.map, ref.map), ref.map) # cut out the piece of the calibration map that overlaps ref map and extend to refmap ref.map &lt;- mask(ref.map, tmp, inverse=TRUE) } cat(&quot; &quot;) cat(paste0(&quot; -Sampling map (n=&quot;, AGB.STRATA, &quot;*&quot;, round(n.ref.map/AGB.STRATA), &quot;) ... &quot;)) ref.pts &lt;- sampleStratified(cut(ref.map, AGB.STRATA), round(n.ref.map/AGB.STRATA), sp=TRUE)[,-1] # stratified sampling (same number of samples for each class) names(ref.pts) &lt;- &quot;AGB&quot; cat(&quot;extracting values ... \\n&quot;) ref.dat &lt;- cbind(AGB=raster::extract(ref.map, ref.pts, df=TRUE)[,-1], raster::extract(image, ref.pts, df=TRUE)[,-1], raster::extract(bioclim, ref.pts, df=TRUE)[,-1]) cat(&quot;Ref-map points: &quot;, nrow(ref.dat), &quot;/&quot;, ref.map@file@name, file=txtfile, append=TRUE) if(is.null(train.dat)) { train.dat &lt;- ref.dat # use it as training points or add to existing training points } else { train.dat &lt;- rbind(train.dat, ref.dat) } } if(!is.null(cal.map)) { cat(paste0(&quot; -Masking calibration map ... \\n&quot;)) cal.map &lt;- mask(crop(cal.map, image[[1]]), crop(image[[1]], cal.map)) # crop/mask ref.map with image if(!is.null(mask)) cal.map &lt;- mask(cal.map, mask) # mask with additional mask, if provided cat(paste0(&quot; -Sampling map (n=&quot;, AGB.STRATA, &quot;*&quot;, round(n.cal.map/AGB.STRATA), &quot;) ... &quot;)) cal.pts &lt;- sampleStratified(cut(cal.map, AGB.STRATA), round(n.cal.map/AGB.STRATA), sp=TRUE)[,-1] # stratified sampling (same number of samples for each class) names(cal.pts) &lt;- &quot;AGB&quot; cat(&quot;extracting values ... \\n&quot;) cal.dat &lt;- cbind(AGB=raster::extract(cal.map, cal.pts, df=TRUE)[,-1], raster::extract(image, cal.pts, df=TRUE)[,-1], raster::extract(bioclim, cal.pts, df=TRUE)[,-1]) if(is.null(train.dat)) { train.dat &lt;- cal.dat # use it as training points or add to existing training points } else { train.dat &lt;- rbind(train.dat, cal.dat) } cat(&quot;Cal-map points: &quot;, nrow(cal.dat), &quot;from&quot;, cal.map@file@name, file=txtfile, append=TRUE) } cat(&quot;Total points: &quot;, nrow(train.dat), &quot;\\n&quot;, file=txtfile, append=TRUE) # extract spectral values if(is.null(preds)) { preds &lt;- names(image) if(!is.null(bioclim)) preds &lt;- c(preds, names(bioclim)) } # cat(&quot; -Extracting pixel values for bands:&quot;, preds, &quot;... &quot;) # train.pts &lt;- raster::extract(image, train.pts, sp=TRUE) # if(!is.null(bioclim)) train.pts &lt;- raster::extract(bioclim, train.pts, sp=TRUE) # train.dat &lt;- na.omit(train.pts@data)[, c(&quot;CLASS&quot;, preds)] # if(type==&quot;classification&quot;) train.dat[,1] &lt;- as.factor(train.dat[,1]) # cat(&quot;done\\n&quot;) # calibrate RandomForest classifier cat(&quot; -Calibrating RandomForest ... &quot;) sink(txtfile, append=TRUE) if(crossval) { map.model.cv &lt;- train(y = train.dat[,1], x = train.dat[,preds], method = &quot;rf&quot;, importance = TRUE, trControl = trainControl( method = &quot;repeatedcv&quot;, number = 10, repeats = 3)) print(map.model.cv) map.model &lt;- map.model.cv$finalModel print(map.model) cat(&quot;\\n&quot;) print(varImp(map.model, scale=FALSE)) } else { map.model &lt;- randomForest(y=train.dat[,1], x=train.dat[,preds], importance=TRUE) # , do.trace=100) # Parallelization of RandomForest: confusion, err.rate, mse and rsq will be NULL # https://stackoverflow.com/questions/14106010/parallel-execution-of-random-forest-in-r # map.model &lt;- foreach(ntree=rep(100, 5), .combine=randomForest::combine, .multicombine=TRUE, .packages=&#39;randomForest&#39;) %dopar% { # randomForest(x=ref.pts[,!(names(ref.pts) == &quot;CLASS&quot;)], y=ref.pts$CLASS, importance=TRUE, ntree=ntree) # print(map.model) cat(&quot;\\n&quot;) print(varImp(map.model)) } sink() cat(&quot;R2:&quot;, round(map.model$rsq[500], 2), &quot;RMSE:&quot;, round(sqrt(map.model$mse[500]), 2), &quot;\\n&quot;) # write model results dir.create(dirname(filename), recursive=TRUE, showWarnings=FALSE) # save RandomForest Model (too large) # save(map.model, file=paste0(sub(&quot;[.]tif$&quot;, &quot;&quot;, filename), &quot;r_rf.RData&quot;)) # classify image cat(&quot; -Creating map ... &quot;) if(!is.null(bioclim)) image &lt;- stack(image, crop(bioclim, image)) beginCluster(n=n.cores) map &lt;- clusterR(image, predict, args=list(model=map.model)) endCluster() pdf(paste0(name, &quot;.pdf&quot;)) plot(train.dat$AGB ~ map.model$predicted, ylab=&quot;AGB (tDM/ha)&quot;, xlab=&quot;Predicted AGB (tDM/ha)&quot;) abline(0,1, lty=2) if(bias.corr) { bc &lt;- lm(train.dat$AGB ~ map.model$predicted) abline(bc, lty=3, col=&quot;red&quot;) } else { bc &lt;- NULL } dev.off() if(bias.corr) { sink(paste0(name, &quot;_bc.txt&quot;), split=TRUE) print(summary(bc)) sink() save(bc, file=paste0(name, &quot;_bc.RData&quot;)) cat(&quot;Applying linear bias correction ...&quot;) map &lt;- calc(map, fun=function(x){bc$coefficients[1] + bc$coefficients[2]*x}) } # save map of classified image cat(&quot;writing map ... &quot;) map &lt;- writeRaster(map, filename=filename, format=&quot;GTiff&quot;, datatype=&quot;INT2U&quot;, overwrite=TRUE) cat(&quot;done\\n&quot;) cat(&quot;-- Done: &quot;, basename(filename), &quot;/&quot;, date(), &quot; --\\n&quot;, file=txtfile, append=TRUE) invisible(list( &quot;rf.model&quot; = map.model, &quot;bc.model&quot; = bc, &quot;map&quot; = map )) } ### DO THE WORK ############################################################ # read images 2015 and bioclim variables --------------------------- ref.p192 &lt;- brick(paste0(IMAGES.DIR, &quot;/p192/p192_2015_m.tif&quot;)) ref.p193 &lt;- brick(paste0(IMAGES.DIR, &quot;/p193/p193_2015_m.tif&quot;)) ref.p194 &lt;- brick(paste0(IMAGES.DIR, &quot;/p194/p194_2015_m.tif&quot;)) names(ref.p192) &lt;- names(ref.p193) &lt;- names(ref.p194) &lt;- BANDS ref.images &lt;- list(p192=ref.p192, p193=ref.p193, p194=ref.p194) N.PIXELS &lt;- list(p192 = ncell(ref.p192[[&quot;B&quot;]]) - summary(ref.p192)[&quot;NA&#39;s&quot;,&quot;B&quot;], p193 = ncell(ref.p193[[&quot;B&quot;]]) - summary(ref.p193)[&quot;NA&#39;s&quot;,&quot;B&quot;], p194 = ncell(ref.p194[[&quot;B&quot;]]) - summary(ref.p194)[&quot;NA&#39;s&quot;,&quot;B&quot;]) bioclim.p192 &lt;- brick(paste0(IMAGES.DIR, &quot;/p192/p192_bioclim.tif&quot;)) bioclim.p193 &lt;- brick(paste0(IMAGES.DIR, &quot;/p193/p193_bioclim.tif&quot;)) bioclim.p194 &lt;- brick(paste0(IMAGES.DIR, &quot;/p194/p194_bioclim.tif&quot;)) names(bioclim.p192) &lt;- names(bioclim.p193) &lt;- names(bioclim.p194) &lt;- BIOCLIM bioclim &lt;- list(p192=bioclim.p192, p193=bioclim.p193, p194=bioclim.p194) # # Load 30m DEM and calculate slope and aspect # dem &lt;- stack(raster(paste0(OUTPUT.DIR, &quot;/1_images/SRTM/SRTM-1arcsec.tif&quot;))) # names(dem) &lt;- &quot;ALT&quot; # dem$SLP &lt;- terrain(dem$ALT, opt=&quot;slope&quot;) # dem$ASP &lt;- terrain(dem$ALT, opt=&quot;aspect&quot;) # # # wc2 &lt;- stack(paste0(OUTPUT.DIR, &quot;/1_images/WCv2/wc2.0_30s_tmin_Togo.tif&quot;), # paste0(OUTPUT.DIR, &quot;/1_images/WCv2/wc2.0_30s_tmax_Togo.tif&quot;), # paste0(OUTPUT.DIR, &quot;/1_images/WCv2/wc2.0_30s_prec_Togo.tif&quot;), # paste0(OUTPUT.DIR, &quot;/1_images/WCv2/wc2.0_30s_bio_Togo.tif&quot;)) # # names(wc2) &lt;- c(paste0(&quot;tmin.&quot;, 1:12), paste0(&quot;tmax.&quot;, 1:12), paste0(&quot;prec.&quot;, 1:12), paste0(&quot;bio.&quot;, 1:19)) # load inventory data ------------------------------------------------------ plots &lt;- read.csv(paste0(AGB.REF.DIR, &quot;/IFN-plots.csv&quot;)) # , fileEncoding=&quot;macintosh&quot;) coordinates(plots) &lt;- ~X+Y proj4string(plots) &lt;- utm.31 pdf(paste0(AGB.REF.DIR, &quot;/IFN-plots_location.pdf&quot;), width=3.5, height=7) par(mar=c(1,1,1,1)) plot(spTransform(TGO, utm.31), col=&quot;lightyellow&quot;) plot(plots, add=TRUE, col=&quot;black&quot;, pch=16, cex=0.3) plot(plots, add=TRUE, col=&quot;darkgreen&quot;, pch=1, cex=plots$AGB/100) dev.off() # convert points to polygons, for extraction of raster values plots.poly &lt;- SpatialPolygonsDataFrame(gBuffer(plots, byid=TRUE, width=20), plots@data) # extract raster values registerDoParallel(.env$numCores-1) x &lt;- foreach(i=1:length(ref.images), .combine=cbind) %:% foreach(j=1:nlayers(ref.images[[i]]), .combine=cbind) %dopar% { raster::extract(ref.images[[i]][[j]], plots.poly, weights=TRUE, fun=mean, df=TRUE)[,2] # weighted means for spectral values } x2 &lt;- foreach(i=1:length(bioclim), .combine=cbind) %:% foreach(j=1:nlayers(bioclim[[i]]), .combine=cbind) %dopar% { raster::extract(bioclim[[i]][[j]], plots, df=TRUE)[,2] # bioclimatic values } dat.p192 &lt;- na.omit(as.data.frame(cbind(plots$AGB, x[,1:13], x2[,1:19]))) dat.p193 &lt;- na.omit(as.data.frame(cbind(plots$AGB, x[,14:26], x2[,20:38]))) dat.p194 &lt;- na.omit(as.data.frame(cbind(plots$AGB, x[,27:39], x2[,39:57]))) names(dat.p192) &lt;- names(dat.p193) &lt;- names(dat.p194) &lt;- c(&quot;AGB&quot;, BANDS, BIOCLIM) train.data &lt;- list(p192=dat.p192, p193=dat.p193, p194=dat.p194) # variable selection ----------------------------- # train.data &lt;- na.omit(x[,c(1, (2*13-13)+2:14, 41:98)]) # use &#39;2&#39; for selecting p193 # # # determine explicative variables # # # TODO: include regeneration map as predictor!! # # var.sel &lt;- rfe(train.data[,2:ncol(train.data)], train.data[,1], # sizes=c(1:15, 20, 50), #sizes=seq(8,26,2), # # rfeControl=rfeControl( # functions=rfFuncs, # method = &quot;repeatedcv&quot;, # number = 10, # repeats = 10 # )) # # sink(paste0(OUTPUT.DIR, &quot;/3_forest-biomass/2_ref-maps/p193_2015_AGB_varsel-fin.txt&quot;), split=TRUE) # predictors(var.sel) # print(var.sel) # sink() # # pdf(paste0(OUTPUT.DIR, &quot;/3_forest-biomass/2_ref-maps/p193_2015_AGB_varsel-fin.pdf&quot;)) # plot(var.sel, type=c(&quot;g&quot;, &quot;o&quot;)) # dev.off() # # # train.data &lt;- train.data[, c(&quot;AGB&quot;, predictors(var.sel))] # # # use Landsat bands + ndvi + annual mean temp, temp seasonality, annual prec and prec seasonality # train.data &lt;- train.data[, c(&quot;AGB&quot;, # &quot;B&quot;, &quot;G&quot;, &quot;R&quot;, &quot;NIR&quot;, &quot;SWIR1&quot;, &quot;SWIR2&quot;, &quot;ndvi&quot;, # paste0(&quot;bio.&quot;, c(1, 4, 12, 15)) # )] # create reference biomass maps 2015 ------------------------------ set.seed(SEED) p193.2015.agb &lt;- agb.map(image = load.image(&quot;/p193/p193_2015_m.tif&quot;), bioclim = bioclim[[&quot;p193&quot;]], filename = paste0(AGB.REF.DIR, &quot;/p193_2015_AGB_R.tif&quot;), train.dat = train.data[[&quot;p193&quot;]], preds = PREDICTORS, crossval = TRUE, bias.corr = FALSE, n.cores = 32) set.seed(SEED) p192.2015.agb &lt;- agb.map(image = load.image(&quot;/p192/p192_2015_m.tif&quot;), bioclim = bioclim[[&quot;p192&quot;]], filename = paste0(AGB.REF.DIR, &quot;/p192_2015_AGB_R.tif&quot;), train.dat = train.data[[&quot;p192&quot;]], cal.map = raster(paste0(AGB.REF.DIR, &quot;/p193_2015_AGB_R.tif&quot;)), n.cal.map = max(200, nrow(train.data[[&quot;p192&quot;]])*CAL.RATIO), preds = PREDICTORS, crossval = FALSE, bias.corr = FALSE, n.cores = 32, mask = TGO) set.seed(SEED) p194.2015.agb &lt;- agb.map(image = load.image(&quot;/p194/p194_2015_m.tif&quot;), bioclim = bioclim[[&quot;p194&quot;]], filename = paste0(AGB.REF.DIR, &quot;/p194_2015_AGB_R.tif&quot;), train.dat = train.data[[&quot;p194&quot;]], cal.map = raster(paste0(AGB.REF.DIR, &quot;/p193_2015_AGB_R.tif&quot;)), n.cal.map = max(200, nrow(train.data[[&quot;p194&quot;]])*CAL.RATIO), preds = PREDICTORS, crossval = FALSE, bias.corr = FALSE, n.cores = 32, mask = TGO) # merge the three maps agb.2015 &lt;- mask(crop(mosaic(raster(paste0(AGB.REF.DIR, &quot;/p192_2015_AGB_R.tif&quot;)), raster(paste0(AGB.REF.DIR, &quot;/p193_2015_AGB_R.tif&quot;)), raster(paste0(AGB.REF.DIR, &quot;/p194_2015_AGB_R.tif&quot;)), fun=mean), TGO), TGO, filename=paste0(AGB.REF.DIR, &quot;/TGO_2015_AGB_R.tif&quot;), overwrite=TRUE) # plot biomass map library(RColorBrewer) pdf(paste0(AGB.REF.DIR, &quot;/TGO_2015_AGB_R.pdf&quot;), width=3.5, height=7) par(mar=c(1,1,1,1)) plot(agb.2015, axes=FALSE, col=brewer.pal(9, &quot;YlGn&quot;), zlim=c(0,220)) plot(spTransform(TGO, utm.31), add=TRUE) dev.off() # compare resulting biomass map with the IFN AGB values agb.pred &lt;- raster::extract(agb.2015, plots.poly, weights=TRUE, fun=mean, df=TRUE)[,2] # weighted means for spectral values pdf(paste0(AGB.REF.DIR, &quot;/AGB-model_2015.pdf&quot;)) plot(agb.pred ~ plots.poly$AGB, xlab=&quot;Biomasse aérienne IFN (t/ha)&quot;, ylab=&quot;Carte AGB 2015 (t/ha)&quot;, xlim=c(0,350), ylim=c(0,350)) abline(0, 1, lty=&quot;dashed&quot;) dev.off() plots.poly$AGB.pred &lt;- agb.pred # bias correction train.control &lt;- trainControl(method = &quot;cv&quot;, number = 10) cv &lt;- train(AGB ~ AGB.pred, data=plots.poly@data[!is.na(plots.poly$AGB.pred),], method = &quot;lm&quot;, trControl = train.control) model &lt;- lm(AGB ~ AGB.pred, data=plots.poly@data[!is.na(plots.poly$AGB.pred),]) sink(paste0(AGB.REF.DIR, &quot;/TGO_2015_AGB_Rc.txt&quot;), split=TRUE) print(cv) summary(model) sink() agb.2015 &lt;- writeRaster(model$coefficients[&quot;(Intercept)&quot;] + model$coefficients[&quot;AGB.pred&quot;] * agb.2015, paste0(AGB.REF.DIR, &quot;/TGO_2015_AGB_R.tif&quot;), overwrite=TRUE) # compare bias corrected biomass map with the IFN AGB values agb.pred.c &lt;- raster::extract(agb.2015, plots.poly, weights=TRUE, fun=mean, df=TRUE)[,2] # weighted means for spectral values pdf(paste0(AGB.REF.DIR, &quot;/AGB-model_2015_c.pdf&quot;)) plot(agb.pred ~ plots.poly$AGB, xlab=&quot;Biomasse aérienne IFN (t/ha)&quot;, ylab=&quot;Carte AGB 2015 (t/ha)&quot;, xlim=c(0,350), ylim=c(0,350)) abline(0, 1, lty=&quot;dashed&quot;) dev.off() # create reference biomass maps 2003 ------------------------------ set.seed(SEED) p193.2003.agb &lt;- agb.map(image = load.image(&quot;/p193/p193_2003_m.tif&quot;), bioclim = bioclim[[&quot;p193&quot;]], filename = paste0(AGB.REF.DIR, &quot;/p193_2003_AGB_R.tif&quot;), train.dat = NULL, ref.map = raster(paste0(AGB.REF.DIR, &quot;/TGO_2015_AGB_R.tif&quot;)), n.ref.map = SAMPLE.RATIO * N.PIXELS[[&quot;p193&quot;]], preds = PREDICTORS, crossval = FALSE, bias.corr = TRUE, n.cores = 32) set.seed(SEED) p192.2003.agb &lt;- agb.map(image = load.image(&quot;/p192/p192_2003_m.tif&quot;), bioclim = bioclim[[&quot;p192&quot;]], filename = paste0(AGB.REF.DIR, &quot;/p192_2003_AGB_R.tif&quot;), train.dat = NULL, ref.map = raster(paste0(AGB.REF.DIR, &quot;/TGO_2015_AGB_R.tif&quot;)), n.ref.map = SAMPLE.RATIO * N.PIXELS[[&quot;p192&quot;]] / (1 + CAL.RATIO), cal.map = raster(paste0(AGB.REF.DIR, &quot;/p193_2003_AGB_R.tif&quot;)), n.cal.map = SAMPLE.RATIO * N.PIXELS[[&quot;p192&quot;]] / (1 + 1/CAL.RATIO), preds = PREDICTORS, crossval = FALSE, bias.corr = FALSE, n.cores = 32, mask = TGO) set.seed(SEED) p194.2003.agb &lt;- agb.map(image = load.image(&quot;/p194/p194_2003_m.tif&quot;), bioclim = bioclim[[&quot;p194&quot;]], filename = paste0(AGB.REF.DIR, &quot;/p194_2003_AGB_R.tif&quot;), train.dat = NULL, ref.map = raster(paste0(AGB.REF.DIR, &quot;/TGO_2015_AGB_R.tif&quot;)), n.ref.map = SAMPLE.RATIO * N.PIXELS[[&quot;p194&quot;]] / (1 + CAL.RATIO), cal.map = raster(paste0(AGB.REF.DIR, &quot;/p193_2003_AGB_R.tif&quot;)), n.cal.map = SAMPLE.RATIO * N.PIXELS[[&quot;p194&quot;]] / (1 + 1/CAL.RATIO), preds = PREDICTORS, crossval = FALSE, bias.corr = FALSE, n.cores = 32, mask = TGO) # merge the three maps agb.2003 &lt;- mask(crop(mosaic(raster(paste0(AGB.REF.DIR, &quot;/p192_2003_AGB_R.tif&quot;)), raster(paste0(AGB.REF.DIR, &quot;/p193_2003_AGB_R.tif&quot;)), raster(paste0(AGB.REF.DIR, &quot;/p194_2003_AGB_R.tif&quot;)), fun=mean), TGO), TGO, filename=paste0(AGB.REF.DIR, &quot;/TGO_2003_AGB_R.tif&quot;), overwrite=TRUE) # create reference biomass maps 2018 ------------------------------ set.seed(SEED) p193.2018.agb &lt;- agb.map(image = load.image(&quot;/p193/p193_2018_m.tif&quot;), bioclim = bioclim[[&quot;p193&quot;]], filename = paste0(AGB.REF.DIR, &quot;/p193_2018_AGB_R.tif&quot;), train.dat = NULL, ref.map = raster(paste0(AGB.REF.DIR, &quot;/TGO_2015_AGB_R.tif&quot;)), n.ref.map = SAMPLE.RATIO * N.PIXELS[[&quot;p193&quot;]], preds = PREDICTORS, crossval = FALSE, bias.corr = TRUE, n.cores = 32) set.seed(SEED) p192.2018.agb &lt;- agb.map(image = load.image(&quot;/p192/p192_2018_m.tif&quot;), bioclim = bioclim[[&quot;p192&quot;]], filename = paste0(AGB.REF.DIR, &quot;/p192_2018_AGB_R.tif&quot;), train.dat = NULL, ref.map = raster(paste0(AGB.REF.DIR, &quot;/TGO_2015_AGB_R.tif&quot;)), n.ref.map = SAMPLE.RATIO * N.PIXELS[[&quot;p192&quot;]] / (1 + CAL.RATIO), cal.map = raster(paste0(AGB.REF.DIR, &quot;/p193_2018_AGB_R.tif&quot;)), n.cal.map = SAMPLE.RATIO * N.PIXELS[[&quot;p192&quot;]] / (1 + 1/CAL.RATIO), preds = PREDICTORS, crossval = FALSE, bias.corr = FALSE, n.cores = 32, mask = TGO) set.seed(SEED) p194.2018.agb &lt;- agb.map(image = load.image(&quot;/p194/p194_2018_m.tif&quot;), bioclim = bioclim[[&quot;p194&quot;]], filename = paste0(AGB.REF.DIR, &quot;/p194_2018_AGB_R.tif&quot;), train.dat = NULL, ref.map = raster(paste0(AGB.REF.DIR, &quot;/TGO_2015_AGB_R.tif&quot;)), n.ref.map = SAMPLE.RATIO * N.PIXELS[[&quot;p194&quot;]] / (1 + CAL.RATIO), cal.map = raster(paste0(AGB.REF.DIR, &quot;/p193_2018_AGB_R.tif&quot;)), n.cal.map = SAMPLE.RATIO * N.PIXELS[[&quot;p194&quot;]] / (1 + 1/CAL.RATIO), preds = PREDICTORS, crossval = FALSE, bias.corr = FALSE, n.cores = 32, mask = TGO) # merge the three maps agb.2018 &lt;- mask(crop(mosaic(raster(paste0(AGB.REF.DIR, &quot;/p192_2018_AGB_R.tif&quot;)), raster(paste0(AGB.REF.DIR, &quot;/p193_2018_AGB_R.tif&quot;)), raster(paste0(AGB.REF.DIR, &quot;/p194_2018_AGB_R.tif&quot;)), fun=mean), TGO), TGO, filename=paste0(AGB.REF.DIR, &quot;/TGO_2018_AGB_R.tif&quot;), overwrite=TRUE) # # biomass maps for p193 -------------------------------------- # # registerDoParallel(.env$numCores-1) # foreach(file=dir(paste0(IMAGES.DIR, &quot;/p193&quot;), pattern=&quot;\\\\_[[:digit:]]+\\\\_m\\\\.tif&quot;)) %dopar% { # #foreach(file=c(&quot;p193_1987.tif&quot;, &quot;p193_2003.tif&quot;, &quot;p193_2015.tif&quot;, &quot;p193_2018.tif&quot;)) %dopar% { # # foreach(file=c(&quot;p193_1985.tif&quot;, &quot;p193_1990_1.tif&quot;, &quot;p193_1990_2.tif&quot;, &quot;p193_2000.tif&quot;, &quot;p193_2005.tif&quot;, &quot;p193_2007.tif&quot;, &quot;p193_2009.tif&quot;, &quot;p193_2013.tif&quot;, &quot;p193_2017.tif&quot;, &quot;p193_2019.tif&quot;)) %dopar% { # # agb.map(image = load.image(paste0(&quot;/p193/&quot;, file)), # bioclim = bioclim[[&quot;p193&quot;]], # filename = paste0(BIOMASS.DIR, &quot;/3_raw-maps/p193/&quot;, sub(&quot;\\\\.tif$&quot;, &quot;r.tif&quot;, file)), # ref.map = raster(paste0(BIOMASS.DIR, &quot;/2_ref-maps/p193_2003_AGB_R.tif&quot;)), # n.ref.map = SAMPLE.RATIO * N.PIXELS[[&quot;p193&quot;]], # preds = PREDICTORS, # mask = TGO, # n.cores = 6) # # n.cores = 32) # } # # # merge the two p193_1990 tiles # merge(raster(paste0(BIOMASS.DIR, &quot;/3_raw-maps/p193/p193_1990_1_mr.tif&quot;)), # raster(paste0(BIOMASS.DIR, &quot;/3_raw-maps/p193/p193_1990_2_mr.tif&quot;)), # filename=paste0(BIOMASS.DIR, &quot;/3_raw-maps/p193/p193_1990_mr.tif&quot;), format=&quot;GTiff&quot;, datatype=&quot;INT2U&quot;, overwrite=TRUE) # # # # biomass maps for p192 and p194 ---------------------- # # registerDoParallel(.env$numCores-1) # foreach(file=c(dir(paste0(IMAGES.DIR, &quot;/p192&quot;), pattern=&quot;\\\\_[[:digit:]]+\\\\_m\\\\.tif&quot;), # dir(paste0(IMAGES.DIR, &quot;/p194&quot;), pattern=&quot;\\\\_[[:digit:]]+\\\\_m\\\\.tif&quot;))) %dopar% { # # path &lt;- sub(&quot;\\\\_.*&quot;, &quot;&quot;, file) # # if(file.exists(paste0(BIOMASS.DIR, &quot;/3_raw-maps/p193/&quot;, sub(&quot;\\\\.tif$&quot;, &quot;r.tif&quot;, sub(path, &quot;p193&quot;, file))))) { # cal.map &lt;- raster(paste0(BIOMASS.DIR, &quot;/3_raw-maps/p193/&quot;, sub(&quot;\\\\.tif$&quot;, &quot;r.tif&quot;, sub(path, &quot;p193&quot;, file)))) # n.cal.map &lt;- SAMPLE.RATIO * N.PIXELS[[path]] / (1 + 1/CAL.RATIO) # n.ref.map &lt;- SAMPLE.RATIO * N.PIXELS[[path]] / (1 + CAL.RATIO) # } else { # cal.map &lt;- NULL # n.cal.map &lt;- NULL # n.ref.map &lt;- SAMPLE.RATIO * N.PIXELS[[path]] # } # # agb.map(image = load.image(paste0(&quot;/&quot;, path, &quot;/&quot;, file)), # bioclim = bioclim[[path]], # filename = paste0(BIOMASS.DIR, &quot;/3_raw-maps/&quot;, path, &quot;/&quot;, sub(&quot;\\\\.tif$&quot;, &quot;r.tif&quot;, file)), # ref.map = raster(paste0(BIOMASS.DIR, &quot;/2_ref-maps/&quot;, path, &quot;_2003_AGB_R.tif&quot;)), # n.ref.map = n.ref.map, # cal.map = cal.map, # n.cal.map = n.cal.map, # preds = PREDICTORS, # mask = TGO, # n.cores = 32) # } "],
["03_clean-AGB-maps.html", "", " 4.2.3 Nettoyage des cartes brutes Description Description de la méthodologie / script AGB/4_clean-AGB-maps.R #################################################################### # NERF_Togo/AGB/4_clean-agb-maps.R: clean AGB raw maps # ------------------------------------------------------------------ # Bern University of Applied Sciences # Oliver Gardi, &lt;oliver.gardi@bfh.ch&gt; # 10 October 2019 ### DEFINITIONS ############################################################ PERIOD &lt;- 1985:2019 path &lt;- &quot;p193&quot; # LOESS smoothing of time series loess.filter &lt;- function (agb, years, pred, span) { ifelse( all(is.na(agb)), return(rep(NA, length(pred))), return(predict(loess(agb ~ years, degree = 1, span = span), pred) )) } clean.agb &lt;- function(path) { # Preparation ------------------------------------------------------------- maps &lt;- stack(dir(paste0(BIOMASS.DIR, &quot;/3_raw-maps/&quot;, path), pattern=&quot;.*[[:digit:]]{4}\\\\_mr\\\\.tif$&quot;, full.names=TRUE)) map.names &lt;- sub(&quot;r$&quot;, &quot;&quot;, names(maps)) map.cols &lt;- sub(paste0(path, &quot;\\\\_&quot;), &quot;X&quot;, sub(&quot;\\\\_[[:alnum:]]+$&quot;, &quot;M&quot;, map.names)) map.years &lt;- as.numeric(substr(map.cols, 2, 5)) maps.values &lt;- values(maps) # extract vector with cell values (huge matrix, takes some time) colnames(maps.values) &lt;- map.cols nsubsets &lt;- numCores - 1 # define subsets for parallel processing subsets &lt;- c(0, floor((1:nsubsets)*(nrow(maps.values)/nsubsets))) # Parallel cleaning of pixel trajectories ############################### registerDoParallel(.env$numCores-1) loess.dat &lt;- foreach(i=1:nsubsets, .combine=rbind) %dopar% { val &lt;- maps.values[(subsets[i]+1):subsets[i+1], ] # get one tile of the matrix t(apply(val, 1, loess.filter, years=map.years, pred=map.years, span=2)) } maps.loess &lt;- maps maps.loess[] &lt;- loess.dat writeRaster(maps.loess, paste0(BIOMASS.DIR, &quot;/4_clean-maps/&quot;, path, &quot;_loess.tif&quot;), overwrite=TRUE) } ################################ dates &lt;- c(1987, 1991, 2000, 2003, 2005, 2007, 2008, 2009, 2010, 2012, 2013, 2015, 2017, 2018, 2019) for(year in dates) { print(paste(&quot;Creating AGB map&quot;, year, &quot;...&quot;)) image &lt;- dir(path=&quot;../results/images/&quot;, pattern=paste0(&quot;^&quot;, year, &quot;.*_L2.tif$&quot;), full.names=TRUE) base &lt;- paste0(&quot;../results/carbone/AGB_&quot;, year, &quot;_2015&quot;) biomass.map &lt;- create.biomass.map(ref = raster(&quot;../results/carbone/AGB_2015_REF_corr.tif&quot;), x = brick(image)) writeRaster(biomass.map$map, filename = paste0(base, &quot;.tif&quot;), overwrite=TRUE) writeRaster(biomass.map$map.bias.corr, filename = paste0(base, &quot;_corr.tif&quot;), overwrite=TRUE) sink( file = paste0(base, &quot;_models.txt&quot;)) print(biomass.map$rf); cat(&quot;\\n&quot;) print(importance(biomass.map$rf)); cat(&quot;\\n&quot;) print(summary(biomass.map$lm.bias.corr)) sink() } # Show the effect of bias correction ggplot(biomass.map$sample.pts@data, aes(y=ref.agb, x=agb)) + geom_point(alpha=0.01) + geom_abline(slope=1, linetype=3, alpha=0.9) + geom_abline(intercept = biomass.map$lm.bias.corr$coefficients[1], slope = biomass.map$lm.bias.corr$coefficients[2], linetype=2, alpha=0.9) + theme_light() # load the raw carbon maps and do some analysis # ≠≠≠≠≠≠≠≠≠≠≠≠≠≠≠≠≠≠≠≠≠≠≠≠≠≠≠≠≠≠≠≠≠≠≠≠≠≠≠≠≠≠≠≠≠ files &lt;- dir(path=&quot;../results/carbone&quot;, pattern=&quot;^.*2015.tif$&quot;, full.names=TRUE) maps &lt;- stack(files) names(maps) &lt;- dates maps.dat &lt;- as.data.frame(na.omit(maps[])) dens.plot &lt;- maps.dat %&gt;% gather(variable, value, X1987, X1991, X2000, X2003, X2008, X2013, X2015, X2018) %&gt;% ggplot(aes(x=value, color=variable, linetype=variable)) + geom_density() + theme_light() print(dens.plot) pdf(&quot;../results/carbone/densities_all.pdf&quot;) print(dens.plot) dev.off() # Validation of loess-smoothed 2015 map.2015 &lt;- stack(&quot;../results/carbone/level2/AGBmaps_loess.tif&quot;)[[12]] map.2003 &lt;- stack(&quot;../results/carbone/level2/AGBmaps_loess.tif&quot;)[[4]] plots &lt;- raster::extract(map.2015, plots, sp=TRUE, fun=mean, buffer=20, normalizeWeights=TRUE) plots &lt;- raster::extract(map.2003, plots, sp=TRUE, fun=mean, buffer=20, normalizeWeights=TRUE) names(plots) tmp &lt;- plots@data[,c(&quot;BaHa&quot;, &quot;AGBmaps_loess.12&quot;, &quot;AGBmaps_loess.4&quot;)] tmp$cex &lt;- 1+abs(tmp$AGBmaps_loess.12-tmp$AGBmaps_loess.4)/30 tmp$col[tmp$AGBmaps_loess.12-tmp$AGBmaps_loess.4 &lt;= 0] &lt;- &quot;red&quot; tmp$col[tmp$AGBmaps_loess.12-tmp$AGBmaps_loess.4 &gt; 0] &lt;- &quot;blue&quot; pdf(&quot;../results/carbone/figures/Validation_LoessMap2015.pdf&quot;) plot(tmp$BaHa, tmp$AGBmaps_loess.12, main=&quot;AGB Carbon Stocks on Permanent Inventory Plots&quot;, xlab=&quot;National Forest Inventory 2015 (tC/ha)&quot;, ylab=&quot;LOESS-smoothed Carbon Map 2015&quot;) points(tmp$BaHa[tmp$cex &gt; 1.7], tmp$AGBmaps_loess.12[tmp$cex &gt; 1.7], cex=tmp$cex[tmp$cex &gt; 1.7], col=tmp$col[tmp$cex &gt; 1.7]) abline(0,1, lty=2) abline(0,0.5, lty=3) abline(0,1/0.5, lty=3) legend(&quot;bottomright&quot;, col=c(&quot;red&quot;, &quot;blue&quot;), pch=1, legend=c(&quot;Loss of &gt;= 20 tC since 2003&quot;, &quot;Gain of &gt;= 20 tC since 2003&quot;)) dev.off() pdf(&quot;../results/carbone/figures/Validation_LoessMap2015_Residuals.pdf&quot;) plot(tmp$AGBmaps_loess.12-tmp$BaHa~tmp$BaHa, main=&quot;AGB Carbon Stocks on Permanent Inventory Plots&quot;, xlab=&quot;National Forest Inventory 2015 (tC/ha)&quot;, ylab=&quot;Residuals of LOESS-smoothed Carbon Map 2015&quot;) points(tmp$BaHa[tmp$cex &gt; 1.7], tmp$AGBmaps_loess.12[tmp$cex &gt; 1.7]-tmp$BaHa[tmp$cex &gt; 1.7], cex=tmp$cex[tmp$cex &gt; 1.7], col=tmp$col[tmp$cex &gt; 1.7]) bias.corr &lt;- lm(tmp$AGBmaps_loess.12-tmp$BaHa~tmp$BaHa) abline(0,0) abline(bias.corr, lty=2) legend(&quot;topright&quot;, col=c(&quot;red&quot;, &quot;blue&quot;), pch=1, legend=c(&quot;Loss of &gt;= 20 tC since 2003&quot;, &quot;Gain of &gt;= 20 tC since 2003&quot;)) dev.off() # Load the cleaned maps and create tables # ======================================= biomass.tabs &lt;- function(map, aoi=NULL) { if(!is.null(aoi)) map &lt;- crop(map, aoi) agb.dat &lt;- map[] agb.change.dat &lt;- agb.dat[,2:ncol(agb.dat)] - agb.dat[,1:ncol(agb.dat)-1] agb.loss.dat &lt;- agb.change.dat; agb.loss.dat[agb.loss.dat &gt; 0] &lt;- 0 agb.gain.dat &lt;- agb.change.dat; agb.gain.dat[agb.gain.dat &lt; 0] &lt;- 0 dates &lt;- as.numeric(substr(names(map), 2, 5)) agb.tab &lt;- data.frame(year = dates, total.agb = colSums(agb.dat, na.rm=TRUE) * 30^2/10000, period = c(NA, dates[-1] - dates[-length(dates)]), center = c(NA, (dates[-1] + dates[-length(dates)])/2)) agb.tab$loss.t.yr &lt;- c(NA, colSums(agb.loss.dat, na.rm=TRUE) * 30^2/10000)/agb.tab$period agb.tab$gain.t.yr &lt;- c(NA, colSums(agb.gain.dat, na.rm=TRUE) * 30^2/10000)/agb.tab$period agb.tab$netchange.t.yr &lt;- agb.tab$loss.t.yr + agb.tab$gain.t.yr agb.tab$loss.pc.yr &lt;- round(100 * c(NA, 1/agb.tab$period[2:(nrow(agb.tab))] * log((agb.tab$total.agb[1:nrow(agb.tab)-1] + agb.tab$period[2:(nrow(agb.tab))] * agb.tab$loss.t.yr[2:(nrow(agb.tab))]) / agb.tab$total.agb[1:nrow(agb.tab)-1])), 3) agb.tab$gain.pc.yr &lt;- round(100 * c(NA, 1/agb.tab$period[2:(nrow(agb.tab))] * log((agb.tab$total.agb[1:nrow(agb.tab)-1] + agb.tab$period[2:(nrow(agb.tab))] * agb.tab$gain.t.yr[2:(nrow(agb.tab))]) / agb.tab$total.agb[1:nrow(agb.tab)-1])), 3) agb.tab$netchange.pc.yr &lt;- round(100 * c(NA, 1/agb.tab$period[2:(nrow(agb.tab))] * log( agb.tab$total.agb[2:nrow(agb.tab)] / agb.tab$total.agb[1:nrow(agb.tab)-1])), 3) return(agb.tab) } plot.biomass &lt;- function(fc, zone, filename=NULL) { if(zone==&quot;Zone IV&quot;) { title &lt;- &quot;Net and Gross Changes in Forest Biomass 1987 - 2019 in Ecological Zone IV&quot; ylim &lt;- c(-750000, 750000) ybreaks &lt;- seq(-750000, 750000, 100000) ymbreaks &lt;- seq(-750000, 750000, 50000) } if (zone==&quot;AOI.1&quot;) { title &lt;- &quot;Net and Gross Changes in Forest Biomass 1987 - 2019 in Plaine de Litimé (AOI.1)&quot; ylim &lt;- c(-18000, 15000) ybreaks &lt;- seq(-18000, 15000, 2000) ymbreaks &lt;- seq(-18000, 15000, 1000) } if (zone==&quot;AOI.2&quot;) { title &lt;- &quot;Net and Gross Changes in Forest Biomass 1987 - 2019 in Kpélé (AOI.2)&quot; ylim &lt;- c(-18000, 15000) ybreaks &lt;- seq(-18000, 15000, 2000) ymbreaks &lt;- seq(-18000, 15000, 1000) } if(!is.null(filename)) pdf(filename) fc$netloss.t.yr &lt;- fc$netchange.t.yr fc$netloss.t.yr[fc$netloss.t.yr &gt; 0] &lt;- 0 fc$netgain.t.yr &lt;- fc$netchange.t.yr fc$netgain.t.yr[fc$netgain.t.yr &lt; 0] &lt;- 0 print( fc[-1,] %&gt;% gather(variable, value, loss.t.yr, gain.t.yr, netloss.t.yr, netgain.t.yr) %&gt;% ggplot(aes(y=value, x = center, width=period-0.7)) + geom_bar(data=. %&gt;% filter(variable %in% c(&quot;loss.t.yr&quot;, &quot;gain.t.yr&quot;)), aes(fill=variable, alpha=0.9), stat = &quot;identity&quot;, show.legend=FALSE) + guides(alpha = FALSE) + geom_bar(data=. %&gt;% filter(variable %in% c(&quot;netloss.t.yr&quot;, &quot;netgain.t.yr&quot;)), aes(fill=variable), stat = &quot;identity&quot;) + scale_fill_manual(name = NULL, breaks = c(&quot;gain.t.yr&quot;, &quot;loss.t.yr&quot;), values=c(&quot;#00BFC4&quot;, &quot;#F8766D&quot;, &quot;#00BFC4&quot;, &quot;#F8766D&quot;), labels=c(&quot;Biomass gain &quot;, &quot;Biomass loss &quot;)) + xlab(&quot;Year&quot;) + ylab(&quot;Tonnes AGB per Year&quot;) + ggtitle(title) + scale_x_continuous(minor_breaks = NULL, breaks=c(1987, 2000, 2005, 2010, 2015, 2019)) + scale_y_continuous(breaks=ybreaks, minor_breaks = ymbreaks) + coord_cartesian(ylim=ylim) + theme_light() + theme(legend.position=c(0,1), legend.box = &quot;horizontal&quot;, legend.justification=c(-0.2,1.2)) ) if(!is.null(filename)) dev.off() } maps &lt;- brick(&quot;../results/carbone/level2/AGBmaps_loess.tif&quot;) names(maps) &lt;- dates maps.dat &lt;- as.data.frame(na.omit(maps[])) dens.plot &lt;- maps.dat %&gt;% gather(variable, value, X1991, X2000, X2005, X2010, X2015, X2018) %&gt;% ggplot(aes(x=value, color=variable, linetype=variable)) + geom_density() + theme_light() print(dens.plot) pdf(&quot;../results/carbone/figures/densities.pdf&quot;) print(dens.plot) dev.off() res.all &lt;- biomass.tabs(map=maps) res &lt;- biomass.tabs(map=maps[[c(&quot;X1991&quot;, &quot;X2000&quot;, &quot;X2005&quot;, &quot;X2010&quot;, &quot;X2015&quot;, &quot;X2018&quot;)]]) res.aoi1.all &lt;- biomass.tabs(map=maps, AOI.1) res.aoi1 &lt;- biomass.tabs(map=maps[[c(&quot;X1991&quot;, &quot;X2000&quot;, &quot;X2005&quot;, &quot;X2010&quot;, &quot;X2015&quot;, &quot;X2018&quot;)]], AOI.1) res.aoi2.all &lt;- biomass.tabs(map=maps, AOI.2) res.aoi2 &lt;- biomass.tabs(map=maps[[c(&quot;X1991&quot;, &quot;X2000&quot;, &quot;X2005&quot;, &quot;X2010&quot;, &quot;X2015&quot;, &quot;X2018&quot;)]], AOI.2) res.tmp &lt;- biomass.tabs(map=maps[[c(&quot;X1991&quot;, &quot;X2003&quot;, &quot;X2018&quot;)]]) plot.biomass(res, &quot;Zone IV&quot;, &quot;../results/carbone/figures/biomass.pdf&quot;) plot.biomass(res.aoi1, &quot;AOI.1&quot;, &quot;../results/carbone/figures/biomass_aoi1.pdf&quot;) plot.biomass(res.aoi2, &quot;AOI.2&quot;, &quot;../results/carbone/figures/biomass_aoi2.pdf&quot;) write.xlsx(list(&quot;Total&quot; = res, &quot;AOI.1&quot; = res.aoi1, &quot;AOI.2&quot; = res.aoi2), file = &quot;../results/carbone/figures/biomass-change.xlsx&quot;) Y.diff &lt;- raster(&quot;../results/carbone/AGB_2018_2015_corr.tif&quot;) - Y.corr Y.gain &lt;- Y.diff; Y.gain[Y.gain&lt;=0] &lt;- 0 Y.loss &lt;- Y.diff; Y.loss[Y.loss&gt;=0] &lt;- 0 Y.diff &lt;- writeRaster(round(Y.diff), &quot;../results/carbone/AGB_2018_1991_diff.tif&quot;, datatype=&quot;INT2S&quot;, overwrite=TRUE) -sum(Y.loss[], na.rm=TRUE)/sum(raster(&quot;../results/carbone/AGB_2003_2015_corr.tif&quot;)[], na.rm=TRUE)/15 sum(Y.gain[], na.rm=TRUE)/sum(raster(&quot;../results/carbone/AGB_2003_2015_corr.tif&quot;)[], na.rm=TRUE)/15 dat.c &lt;- data.frame(X1991=raster(&quot;../results/carbone/AGB_1991_2015_corr.tif&quot;)[], X2003=raster(&quot;../results/carbone/AGB_2003_2015_corr.tif&quot;)[], X2018=raster(&quot;../results/carbone/AGB_2018_2015_corr.tif&quot;)[]) dat.c &lt;- na.omit(dat.c) dat.c %&gt;% gather(variable, value, X1991:X2018) %&gt;% ggplot(aes(y=value, x=variable, color=variable)) + geom_boxplot() + coord_flip() + theme_light() pdf(&quot;../results/carbone/densities_91-03-18.pdf&quot;) dat.c %&gt;% gather(variable, value, X1991:X2018) %&gt;% ggplot(aes(x=value, color=variable)) + geom_density() + theme_light() dev.off() # verify the map val.points &lt;- raster::extract(Y.corr, dat.placette, sp=TRUE, fun=mean, buffer=20, normalizeWeights=TRUE) val.points &lt;- raster::extract(Y.2003.corr, val.points, sp=TRUE, fun=mean, buffer=20, normalizeWeights=TRUE) val.points &lt;- raster::extract(Y.2015, val.points, sp=TRUE, fun=mean, buffer=20, normalizeWeights=TRUE) plot(val.points$BaHa, val.points$AGB_2015_2003_corr) abline(0,1) mod &lt;- lm(AGB_2015_2003_corr ~ BaHa, data=val.points) abline(mod, lty=2) ############ compare 2015 carbon map with the forest change map ####################### map.carb &lt;- raster(&quot;../results/carbone/AGB_2015_REF.tif&quot;) map.gain &lt;- raster(&quot;../results/fcc/3_alldates-direct2003/level3/forest-gain.tif&quot;) map.loss &lt;- raster(&quot;../results/fcc/3_alldates-direct2003/level3/forest-loss.tif&quot;) map.loss.a2015 &lt;- map.loss; map.loss.a2015[map.loss.a2015&lt;2015] &lt;- NA map.carb.loss.a2015 &lt;- mask(map.carb, map.loss.a2015) hist(map.carb.loss.a2015[]); summary(map.carb.loss.a2015[]); sum(map.carb.loss.a2015[], na.rm=TRUE)*(30*30/10000) map.gain.b2015 &lt;- map.gain; map.gain.b2015[map.gain.b2015&gt;=2015] &lt;- NA map.gain.b2015[] &lt;- 2015 - map.gain.b2015[] map.carb.gain.b2015 &lt;- mask(map.carb, map.gain.b2015) hist(map.carb.gain.b2015[]); summary(map.carb.gain.b2015[]); sum(map.carb.gain.b2015[], na.rm=TRUE)*(30*30/10000) boxplot(map.carb.gain.b2015[] ~ map.gain.b2015[]) map.carb.loss.a2015.2 &lt;- mask(Y.loss, map.loss.a2015) # ######### Try with linear regression ############ # X$evi &lt;- evi(nir=X$NIR, r=X$R, b=X$B) # X$ndvi &lt;- nd(X$NIR, X$R) # X$ndmi &lt;- nd(X$NIR, X$SWIR1) # X$nbr &lt;- nd(X$NIR, X$SWIR2) # cor(-X$SWIR1[], Y.2015[], use=&quot;pairwise.complete.obs&quot;) # s &lt;- sample(1:ncell(Y.2015), 10000) # plot(log(Y.2015[s]) ~ log(X$SWIR1[s])) # # # val.points &lt;- raster::extract(X, val.points, sp=TRUE, fun=mean, buffer=20, normalizeWeights=TRUE) # plot(log(val.points$BaHa) ~ log(val.points$SWIR1)) # val.points$BaHa.log &lt;- log(val.points$BaHa) # val.points$SWIR1.log &lt;- log(val.points$SWIR1) # lm.swir &lt;- lm(BaHa.log ~ SWIR1.log, data=val.points@data) # abline(lm.swir) # # plot(val.points$BaHa ~ val.points$SWIR1) # lines(seq(1000, 2500, 50), exp(lm.swir$coeff[1] + lm.swir$coeff[2]*log(seq(1000, 2500, 50))), lty=4) # # Y.2015.swir &lt;- exp(lm.swir$coeff[1] + lm.swir$coeff[2]*log(X$SWIR1)) # val.points &lt;- raster::extract(Y.2015.swir, val.points, sp=TRUE, fun=mean, buffer=20, normalizeWeights=TRUE) # # plot(val.points$BaHa, val.points@data[,25]) # abline(0,1) # # # # Y.2015.log &lt;- log(Y.2015) # SWIR1.log &lt;- log(X$SWIR1) # lm.swir.map &lt;- lm(Y.2015.log[] ~ SWIR1.log[]) # abline(lm.swir.map, lty=2) # # plot(Y.2015[s] ~ X$SWIR1[s]) # lines(seq(1000, 4000, 50), exp(lm.swir.map$coeff[1] + lm.swir.map$coeff[2]*log(seq(1000, 4000, 50))), lty=4, col=&quot;red&quot;) # Y.2015.swir &lt;- exp(lm.swir.map$coeff[1] + lm.swir.map$coeff[2]*log(X$SWIR1)) # Y.2015.swir &lt;- writeRaster(Y.2015.swir, &quot;../results/carbone/AGB_2015_SWIR1.tif&quot;) # # # Y &lt;- Y.2015.swir # X &lt;- stack(&quot;../results/images/20030127_L7_B123457_L2.tif&quot;) # names(X) &lt;- c(&quot;B&quot;, &quot;G&quot;, &quot;R&quot;, &quot;NIR&quot;, &quot;SWIR1&quot;, &quot;SWIR2&quot;) # # Y.log &lt;- log(Y) # SWIR.log &lt;- log(X$SWIR1) # # lm.swir &lt;- lm(Y.log[] ~ SWIR.log[]) # Y.swir &lt;- exp(lm.swir$coeff[1] + lm.swir$coeff[2]*SWIR.log) # Y.swir &lt;- writeRaster(Y.swir, &quot;../results/carbone/AGB_2003_SWIR1.tif&quot;) # # Y.diff &lt;- Y.2015.swir - Y.swir # Y.diff &lt;- writeRaster(Y.diff, &quot;../results/carbone/AGB_diff_SWIR1.tif&quot;) ggplot(plots@data[], aes(x=cov18, y=BaHa)) + geom_point(aes(colour=OccSol), # colour depends on cond2 size=3) + facet_wrap( ~ OccSol) "],
["04_analyze-AGB-maps.html", "", " 4.2.4 Production des résultats Description Description de la méthodologie / script AGB/5_analyze-AGB.R #################################################################### # NERF_Togo/AGB/5_analyze-AGB.R: analyze AGB maps # ------------------------------------------------------------------ # Bern University of Applied Sciences # Oliver Gardi, &lt;oliver.gardi@bfh.ch&gt; # 10 October 2019 RSR.Y &lt;- 0.563 RSR.Y.SE &lt;- 0.086 RSR.O &lt;- 0.275 RSR.O.SE &lt;- 0.003 RSR.AGB &lt;- 20 C.RATIO &lt;- 0.47 # evaluate 2015 biomass map with IFN strata ------------------ agb.2015 &lt;- raster(paste0(AGB.REF.DIR, &quot;/TGO_2015_AGB_R.tif&quot;)) strata &lt;- raster(paste0(DATA.DIR, &quot;/RapidEye/TGO_30m.tif&quot;)) zonal(agb.2015, strata, fun=&quot;mean&quot;) zonal(agb.2015, strata, fun=&quot;sd&quot;) # AGB density plots --------- files &lt;- dir(AGB.REF.DIR, pattern=&quot;^TGO.*\\\\.tif$&quot;, full.names=TRUE) maps &lt;- stack(files) names(maps) &lt;- substr(names(maps), 5, 8) maps.dat &lt;- as.data.frame(na.omit(maps[[c(&quot;X2003&quot;, &quot;X2018&quot;)]][])) dens.plot &lt;- maps.dat %&gt;% gather(variable, value, X2003, X2018) %&gt;% ggplot(aes(x=value, color=variable, linetype=variable)) + geom_density() + theme_light() # print(dens.plot) pdf(paste0(AGB.RES.DIR, &quot;/AGB-densities_03-18.pdf&quot;)) print(dens.plot) dev.off() # Analyze emissions and enhancements 2003/2018 ---------------- agb.2003 &lt;- raster(paste0(AGB.REF.DIR, &quot;/TGO_2003_AGB_R.tif&quot;)) bgb.2003 &lt;- agb.2003 * RSR.O bgb.2003[agb.2003 &lt;= RSR.AGB] &lt;- agb.2003[agb.2003 &lt;= RSR.AGB] * RSR.Y agb.2018 &lt;- raster(paste0(AGB.REF.DIR, &quot;/TGO_2018_AGB_R.tif&quot;)) bgb.2018 &lt;- agb.2018 * RSR.O bgb.2018[agb.2018 &lt;= RSR.AGB] &lt;- agb.2018[agb.2018 &lt;= RSR.AGB] * RSR.Y fc.2003 &lt;- raster(paste0(FCC.CLN.DIR, &quot;/FC30/TGO/TGO_2003_F30cf.tif&quot;)) fc.2018 &lt;- raster(paste0(FCC.CLN.DIR, &quot;/FC30/TGO/TGO_2018_F30cf.tif&quot;)) # emissions from deforestation defor &lt;- fc.2003 == 1 &amp; fc.2018 != 1 nf.agb &lt;- mean(agb.2018[fc.2018 == 3], na.rm=TRUE) # average non-forest AGB pd.agb &lt;- mean(agb.2018[defor], na.rm = TRUE) # average post-defor AGB defor.agb &lt;- mask(agb.2003, defor, maskvalue=0) - nf.agb defor.agb[defor.agb &lt; 0] &lt;- 0 nf.bgb &lt;- mean(bgb.2018[fc.2018 == 3], na.rm=TRUE) # average non-forest BGB pd.bgb &lt;- mean(bgb.2018[defor], na.rm = TRUE) # average post-defor BGB defor.bgb &lt;- mask(bgb.2003, defor, maskvalue=0) - nf.bgb defor.bgb[defor.bgb &lt; 0] &lt;- 0 # enhancements from reforestation regen &lt;- fc.2003 != 1 &amp; fc.2018 == 1 regen.agb.2003 &lt;- mask(agb.2003, regen, maskvalue=0) regen.agb.2018 &lt;- mask(agb.2018, regen, maskvalue=0) regen.agb &lt;- regen.agb.2018 - regen.agb.2003 regen.agb[regen.agb &lt; 0] &lt;- 0 regen.agb.2003 &lt;- mask(bgb.2003, regen, maskvalue=0) regen.agb.2018 &lt;- mask(bgb.2018, regen, maskvalue=0) regen.bgb &lt;- regen.bgb.2018 - regen.bgb.2003 regen.bgb[regen.bgb &lt; 0] &lt;- 0 # statistics for TGO and regions emissions &lt;- function(defor, defor.agb, defor.bgb, regen, regen.agb, regen.bgb, aoi=NULL, years=15) { if(!is.null(aoi)) { defor &lt;- mask(crop(defor, aoi), aoi) defor.agb &lt;- mask(crop(defor.agb, aoi), aoi) defor.bgb &lt;- mask(crop(defor.agb, aoi), aoi) regen &lt;- mask(crop(regen, aoi), aoi) regen.agb &lt;- mask(crop(regen.agb, aoi), aoi) regen.bgb &lt;- mask(crop(regen.bgb, aoi), aoi) } defor.co2 &lt;- (defor.agb + defor.bgb) * C.RATIO * 44/12 defor.area &lt;- sum(defor[], na.rm=TRUE) * 30^2 / 10000 defor.area.a &lt;- defor.area / years defor.agb.ha &lt;- mean(defor.agb[], na.rm=TRUE) defor.bgb.ha &lt;- mean(defor.bgb[], na.rm=TRUE) defor.co2.ha &lt;- mean(defor.co2[], na.rm=TRUE) regen.co2 &lt;- (regen.agb + regen.bgb) * C.RATIO * 44/12 regen.area &lt;- sum(regen[], na.rm=TRUE) * 30^2 / 10000 regen.area.a &lt;- regen.area / years regen.agb.ha &lt;- mean(regen.agb[], na.rm=TRUE) regen.bgb.ha &lt;- mean(regen.bgb[], na.rm=TRUE) regen.co2.ha &lt;- mean(regen.co2[], na.rm=TRUE) return(list( defor.area = defor.area, defor.area.a = defor.area.a, defor.agb.ha = defor.agb.ha, defor.agb.a = defor.agb.ha * defor.area.a, defor.bgb.ha = defor.bgb.ha, defor.bgb.a = defor.bgb.ha * defor.area.a, defor.co2.ha = defor.co2.ha, defor.co2.a = defor.co2.ha * defor.area.a, regen.area = regen.area, regen.area.a = regen.area.a, regen.agb.ha = regen.agb.ha, regen.agb.a = regen.agb.ha * regen.area.a, regen.bgb.ha = regen.bgb.ha, regen.bgb.a = regen.bgb.ha * regen.area.a, regen.co2.ha = regen.co2.ha, regen.co2.a = regen.co2.ha * regen.area.a )) } # DO THE WORK registerDoParallel(.env$numCores-1) res &lt;- foreach(i=0:length(TGO.reg), .combine=rbind) %dopar% { if(i==0) { data.frame(reg = &quot;TGO&quot;, emissions(defor, defor.agb, defor.bgb, regen, regen.agb, regen.bgb)) } else { region &lt;- TGO.reg[i,] data.frame(reg = region$NAME_1, emissions(defor, defor.agb, defor.bgb, regen, regen.agb, regen.bgb, aoi=region)) } } write.csv(res, paste0(AGB.RES.DIR, &quot;/NERF-Results.csv&quot;), row.names=FALSE) # Aggregated uncertainty (Monte Carlo Simulation) # activity data (uncertainty) mc.defor &lt;- rnorm(1000, mean.defor, sd) mc.regen &lt;- rnorm(1000, mean.regen, sd) # emission factor mean.agb.2003 &lt;- mean.agb.2018 &lt;- mean(agb.2018[defor]) mc.agb.2003 &lt;- rnorm(1000, mean(agb.2003[defor]), sqrt((RMSE^2)/n) ) mc.bgb.2003 &lt;- mc.agb.2003 * rnorm(1000, mean(bgb.2003), RSR.O.SE) mc.agb.2018 &lt;- rnorm(1000, mean(agb.2018[defor]), sqrt((RMSE^2)/n) ) mc.bgb.2018 &lt;- mc.agb.2003 * rnorm(1000, mean(bgb.2003), RSR.O.SE) "]
]
